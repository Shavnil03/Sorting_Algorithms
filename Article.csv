ID,TITLE,ABSTRACT,Computer Science,Physics,Mathematics,Statistics,Quantitative Biology,Quantitative Finance
1,Reconstructing Subject-Specific Effect Maps,"  Predictive models allow subject-specific inference when analyzing disease
related alterations in neuroimaging data. Given a subject's data, inference can
be made at two levels: global, i.e. identifiying condition presence for the
subject, and local, i.e. detecting condition effect on each individual
measurement extracted from the subject's data. While global inference is widely
used, local inference, which can be used to form subject-specific effect maps,
is rarely used because existing models often yield noisy detections composed of
dispersed isolated islands. In this article, we propose a reconstruction
method, named RSM, to improve subject-specific detections of predictive
modeling approaches and in particular, binary classifiers. RSM specifically
aims to reduce noise due to sampling error associated with using a finite
sample of examples to train classifiers. The proposed method is a wrapper-type
algorithm that can be used with different binary classifiers in a diagnostic
manner, i.e. without information on condition presence. Reconstruction is posed
as a Maximum-A-Posteriori problem with a prior model whose parameters are
estimated from training data in a classifier-specific fashion. Experimental
evaluation is performed on synthetically generated data and data from the
Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on
synthetic data demonstrate that using RSM yields higher detection accuracy
compared to using models directly or with bootstrap averaging. Analyses on the
ADNI dataset show that RSM can also improve correlation between
subject-specific detections in cortical thickness data and non-imaging markers
of Alzheimer's Disease (AD), such as the Mini Mental State Examination Score
and Cerebrospinal Fluid amyloid-$\beta$ levels. Further reliability studies on
the longitudinal ADNI dataset show improvement on detection reliability when
RSM is used.
",1,0,0,0,0,0
2,Rotation Invariance Neural Network,"  Rotation invariance and translation invariance have great values in image
recognition tasks. In this paper, we bring a new architecture in convolutional
neural network (CNN) named cyclic convolutional layer to achieve rotation
invariance in 2-D symbol recognition. We can also get the position and
orientation of the 2-D symbol by the network to achieve detection purpose for
multiple non-overlap target. Last but not least, this architecture can achieve
one-shot learning in some cases using those invariance.
",1,0,0,0,0,0
3,Spherical polyharmonics and Poisson kernels for polyharmonic functions,"  We introduce and develop the notion of spherical polyharmonics, which are a
natural generalisation of spherical harmonics. In particular we study the
theory of zonal polyharmonics, which allows us, analogously to zonal harmonics,
to construct Poisson kernels for polyharmonic functions on the union of rotated
balls. We find the representation of Poisson kernels and zonal polyharmonics in
terms of the Gegenbauer polynomials. We show the connection between the
classical Poisson kernel for harmonic functions on the ball, Poisson kernels
for polyharmonic functions on the union of rotated balls, and the Cauchy-Hua
kernel for holomorphic functions on the Lie ball.
",0,0,1,0,0,0
4,A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system,"  The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the
Maxwell equations (the so called stochastic MLLG system) describes the creation
of domain walls and vortices (fundamental objects for the novel nanostructured
magnetic memories). We first reformulate the stochastic LLG equation into an
equation with time-differentiable solutions. We then propose a convergent
$\theta$-linear scheme to approximate the solutions of the reformulated system.
As a consequence, we prove convergence of the approximate solutions, with no or
minor conditions on time and space steps (depending on the value of $\theta$).
Hence, we prove the existence of weak martingale solutions of the stochastic
MLLG system. Numerical results are presented to show applicability of the
method.
",0,0,1,0,0,0
5,Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants,"  Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species
were used to explore the influence of preprocessing and feature extraction on
efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and
Discrete Wavelet Transforms (DWT) were compared as feature extraction
techniques for FTIR data of medicinal plants. Various combinations of signal
processing steps showed different behavior when applied to classification and
clustering tasks. Best results for WTT and DWT found through grid search were
similar, significantly improving quality of clustering as well as
classification accuracy for tuned logistic regression in comparison to original
spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a
more versatile and easier to use as a data processing tool in various signal
processing applications.
",1,0,0,1,0,0
6,On maximizing the fundamental frequency of the complement of an obstacle,"  Let $\Omega \subset \mathbb{R}^n$ be a bounded domain satisfying a
Hayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain
referred to as ""obstacle"". We are interested in the behaviour of the first
Dirichlet eigenvalue $ \lambda_1(\Omega \setminus (x+D)) $. First, we prove an
upper bound on $ \lambda_1(\Omega \setminus (x+D)) $ in terms of the distance
of the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet
ground state $ \phi_{\lambda_1} > 0 $ of $ \Omega $. In short, a direct
corollary is that if \begin{equation} \mu_\Omega := \max_{x}\lambda_1(\Omega
\setminus (x+D)) \end{equation} is large enough in terms of $ \lambda_1(\Omega)
$, then all maximizer sets $ x+D $ of $ \mu_\Omega $ are close to each maximum
point $ x_0 $ of $ \phi_{\lambda_1} $.
Second, we discuss the distribution of $ \phi_{\lambda_1(\Omega)} $ and the
possibility to inscribe wavelength balls at a given point in $ \Omega $.
Finally, we specify our observations to convex obstacles $ D $ and show that
if $ \mu_\Omega $ is sufficiently large with respect to $ \lambda_1(\Omega) $,
then all maximizers $ x+D $ of $ \mu_\Omega $ contain all maximum points $ x_0
$ of $ \phi_{\lambda_1(\Omega)} $.
",0,0,1,0,0,0
7,On the rotation period and shape of the hyperbolic asteroid 1I/`Oumuamua (2017) U1 from its lightcurve,"  We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017
U1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel
Telescope. From these observations, we derived a partial lightcurve with
peak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out
rotation periods less than 3 hr and suggests that the period is at least 5 hr.
On the assumption that the variability is due to a changing cross section, the
axial ratio is at least 3:1. We saw no evidence for a coma or tail in either
individual images or in a stacked image having an equivalent exposure time of
9000 s.
",0,1,0,0,0,0
8,Adverse effects of polymer coating on heat transport at solid-liquid interface,"  The ability of metallic nanoparticles to supply heat to a liquid environment
under exposure to an external optical field has attracted growing interest for
biomedical applications. Controlling the thermal transport properties at a
solid-liquid interface then appears to be particularly relevant. In this work,
we address the thermal transport between water and a gold surface coated by a
polymer layer. Using molecular dynamics simulations, we demonstrate that
increasing the polymer density displaces the domain resisting to the heat flow,
while it doesn't affect the final amount of thermal energy released in the
liquid. This unexpected behavior results from a trade-off established by the
increasing polymer density which couples more efficiently with the solid but
initiates a counterbalancing resistance with the liquid.
",0,1,0,0,0,0
9,"SPH calculations of Mars-scale collisions: the role of the Equation of State, material rheologies, and numerical effects","  We model large-scale ($\approx$2000km) impacts on a Mars-like planet using a
Smoothed Particle Hydrodynamics code. The effects of material strength and of
using different Equations of State on the post-impact material and temperature
distributions are investigated. The properties of the ejected material in terms
of escaping and disc mass are analysed as well. We also study potential
numerical effects in the context of density discontinuities and rigid body
rotation. We find that in the large-scale collision regime considered here
(with impact velocities of 4km/s), the effect of material strength is
substantial for the post-impact distribution of the temperature and the
impactor material, while the influence of the Equation of State is more subtle
and present only at very high temperatures.
",0,1,0,0,0,0
10,$\mathcal{R}_{0}$ fails to predict the outbreak potential in the presence of natural-boosting immunity,"  Time varying susceptibility of host at individual level due to waning and
boosting immunity is known to induce rich long-term behavior of disease
transmission dynamics. Meanwhile, the impact of the time varying heterogeneity
of host susceptibility on the shot-term behavior of epidemics is not
well-studied, even though the large amount of the available epidemiological
data are the short-term epidemics. Here we constructed a parsimonious
mathematical model describing the short-term transmission dynamics taking into
account natural-boosting immunity by reinfection, and obtained the explicit
solution for our model. We found that our system show ""the delayed epidemic"",
the epidemic takes off after negative slope of the epidemic curve at the
initial phase of epidemic, in addition to the common classification in the
standard SIR model, i.e., ""no epidemic"" as $\mathcal{R}_{0}\leq1$ or normal
epidemic as $\mathcal{R}_{0}>1$. Employing the explicit solution we derived the
condition for each classification.
",0,0,0,0,1,0
11,A global sensitivity analysis and reduced order models for hydraulically-fractured horizontal wells,"  We present a systematic global sensitivity analysis using the Sobol method
which can be utilized to rank the variables that affect two quantity of
interests -- pore pressure depletion and stress change -- around a
hydraulically-fractured horizontal well based on their degree of importance.
These variables include rock properties and stimulation design variables. A
fully-coupled poroelastic hydraulic fracture model is used to account for pore
pressure and stress changes due to production. To ease the computational cost
of a simulator, we also provide reduced order models (ROMs), which can be used
to replace the complex numerical model with a rather simple analytical model,
for calculating the pore pressure and stresses at different locations around
hydraulic fractures. The main findings of this research are: (i) mobility,
production pressure, and fracture half-length are the main contributors to the
changes in the quantities of interest. The percentage of the contribution of
each parameter depends on the location with respect to pre-existing hydraulic
fractures and the quantity of interest. (ii) As the time progresses, the effect
of mobility decreases and the effect of production pressure increases. (iii)
These two variables are also dominant for horizontal stresses at large
distances from hydraulic fractures. (iv) At zones close to hydraulic fracture
tips or inside the spacing area, other parameters such as fracture spacing and
half-length are the dominant factors that affect the minimum horizontal stress.
The results of this study will provide useful guidelines for the stimulation
design of legacy wells and secondary operations such as refracturing and infill
drilling.
",1,0,0,0,0,0
12,Role-separating ordering in social dilemmas controlled by topological frustration,"  ""Three is a crowd"" is an old proverb that applies as much to social
interactions, as it does to frustrated configurations in statistical physics
models. Accordingly, social relations within a triangle deserve special
attention. With this motivation, we explore the impact of topological
frustration on the evolutionary dynamics of the snowdrift game on a triangular
lattice. This topology provides an irreconcilable frustration, which prevents
anti-coordination of competing strategies that would be needed for an optimal
outcome of the game. By using different strategy updating protocols, we observe
complex spatial patterns in dependence on payoff values that are reminiscent to
a honeycomb-like organization, which helps to minimize the negative consequence
of the topological frustration. We relate the emergence of these patterns to
the microscopic dynamics of the evolutionary process, both by means of
mean-field approximations and Monte Carlo simulations. For comparison, we also
consider the same evolutionary dynamics on the square lattice, where of course
the topological frustration is absent. However, with the deletion of diagonal
links of the triangular lattice, we can gradually bridge the gap to the square
lattice. Interestingly, in this case the level of cooperation in the system is
a direct indicator of the level of topological frustration, thus providing a
method to determine frustration levels in an arbitrary interaction network.
",0,1,0,0,0,0
13,Dynamics of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells: the effect of self-localization,"  We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se
diluted-magnetic-semiconductor quantum wells using time-resolved
photoluminescence (PL). The magnetic field and temperature dependencies of this
dynamics allow us to separate the non-magnetic and magnetic contributions to
the exciton localization. We deduce the EMP energy of 14 meV, which is in
agreement with time-integrated measurements based on selective excitation and
the magnetic field dependence of the PL circular polarization degree. The
polaron formation time of 500 ps is significantly longer than the corresponding
values reported earlier. We propose that this behavior is related to strong
self-localization of the EMP, accompanied with a squeezing of the heavy-hole
envelope wavefunction. This conclusion is also supported by the decrease of the
exciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and
temperature.
",0,1,0,0,0,0
14,On Varieties of Ordered Automata,"  The classical Eilenberg correspondence, based on the concept of the syntactic
monoid, relates varieties of regular languages with pseudovarieties of finite
monoids. Various modifications of this correspondence appeared, with more
general classes of regular languages on one hand and classes of more complex
algebraic structures on the other hand. For example, classes of languages need
not be closed under complementation or all preimages under homomorphisms, while
monoids can be equipped with a compatible order or they can have a
distinguished set of generators. Such generalized varieties and pseudovarieties
also have natural counterparts formed by classes of finite (ordered) automata.
In this paper the previous approaches are combined. The notion of positive
$\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final
states are specified) is introduced and their correspondence with positive
$\mathcal C$-varieties of languages is proved.
",1,0,0,0,0,0
15,Direct Evidence of Spontaneous Abrikosov Vortex State in Ferromagnetic Superconductor EuFe$_2$(As$_{1-x}$P$_x$)$_2$ with $x=0.21$,"  Using low-temperature Magnetic Force Microscopy (MFM) we provide direct
experimental evidence for spontaneous vortex phase (SVP) formation in
EuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting
$T^{\rm 0}_{\rm SC}=23.6$~K and ferromagnetic $T_{\rm FM}\sim17.7$~K transition
temperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the
vicinity of $T_{\rm FM}$. Also, upon cooling cycle near $T_{\rm FM}$ we observe
the first-order transition from the short period domain structure, which
appears in the Meissner state, into the long period domain structure with
spontaneous vortices. It is the first experimental observation of this scenario
in the ferromagnetic superconductors. Low-temperature phase is characterized by
much larger domains in V-AV state and peculiar branched striped structures at
the surface, which are typical for uniaxial ferromagnets with perpendicular
magnetic anisotropy (PMA). The domain wall parameters at various temperatures
are estimated.
",0,1,0,0,0,0
16,A rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ with 432 symmetries,"  The recent discovery that the exponent of matrix multiplication is determined
by the rank of the symmetrized matrix multiplication tensor has invigorated
interest in better understanding symmetrized matrix multiplication. I present
an explicit rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ and
describe its symmetry group.
",0,0,1,0,0,0
17,The PdBI Arcsecond Whirlpool Survey (PAWS). The Role of Spiral Arms in Cloud and Star Formation,"  The process that leads to the formation of the bright star forming sites
observed along prominent spiral arms remains elusive. We present results of a
multi-wavelength study of a spiral arm segment in the nearby grand-design
spiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas
spurs. The combined observations of the(ionized, atomic, molecular, dusty)
interstellar medium (ISM) with star formation tracers (HII regions, young
<10Myr stellar clusters) suggest (1) no variation in giant molecular cloud
(GMC) properties between arm and gas spurs, (2) gas spurs and extinction
feathers arising from the same structure with a close spatial relation between
gas spurs and ongoing/recent star formation (despite higher gas surface
densities in the spiral arm), (3) no trend in star formation age either along
the arm or along a spur, (4) evidence for strong star formation feedback in gas
spurs: (5) tentative evidence for star formation triggered by stellar feedback
for one spur, and (6) GMC associations (GMAs) being no special entities but the
result of blending of gas arm/spur cross-sections in lower resolution
observations. We conclude that there is no evidence for a coherent star
formation onset mechanism that can be solely associated to the presence of the
spiral density wave. This suggests that other (more localized) mechanisms are
important to delay star formation such that it occurs in spurs. The evidence of
star formation proceeding over several million years within individual spurs
implies that the mechanism that leads to star formation acts or is sustained
over a longer time-scale.
",0,1,0,0,0,0
18,Higher structure in the unstable Adams spectral sequence,"  We describe a variant construction of the unstable Adams spectral the
sequence for a space $Y$, associated to any free simplicial resolution of
$H^*(Y;R)$ for $R=\mathbb{F}_p$ or $\mathbb{Q}$. We use this construction to
describe the differentials and filtration in the spectral sequence in terms of
appropriate systems of higher cohomology operations.
",0,0,1,0,0,0
19,Comparing Covariate Prioritization via Matching to Machine Learning Methods for Causal Inference using Five Empirical Applications,"  When investigators seek to estimate causal effects, they often assume that
selection into treatment is based only on observed covariates. Under this
identification strategy, analysts must adjust for observed confounders. While
basic regression models have long been the dominant method of statistical
adjustment, more robust methods based on matching or weighting have become more
common. Of late, even more flexible methods based on machine learning methods
have been developed for statistical adjustment. These machine learning methods
are designed to be black box methods with little input from the researcher.
Recent research used a data competition to evaluate various methods of
statistical adjustment and found that black box methods out performed all other
methods of statistical adjustment. Matching methods with covariate
prioritization are designed for direct input from substantive investigators in
direct contrast to black methods. In this article, we use a different research
design to compare matching with covariate prioritization to black box methods.
We use black box methods to replicate results from five studies where matching
with covariate prioritization was used to customize the statistical adjustment
in direct response to substantive expertise. We find little difference across
the methods. We conclude with advice for investigators.
",0,0,0,1,0,0
20,Acoustic Impedance Calculation via Numerical Solution of the Inverse Helmholtz Problem,"  Assigning homogeneous boundary conditions, such as acoustic impedance, to the
thermoviscous wave equations (TWE) derived by transforming the linearized
Navier-Stokes equations (LNSE) to the frequency domain yields a so-called
Helmholtz solver, whose output is a discrete set of complex eigenfunction and
eigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --
reverses such procedure by returning the value of acoustic impedance at one or
more unknown impedance boundaries (IBs) of a given domain via spatial
integration of the TWE for a given real-valued frequency with assigned
conditions on other boundaries. The iHS procedure is applied to a second-order
spatial discretization of the TWEs derived on an unstructured grid with
staggered grid arrangement. The momentum equation only is extended to the
center of each IB face where pressure and velocity components are co-located
and treated as unknowns. One closure condition considered for the iHS is the
assignment of the surface gradient of pressure phase over the IBs,
corresponding to assigning the shape of the acoustic waveform at the IB. The
iHS procedure is carried out independently for each frequency in order to
return the complete broadband complex impedance distribution at the IBs in any
desired frequency range. The iHS approach is first validated against Rott's
theory for both inviscid and viscous, rectangular and circular ducts. The
impedance of a geometrically complex toy cavity is then reconstructed and
verified against companion full compressible unstructured Navier-Stokes
simulations resolving the cavity geometry and one-dimensional impedance test
tube calculations based on time-domain impedance boundary conditions (TDIBC).
The iHS methodology is also shown to capture thermoacoustic effects, with
reconstructed impedance values quantitatively in agreement with thermoacoustic
growth rates.
",0,1,0,0,0,0
21,Deciphering noise amplification and reduction in open chemical reaction networks,"  The impact of random fluctuations on the dynamical behavior a complex
biological systems is a longstanding issue, whose understanding would shed
light on the evolutionary pressure that nature imposes on the intrinsic noise
levels and would allow rationally designing synthetic networks with controlled
noise. Using the Itō stochastic differential equation formalism, we performed
both analytic and numerical analyses of several model systems containing
different molecular species in contact with the environment and interacting
with each other through mass-action kinetics. These systems represent for
example biomolecular oligomerization processes, complex-breakage reactions,
signaling cascades or metabolic networks. For chemical reaction networks with
zero deficiency values, which admit a detailed- or complex-balanced steady
state, all molecular species are uncorrelated. The number of molecules of each
species follow a Poisson distribution and their Fano factors, which measure the
intrinsic noise, are equal to one. Systems with deficiency one have an
unbalanced non-equilibrium steady state and a non-zero S-flux, defined as the
flux flowing between the complexes multiplied by an adequate stoichiometric
coefficient. In this case, the noise on each species is reduced if the flux
flows from the species of lowest to highest complexity, and is amplified is the
flux goes in the opposite direction. These results are generalized to systems
of deficiency two, which possess two independent non-vanishing S-fluxes, and we
conjecture that a similar relation holds for higher deficiency systems.
",0,0,0,0,1,0
22,Many-Body Localization: Stability and Instability,"  Rare regions with weak disorder (Griffiths regions) have the potential to
spoil localization. We describe a non-perturbative construction of local
integrals of motion (LIOMs) for a weakly interacting spin chain in one
dimension, under a physically reasonable assumption on the statistics of
eigenvalues. We discuss ideas about the situation in higher dimensions, where
one can no longer ensure that interactions involving the Griffiths regions are
much smaller than the typical energy-level spacing for such regions. We argue
that ergodicity is restored in dimension d > 1, although equilibration should
be extremely slow, similar to the dynamics of glasses.
",0,1,1,0,0,0
23,Fault Detection and Isolation Tools (FDITOOLS) User's Guide,"  The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB
functions for the analysis and solution of fault detection and model detection
problems. The implemented functions are based on the computational procedures
described in the Chapters 5, 6 and 7 of the book: ""A. Varga, Solving Fault
Diagnosis Problems - Linear Synthesis Techniques, Springer, 2017"". This
document is the User's Guide for the version V1.0 of FDITOOLS. First, we
present the mathematical background for solving several basic exact and
approximate synthesis problems of fault detection filters and model detection
filters. Then, we give in-depth information on the command syntax of the main
analysis and synthesis functions. Several examples illustrate the use of the
main functions of FDITOOLS.
",1,0,0,0,0,0
24,Complexity of Deciding Detectability in Discrete Event Systems,"  Detectability of discrete event systems (DESs) is a question whether the
current and subsequent states can be determined based on observations. Shu and
Lin designed a polynomial-time algorithm to check strong (periodic)
detectability and an exponential-time (polynomial-space) algorithm to check
weak (periodic) detectability. Zhang showed that checking weak (periodic)
detectability is PSpace-complete. This intractable complexity opens a question
whether there are structurally simpler DESs for which the problem is tractable.
In this paper, we show that it is not the case by considering DESs represented
as deterministic finite automata without non-trivial cycles, which are
structurally the simplest deadlock-free DESs. We show that even for such very
simple DESs, checking weak (periodic) detectability remains intractable. On the
contrary, we show that strong (periodic) detectability of DESs can be
efficiently verified on a parallel computer.
",1,0,0,0,0,0
25,The Knaster-Tarski theorem versus monotone nonexpansive mappings,"  Let $X$ be a partially ordered set with the property that each family of
order intervals of the form $[a,b],[a,\rightarrow )$ with the finite
intersection property has a nonempty intersection. We show that every directed
subset of $X$ has a supremum. Then we apply the above result to prove that if
$X$ is a topological space with a partial order $\preceq $ for which the order
intervals are compact, $\mathcal{F}$ a nonempty commutative family of monotone
maps from $X$ into $X$ and there exists $c\in X$ such that $c\preceq Tc$ for
every $T\in \mathcal{F}$, then the set of common fixed points of $\mathcal{F}$
is nonempty and has a maximal element. The result, specialized to the case of
Banach spaces gives a general fixed point theorem that drops almost all
assumptions from the recent results in this area. An application to the theory
of integral equations of Urysohn's type is also given.
",0,0,1,0,0,0
26,Efficient methods for computing integrals in electronic structure calculations,"  Efficient methods are proposed, for computing integrals appeaing in
electronic structure calculations. The methods consist of two parts: the first
part is to represent the integrals as contour integrals and the second one is
to evaluate the contour integrals by the Clenshaw-Curtis quadrature. The
efficiency of the proposed methods is demonstrated through numerical
experiments.
",0,1,0,0,0,0
27,Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source,"  We present a novel sound localization algorithm for a non-line-of-sight
(NLOS) sound source in indoor environments. Our approach exploits the
diffraction properties of sound waves as they bend around a barrier or an
obstacle in the scene. We combine a ray tracing based sound propagation
algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate
bending effects by placing a virtual sound source on a wedge in the
environment. We precompute the wedges of a reconstructed mesh of an indoor
scene and use them to generate diffraction acoustic rays to localize the 3D
position of the source. Our method identifies the convergence region of those
generated acoustic rays as the estimated source position based on a particle
filter. We have evaluated our algorithm in multiple scenarios consisting of a
static and dynamic NLOS sound source. In our tested cases, our approach can
localize a source position with an average accuracy error, 0.7m, measured by
the L2 distance between estimated and actual source locations in a 7m*7m*3m
room. Furthermore, we observe 37% to 130% improvement in accuracy over a
state-of-the-art localization method that does not model diffraction effects,
especially when a sound source is not visible to the robot.
",1,0,0,0,0,0
28,"Jacob's ladders, crossbreeding in the set of $ζ$-factorization formulas and selection of families of $ζ$-kindred real continuous functions","  In this paper we introduce the notion of $\zeta$-crossbreeding in a set of
$\zeta$-factorization formulas and also the notion of complete hybrid formula
as the final result of that crossbreeding. The last formula is used as a
criterion for selection of families of $\zeta$-kindred elements in class of
real continuous functions.
Dedicated to recalling of Gregory Mendel's pea-crossbreeding.
",0,0,1,0,0,0
29,Minimax Estimation of the $L_1$ Distance,"  We consider the problem of estimating the $L_1$ distance between two discrete
probability measures $P$ and $Q$ from empirical data in a nonasymptotic and
large alphabet setting. When $Q$ is known and one obtains $n$ samples from $P$,
we show that for every $Q$, the minimax rate-optimal estimator with $n$ samples
achieves performance comparable to that of the maximum likelihood estimator
(MLE) with $n\ln n$ samples. When both $P$ and $Q$ are unknown, we construct
minimax rate-optimal estimators whose worst case performance is essentially
that of the known $Q$ case with $Q$ being uniform, implying that $Q$ being
uniform is essentially the most difficult case. The \emph{effective sample size
enlargement} phenomenon, identified in Jiao \emph{et al.} (2015), holds both in
the known $Q$ case for every $Q$ and the $Q$ unknown case. However, the
construction of optimal estimators for $\|P-Q\|_1$ requires new techniques and
insights beyond the approximation-based method of functional estimation in Jiao
\emph{et al.} (2015).
",0,0,1,1,0,0
30,Density large deviations for multidimensional stochastic hyperbolic conservation laws,"  We investigate the density large deviation function for a multidimensional
conservation law in the vanishing viscosity limit, when the probability
concentrates on weak solutions of a hyperbolic conservation law conservation
law. When the conductivity and dif-fusivity matrices are proportional, i.e. an
Einstein-like relation is satisfied, the problem has been solved in [4]. When
this proportionality does not hold, we compute explicitly the large deviation
function for a step-like density profile, and we show that the associated
optimal current has a non trivial structure. We also derive a lower bound for
the large deviation function, valid for a general weak solution, and leave the
general large deviation function upper bound as a conjecture.
",0,1,1,0,0,0
31,mixup: Beyond Empirical Risk Minimization,"  Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
",1,0,0,1,0,0
32,Equality of the usual definitions of Brakke flow,"  In 1978 Brakke introduced the mean curvature flow in the setting of geometric
measure theory. There exist multiple variants of the original definition. Here
we prove that most of them are indeed equal. One central point is to correct
the proof of Brakke's §3.5, where he develops an estimate for the evolution
of the measure of time-dependent test functions.
",0,0,1,0,0,0
33,Dynamic Base Station Repositioning to Improve Spectral Efficiency of Drone Small Cells,"  With recent advancements in drone technology, researchers are now considering
the possibility of deploying small cells served by base stations mounted on
flying drones. A major advantage of such drone small cells is that the
operators can quickly provide cellular services in areas of urgent demand
without having to pre-install any infrastructure. Since the base station is
attached to the drone, technically it is feasible for the base station to
dynamic reposition itself in response to the changing locations of users for
reducing the communication distance, decreasing the probability of signal
blocking, and ultimately increasing the spectral efficiency. In this paper, we
first propose distributed algorithms for autonomous control of drone movements,
and then model and analyse the spectral efficiency performance of a drone small
cell to shed new light on the fundamental benefits of dynamic repositioning. We
show that, with dynamic repositioning, the spectral efficiency of drone small
cells can be increased by nearly 100\% for realistic drone speed, height, and
user traffic model and without incurring any major increase in drone energy
consumption.
",1,0,0,0,0,0
34,An Unsupervised Homogenization Pipeline for Clustering Similar Patients using Electronic Health Record Data,"  Electronic health records (EHR) contain a large variety of information on the
clinical history of patients such as vital signs, demographics, diagnostic
codes and imaging data. The enormous potential for discovery in this rich
dataset is hampered by its complexity and heterogeneity.
We present the first study to assess unsupervised homogenization pipelines
designed for EHR clustering. To identify the optimal pipeline, we tested
accuracy on simulated data with varying amounts of redundancy, heterogeneity,
and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
Z-scoring, and Deep Autoencoders.
",0,0,0,0,1,0
35,Deep Neural Network Optimized to Resistive Memory with Nonlinear Current-Voltage Characteristics,"  Artificial Neural Network computation relies on intensive vector-matrix
multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array
showed a feasibility of implementing such operations with high energy
efficiency, thus there are many works on efficiently utilizing emerging NVM
crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V
characteristics restrain critical design parameters, such as the read voltage
and weight range, resulting in substantial accuracy loss. In this paper,
instead of optimizing hardware parameters to a given neural network, we propose
a methodology of reconstructing a neural network itself optimized to resistive
memory crossbar arrays. To verify the validity of the proposed method, we
simulated various neural network with MNIST and CIFAR-10 dataset using two
different specific Resistive Random Access Memory (RRAM) model. Simulation
results show that our proposed neural network produces significantly higher
inference accuracies than conventional neural network when the synapse devices
have nonlinear I-V characteristics.
",1,0,0,0,0,0
36,Rate-Distortion Region of a Gray-Wyner Model with Side Information,"  In this work, we establish a full single-letter characterization of the
rate-distortion region of an instance of the Gray-Wyner model with side
information at the decoders. Specifically, in this model an encoder observes a
pair of memoryless, arbitrarily correlated, sources $(S^n_1,S^n_2)$ and
communicates with two receivers over an error-free rate-limited link of
capacity $R_0$, as well as error-free rate-limited individual links of
capacities $R_1$ to the first receiver and $R_2$ to the second receiver. Both
receivers reproduce the source component $S^n_2$ losslessly; and Receiver $1$
also reproduces the source component $S^n_1$ lossily, to within some prescribed
fidelity level $D_1$. Also, Receiver $1$ and Receiver $2$ are equipped
respectively with memoryless side information sequences $Y^n_1$ and $Y^n_2$.
Important in this setup, the side information sequences are arbitrarily
correlated among them, and with the source pair $(S^n_1,S^n_2)$; and are not
assumed to exhibit any particular ordering. Furthermore, by specializing the
main result to two Heegard-Berger models with successive refinement and
scalable coding, we shed light on the roles of the common and private
descriptions that the encoder should produce and what they should carry
optimally. We develop intuitions by analyzing the developed single-letter
optimal rate-distortion regions of these models, and discuss some insightful
binary examples.
",1,0,1,0,0,0
37,Fourier-based numerical approximation of the Weertman equation for moving dislocations,"  This work discusses the numerical approximation of a nonlinear
reaction-advection-diffusion equation, which is a dimensionless form of the
Weertman equation. This equation models steadily-moving dislocations in
materials science. It reduces to the celebrated Peierls-Nabarro equation when
its advection term is set to zero. The approach rests on considering a
time-dependent formulation, which admits the equation under study as its
long-time limit. Introducing a Preconditioned Collocation Scheme based on
Fourier transforms, the iterative numerical method presented solves the
time-dependent problem, delivering at convergence the desired numerical
solution to the Weertman equation. Although it rests on an explicit
time-evolution scheme, the method allows for large time steps, and captures the
solution in a robust manner. Numerical results illustrate the efficiency of the
approach for several types of nonlinearities.
",0,1,0,0,0,0
38,Design Decisions for Weave: A Real-Time Web-based Collaborative Visualization Framework,"  There are many web-based visualization systems available to date, each having
its strengths and limitations. The goals these systems set out to accomplish
influence design decisions and determine how reusable and scalable they are.
Weave is a new web-based visualization platform with the broad goal of enabling
visualization of any available data by anyone for any purpose. Our open source
framework supports highly interactive linked visualizations for users of
varying skill levels. What sets Weave apart from other systems is its
consideration for real-time remote collaboration with session history. We
provide a detailed account of the various framework designs we considered with
comparisons to existing state-of-the-art systems.
",1,0,0,0,0,0
39,Suzaku Analysis of the Supernova Remnant G306.3-0.9 and the Gamma-ray View of Its Neighborhood,"  We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using
archival multi-wavelength data. The Suzaku spectra are well described by
two-component thermal plasma models: The soft component is in ionization
equilibrium and has a temperature $\sim$0.59 keV, while the hard component has
temperature $\sim$3.2 keV and ionization time-scale $\sim$$2.6\times10^{10}$
cm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\sim$6.5 keV
from this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the
X-ray emission has an ejecta origin. The centroid energy of the Fe-K line
supports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than
a core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its
surrounding were analyzed using about 6 years of Fermi data. We report about
the non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray
source in the south-west of G306.3$-$0.9 with a significance of
$\sim$13$\sigma$. We discuss several scenarios for these results with the help
of data from other wavebands to understand the SNR and its neighborhood.
",0,1,0,0,0,0
40,Japanese Sentiment Classification using a Tree-Structured Long Short-Term Memory with Attention,"  Previous approaches to training syntax-based sentiment classification models
required phrase-level annotated corpora, which are not readily available in
many languages other than English. Thus, we propose the use of tree-structured
Long Short-Term Memory with an attention mechanism that pays attention to each
subtree of the parse tree. Experimental results indicate that our model
achieves the state-of-the-art performance in a Japanese sentiment
classification task.
",1,0,0,0,0,0
41,"Covariances, Robustness, and Variational Bayes","  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale datasets. However, even when MFVB provides accurate posterior means
for certain parameters, it often mis-estimates variances and covariances.
Furthermore, prior robustness measures have remained undeveloped for MFVB. By
deriving a simple formula for the effect of infinitesimal model perturbations
on MFVB posterior means, we provide both improved covariance estimates and
local robustness measures for MFVB, thus greatly expanding the practical
usefulness of MFVB posterior approximations. The estimates for MFVB posterior
covariances rely on a result from the classical Bayesian robustness literature
relating derivatives of posterior expectations to posterior covariances and
include the Laplace approximation as a special case. Our key condition is that
the MFVB approximation provides good estimates of a select subset of posterior
means---an assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness
measures with runtimes that can be an order of magnitude faster than MCMC.
",0,0,0,1,0,0
42,Are multi-factor Gaussian term structure models still useful? An empirical analysis on Italian BTPs,"  In this paper, we empirically study models for pricing Italian sovereign
bonds under a reduced form framework, by assuming different dynamics for the
short-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek
multi-factor models, with a focus on optimization algorithms applied in the
calibration exercise. The Kalman filter algorithm together with a maximum
likelihood estimation method are considered to fit the Italian term-structure
over a 12-year horizon, including the global financial crisis and the euro area
sovereign debt crisis. Analytic formulas for the gradient vector and the
Hessian matrix of the likelihood function are provided.
",0,0,0,0,0,1
43,Probing valley filtering effect by Andreev reflection in zigzag graphene nanoribbon,"  Ballistic point contact (BPC) with zigzag edges in graphene is a main
candidate of a valley filter, in which the polarization of the valley degree of
freedom can be selected by using a local gate voltage. Here, we propose to
detect the valley filtering effect by Andreev reflection. Because electrons in
the lowest conduction band and the highest valence band of the BPC possess
opposite chirality, the inter-band Andreev reflection is strongly suppressed,
after multiple scattering and interference. We draw this conclusion by both the
scattering matrix analysis and the numerical simulation. The Andreev reflection
as a function of the incident energy of electrons and the local gate voltage at
the BPC is obtained, by which the parameter region for a perfect valley filter
and the direction of valley polarization can be determined. The Andreev
reflection exhibits an oscillatory decay with the length of the BPC, indicating
a negative correlation to valley polarization.
",0,1,0,0,0,0
44,Generalized Approximate Message-Passing Decoder for Universal Sparse Superposition Codes,"  Sparse superposition (SS) codes were originally proposed as a
capacity-achieving communication scheme over the additive white Gaussian noise
channel (AWGNC) [1]. Very recently, it was discovered that these codes are
universal, in the sense that they achieve capacity over any memoryless channel
under generalized approximate message-passing (GAMP) decoding [2], although
this decoder has never been stated for SS codes. In this contribution we
introduce the GAMP decoder for SS codes, we confirm empirically the
universality of this communication scheme through its study on various channels
and we provide the main analysis tools: state evolution and potential. We also
compare the performance of GAMP with the Bayes-optimal MMSE decoder. We
empirically illustrate that despite the presence of a phase transition
preventing GAMP to reach the optimal performance, spatial coupling allows to
boost the performance that eventually tends to capacity in a proper limit. We
also prove that, in contrast with the AWGNC case, SS codes for binary input
channels have a vanishing error floor in the limit of large codewords.
Moreover, the performance of Hadamard-based encoders is assessed for practical
implementations.
",1,0,1,0,0,0
45,LAAIR: A Layered Architecture for Autonomous Interactive Robots,"  When developing general purpose robots, the overarching software architecture
can greatly affect the ease of accomplishing various tasks. Initial efforts to
create unified robot systems in the 1990s led to hybrid architectures,
emphasizing a hierarchy in which deliberative plans direct the use of reactive
skills. However, since that time there has been significant progress in the
low-level skills available to robots, including manipulation and perception,
making it newly feasible to accomplish many more tasks in real-world domains.
There is thus renewed optimism that robots will be able to perform a wide array
of tasks while maintaining responsiveness to human operators. However, the top
layer in traditional hybrid architectures, designed to achieve long-term goals,
can make it difficult to react quickly to human interactions during goal-driven
execution. To mitigate this difficulty, we propose a novel architecture that
supports such transitions by adding a top-level reactive module which has
flexible access to both reactive skills and a deliberative control module. To
validate this architecture, we present a case study of its application on a
domestic service robot platform.
",1,0,0,0,0,0
46,3D Human Pose Estimation in RGBD Images for Robotic Task Learning,"  We propose an approach to estimate 3D human pose in real world units from a
single RGBD image and show that it exceeds performance of monocular 3D pose
estimation approaches from color as well as pose estimation exclusively from
depth. Our approach builds on robust human keypoint detectors for color images
and incorporates depth for lifting into 3D. We combine the system with our
learning from demonstration framework to instruct a service robot without the
need of markers. Experiments in real world settings demonstrate that our
approach enables a PR2 robot to imitate manipulation actions observed from a
human teacher.
",1,0,0,0,0,0
47,Simultaneous non-vanishing for Dirichlet L-functions,"  We extend the work of Fouvry, Kowalski and Michel on correlation between
Hecke eigenvalues of modular forms and algebraic trace functions in order to
establish an asymptotic formula for a generalized cubic moment of modular
L-functions at the central point s = 1/2 and for prime moduli q. As an
application, we exploit our recent result on the mollification of the fourth
moment of Dirichlet L-functions to derive that for any pair
$(\omega_1,\omega_2)$ of multiplicative characters modulo q, there is a
positive proportion of $\chi$ (mod q) such that $L(\chi, 1/2 ), L(\chi\omega_1,
1/2 )$ and $L(\chi\omega_2, 1/2)$ are simultaneously not too small.
",0,0,1,0,0,0
48,Wehrl Entropy Based Quantification of Nonclassicality for Single Mode Quantum Optical States,"  Nonclassical states of a quantized light are described in terms of
Glauber-Sudarshan P distribution which is not a genuine classical probability
distribution. Despite several attempts, defining a uniform measure of
nonclassicality (NC) for the single mode quantum states of light is yet an open
task. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that
the existing well-known measures fail to quantify the NC of single mode states
that are generated under multiple NC-inducing operations. Recently, Ivan et.
al. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of
non-Gaussian character of quantum optical states in terms of Wehrl entropy.
Here, we adopt this concept in the context of single mode NC. In this paper, we
propose a new quantification of NC for the single mode quantum states of light
as the difference between the total Wehrl entropy of the state and the maximum
Wehrl entropy arising due to its classical characteristics. This we achieve by
subtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any
classical state that has same randomness as measured in terms of von-Neumann
entropy. We obtain analytic expressions of NC for most of the states, in
particular, all pure states and Gaussian mixed states. However, the evaluation
of NC for the non-Gaussian mixed states is subject to extensive numerical
computation that lies beyond the scope of the current work. We show that, along
with the states generated under single NC-inducing operations, also for the
broader class of states that are generated under multiple NC-inducing
operations, our quantification enumerates the NC consistently.
",1,1,0,0,0,0
49,Attention-based Natural Language Person Retrieval,"  Following the recent progress in image classification and captioning using
deep learning, we develop a novel natural language person retrieval system
based on an attention mechanism. More specifically, given the description of a
person, the goal is to localize the person in an image. To this end, we first
construct a benchmark dataset for natural language person retrieval. To do so,
we generate bounding boxes for persons in a public image dataset from the
segmentation masks, which are then annotated with descriptions and attributes
using the Amazon Mechanical Turk. We then adopt a region proposal network in
Faster R-CNN as a candidate region generator. The cropped images based on the
region proposals as well as the whole images with attention weights are fed
into Convolutional Neural Networks for visual feature extraction, while the
natural language expression and attributes are input to Bidirectional Long
Short- Term Memory (BLSTM) models for text feature extraction. The visual and
text features are integrated to score region proposals, and the one with the
highest score is retrieved as the output of our system. The experimental
results show significant improvement over the state-of-the-art method for
generic object retrieval and this line of research promises to benefit search
in surveillance video footage.
",1,0,0,0,0,0
50,Large Scale Automated Forecasting for Monitoring Network Safety and Security,"  Real time large scale streaming data pose major challenges to forecasting, in
particular defying the presence of human experts to perform the corresponding
analysis. We present here a class of models and methods used to develop an
automated, scalable and versatile system for large scale forecasting oriented
towards safety and security monitoring. Our system provides short and long term
forecasts and uses them to detect safety and security issues in relation with
multiple internet connected devices well in advance they might take place.
",0,0,0,1,0,0
51,Contextual Regression: An Accurate and Conveniently Interpretable Nonlinear Model for Mining Discovery from Scientific Data,"  Machine learning algorithms such as linear regression, SVM and neural network
have played an increasingly important role in the process of scientific
discovery. However, none of them is both interpretable and accurate on
nonlinear datasets. Here we present contextual regression, a method that joins
these two desirable properties together using a hybrid architecture of neural
network embedding and dot product layer. We demonstrate its high prediction
accuracy and sensitivity through the task of predictive feature selection on a
simulated dataset and the application of predicting open chromatin sites in the
human genome. On the simulated data, our method achieved high fidelity recovery
of feature contributions under random noise levels up to 200%. On the open
chromatin dataset, the application of our method not only outperformed the
state of the art method in terms of accuracy, but also unveiled two previously
unfound open chromatin related histone marks. Our method can fill the blank of
accurate and interpretable nonlinear modeling in scientific data mining tasks.
",1,0,0,1,0,0
52,Multi-time correlators in continuous measurement of qubit observables,"  We consider multi-time correlators for output signals from linear detectors,
continuously measuring several qubit observables at the same time. Using the
quantum Bayesian formalism, we show that for unital (symmetric) evolution in
the absence of phase backaction, an $N$-time correlator can be expressed as a
product of two-time correlators when $N$ is even. For odd $N$, there is a
similar factorization, which also includes a single-time average. Theoretical
predictions agree well with experimental results for two detectors, which
simultaneously measure non-commuting qubit observables.
",0,1,0,0,0,0
53,"Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey","  Constraint Handling Rules is an effective concurrent declarative programming
language and a versatile computational logic formalism. CHR programs consist of
guarded reactive rules that transform multisets of constraints. One of the main
features of CHR is its inherent concurrency. Intuitively, rules can be applied
to parts of a multiset in parallel. In this comprehensive survey, we give an
overview of concurrent and parallel as well as distributed CHR semantics,
standard and more exotic, that have been proposed over the years at various
levels of refinement. These semantics range from the abstract to the concrete.
They are related by formal soundness results. Their correctness is established
as correspondence between parallel and sequential computations. We present
common concise sample CHR programs that have been widely used in experiments
and benchmarks. We review parallel CHR implementations in software and
hardware. The experimental results obtained show a consistent parallel speedup.
Most implementations are available online. The CHR formalism can also be used
to implement and reason with models for concurrency. To this end, the Software
Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus
have been faithfully encoded in CHR. Under consideration in Theory and Practice
of Logic Programming (TPLP).
",1,0,0,0,0,0
54,Robustness against the channel effect in pathological voice detection,"  Many people are suffering from voice disorders, which can adversely affect
the quality of their lives. In response, some researchers have proposed
algorithms for automatic assessment of these disorders, based on voice signals.
However, these signals can be sensitive to the recording devices. Indeed, the
channel effect is a pervasive problem in machine learning for healthcare. In
this study, we propose a detection system for pathological voice, which is
robust against the channel effect. This system is based on a bidirectional LSTM
network. To increase the performance robustness against channel mismatch, we
integrate domain adversarial training (DAT) to eliminate the differences
between the devices. When we train on data recorded on a high-quality
microphone and evaluate on smartphone data without labels, our robust detection
system increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target
sample labels). To the best of our knowledge, this is the first study applying
unsupervised domain adaptation to pathological voice detection. Notably, our
system does not need target device sample labels, which allows for
generalization to many new devices.
",1,0,0,0,0,0
55,An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers,"  Computing a basis for the exponent lattice of algebraic numbers is a basic
problem in the field of computational number theory with applications to many
other areas. The main cost of a well-known algorithm
\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
computing the primitive element of the extended field generated by the given
algebraic numbers. When the extended field is of large degree, the problem
seems intractable by the tool implementing the algorithm. In this paper, a
special kind of exponent lattice basis is introduced. An important feature of
the basis is that it can be inductively constructed, which allows us to deal
with the given algebraic numbers one by one when computing the basis. Based on
this, an effective framework for constructing exponent lattice basis is
proposed. Through computing a so-called pre-basis first and then solving some
linear Diophantine equations, the basis can be efficiently constructed. A new
certificate for multiplicative independence and some techniques for decreasing
degrees of algebraic numbers are provided to speed up the computation. The new
algorithm has been implemented with Mathematica and its effectiveness is
verified by testing various examples. Moreover, the algorithm is applied to
program verification for finding invariants of linear loops.
",1,0,0,0,0,0
56,Competing evolutionary paths in growing populations with applications to multidrug resistance,"  Investigating the emergence of a particular cell type is a recurring theme in
models of growing cellular populations. The evolution of resistance to therapy
is a classic example. Common questions are: when does the cell type first
occur, and via which sequence of steps is it most likely to emerge? For growing
populations, these questions can be formulated in a general framework of
branching processes spreading through a graph from a root to a target vertex.
Cells have a particular fitness value on each vertex and can transition along
edges at specific rates. Vertices represents cell states, say \mic{genotypes
}or physical locations, while possible transitions are acquiring a mutation or
cell migration. We focus on the setting where cells at the root vertex have the
highest fitness and transition rates are small. Simple formulas are derived for
the time to reach the target vertex and for the probability that it is reached
along a given path in the graph. We demonstrate our results on \mic{several
scenarios relevant to the emergence of drug resistance}, including: the
orderings of resistance-conferring mutations in bacteria and the impact of
imperfect drug penetration in cancer.
",0,0,0,0,1,0
57,Transient flows in active porous media,"  Stimuli-responsive materials that modify their shape in response to changes
in environmental conditions -- such as solute concentration, temperature, pH,
and stress -- are widespread in nature and technology. Applications include
micro- and nanoporous materials used in filtration and flow control. The
physiochemical mechanisms that induce internal volume modifications have been
widely studies. The coupling between induced volume changes and solute
transport through porous materials, however, is not well understood. Here, we
consider advective and diffusive transport through a small channel linking two
large reservoirs. A section of stimulus-responsive material regulates the
channel permeability, which is a function of the local solute concentration. We
derive an exact solution to the coupled transport problem and demonstrate the
existence of a flow regime in which the steady state is reached via a damped
oscillation around the equilibrium concentration value. Finally, the
feasibility of an experimental observation of the phenomena is discussed.
Please note that this version of the paper has not been formally peer reviewed,
revised or accepted by a journal.
",0,1,0,0,0,0
58,An information model for modular robots: the Hardware Robot Information Model (HRIM),"  Today's landscape of robotics is dominated by vertical integration where
single vendors develop the final product leading to slow progress, expensive
products and customer lock-in. Opposite to this, an horizontal integration
would result in a rapid development of cost-effective mass-market products with
an additional consumer empowerment. The transition of an industry from vertical
integration to horizontal integration is typically catalysed by de facto
industry standards that enable a simplified and seamless integration of
products. However, in robotics there is currently no leading candidate for a
global plug-and-play standard.
This paper tackles the problem of incompatibility between robot components
that hinder the reconfigurability and flexibility demanded by the robotics
industry. Particularly, it presents a model to create plug-and-play robot
hardware components. Rather than iteratively evolving previous ontologies, our
proposed model answers the needs identified by the industry while facilitating
interoperability, measurability and comparability of robotics technology. Our
approach differs significantly with the ones presented before as it is
hardware-oriented and establishes a clear set of actions towards the
integration of this model in real environments and with real manufacturers.
",1,0,0,0,0,0
59,Detecting Adversarial Samples Using Density Ratio Estimates,"  Machine learning models, especially based on deep architectures are used in
everyday applications ranging from self driving cars to medical diagnostics. It
has been shown that such models are dangerously susceptible to adversarial
samples, indistinguishable from real samples to human eye, adversarial samples
lead to incorrect classifications with high confidence. Impact of adversarial
samples is far-reaching and their efficient detection remains an open problem.
We propose to use direct density ratio estimation as an efficient model
agnostic measure to detect adversarial samples. Our proposed method works
equally well with single and multi-channel samples, and with different
adversarial sample generation methods. We also propose a method to use density
ratio estimates for generating adversarial samples with an added constraint of
preserving density ratio.
",1,0,0,1,0,0
60,The Query Complexity of Cake Cutting,"  We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.
",1,0,0,0,0,0
61,Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition,"  This paper studies the emotion recognition from musical tracks in the
2-dimensional valence-arousal (V-A) emotional space. We propose a method based
on convolutional (CNN) and recurrent neural networks (RNN), having
significantly fewer parameters compared with the state-of-the-art method for
the same task. We utilize one CNN layer followed by two branches of RNNs
trained separately for arousal and valence. The method was evaluated using the
'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for
arousal and 0.268 for valence, which is the best result reported on this
dataset.
",1,0,0,0,0,0
62,Timed Automata with Polynomial Delay and their Expressiveness,"  We consider previous models of Timed, Probabilistic and Stochastic Timed
Automata, we introduce our model of Timed Automata with Polynomial Delay and we
characterize the expressiveness of these models relative to each other.
",1,0,0,0,0,0
63,Superconducting properties of Cu intercalated Bi$_2$Se$_3$ studied by Muon Spin Spectroscopy,"  We present muon spin rotation measurements on superconducting Cu intercalated
Bi$_2$Se$_3$, which was suggested as a realization of a topological
superconductor. We observe a clear evidence of the superconducting transition
below 4 K, where the width of magnetic field distribution increases as the
temperature is decreased. The measured broadening at mK temperatures suggests a
large London penetration depth in the $ab$ plane ($\lambda_{\mathrm{eff}}\sim
1.6$ $\mathrm{\mu}$m). We show that the temperature dependence of this
broadening follows the BCS prediction, but could be consistent with several gap
symmetries.
",0,1,0,0,0,0
64,Time-domain THz spectroscopy reveals coupled protein-hydration dielectric response in solutions of native and fibrils of human lyso-zyme,"  Here we reveal details of the interaction between human lysozyme proteins,
both native and fibrils, and their water environment by intense terahertz time
domain spectroscopy. With the aid of a rigorous dielectric model, we determine
the amplitude and phase of the oscillating dipole induced by the THz field in
the volume containing the protein and its hydration water. At low
concentrations, the amplitude of this induced dipolar response decreases with
increasing concentration. Beyond a certain threshold, marking the onset of the
interactions between the extended hydration shells, the amplitude remains fixed
but the phase of the induced dipolar response, which is initially in phase with
the applied THz field, begins to change. The changes observed in the THz
response reveal protein-protein interactions me-diated by extended hydration
layers, which may control fibril formation and may have an important role in
chemical recognition phenomena.
",0,1,0,0,0,0
65,Inversion of Qubit Energy Levels in Qubit-Oscillator Circuits in the Deep-Strong-Coupling Regime,"  We report on experimentally measured light shifts of superconducting flux
qubits deep-strongly coupled to LC oscillators, where the coupling constants
are comparable to the qubit and oscillator resonance frequencies. By using
two-tone spectroscopy, the energies of the six lowest levels of each circuit
are determined. We find huge Lamb shifts that exceed 90% of the bare qubit
frequencies and inversions of the qubits' ground and excited states when there
are a finite number of photons in the oscillator. Our experimental results
agree with theoretical predictions based on the quantum Rabi model.
",0,1,0,0,0,0
66,Deep Multiple Instance Feature Learning via Variational Autoencoder,"  We describe a novel weakly supervised deep learning framework that combines
both the discriminative and generative models to learn meaningful
representation in the multiple instance learning (MIL) setting. MIL is a weakly
supervised learning problem where labels are associated with groups of
instances (referred as bags) instead of individual instances. To address the
essential challenge in MIL problems raised from the uncertainty of positive
instances label, we use a discriminative model regularized by variational
autoencoders (VAEs) to maximize the differences between latent representations
of all instances and negative instances. As a result, the hidden layer of the
variational autoencoder learns meaningful representation. This representation
can effectively be used for MIL problems as illustrated by better performance
on the standard benchmark datasets comparing to the state-of-the-art
approaches. More importantly, unlike most related studies, the proposed
framework can be easily scaled to large dataset problems, as illustrated by the
audio event detection and segmentation task. Visualization also confirms the
effectiveness of the latent representation in discriminating positive and
negative classes.
",0,0,0,1,0,0
67,Regularity of envelopes in Kähler classes,"  We establish the C^{1,1} regularity of quasi-psh envelopes in a Kahler class,
confirming a conjecture of Berman.
",0,0,1,0,0,0
68,$S^1$-equivariant Index theorems and Morse inequalities on complex manifolds with boundary,"  Let $M$ be a complex manifold of dimension $n$ with smooth connected boundary
$X$. Assume that $\overline M$ admits a holomorphic $S^1$-action preserving the
boundary $X$ and the $S^1$-action is transversal and CR on $X$. We show that
the $\overline\partial$-Neumann Laplacian on $M$ is transversally elliptic and
as a consequence, the $m$-th Fourier component of the $q$-th Dolbeault
cohomology group $H^q_m(\overline M)$ is finite dimensional, for every
$m\in\mathbb Z$ and every $q=0,1,\ldots,n$. This enables us to define
$\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ the $m$-th Fourier
component of the Euler characteristic on $M$ and to study large $m$-behavior of
$H^q_m(\overline M)$. In this paper, we establish an index formula for
$\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ and Morse inequalities for
$H^q_m(\overline M)$.
",0,0,1,0,0,0
69,Internal Model from Observations for Reward Shaping,"  Reinforcement learning methods require careful design involving a reward
function to obtain the desired action policy for a given task. In the absence
of hand-crafted reward functions, prior work on the topic has proposed several
methods for reward estimation by using expert state trajectories and action
pairs. However, there are cases where complete or good action information
cannot be obtained from expert demonstrations. We propose a novel reinforcement
learning method in which the agent learns an internal model of observation on
the basis of expert-demonstrated state trajectories to estimate rewards without
completely learning the dynamics of the external environment from state-action
pairs. The internal model is obtained in the form of a predictive model for the
given expert state distribution. During reinforcement learning, the agent
predicts the reward as a function of the difference between the actual state
and the state predicted by the internal model. We conducted multiple
experiments in environments of varying complexity, including the Super Mario
Bros and Flappy Bird games. We show our method successfully trains good
policies directly from expert game-play videos.
",1,0,0,1,0,0
70,Characterizations of quasitrivial symmetric nondecreasing associative operations,"  In this paper we are interested in the class of n-ary operations on an
arbitrary chain that are quasitrivial, symmetric, nondecreasing, and
associative. We first provide a description of these operations. We then prove
that associativity can be replaced with bisymmetry in the definition of this
class. Finally we investigate the special situation where the chain is finite.
",0,0,1,0,0,0
71,Multivariate Dependency Measure based on Copula and Gaussian Kernel,"  We propose a new multivariate dependency measure. It is obtained by
considering a Gaussian kernel based distance between the copula transform of
the given d-dimensional distribution and the uniform copula and then
appropriately normalizing it. The resulting measure is shown to satisfy a
number of desirable properties. A nonparametric estimate is proposed for this
dependency measure and its properties (finite sample as well as asymptotic) are
derived. Some comparative studies of the proposed dependency measure estimate
with some widely used dependency measure estimates on artificial datasets are
included. A non-parametric test of independence between two or more random
variables based on this measure is proposed. A comparison of the proposed test
with some existing nonparametric multivariate test for independence is
presented.
",0,0,1,1,0,0
72,The nature of the tensor order in Cd2Re2O7,"  The pyrochlore metal Cd2Re2O7 has been recently investigated by
second-harmonic generation (SHG) reflectivity. In this paper, we develop a
general formalism that allows for the identification of the relevant tensor
components of the SHG from azimuthal scans. We demonstrate that the secondary
order parameter identified by SHG at the structural phase transition is the
x2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2
symmetry of the atomic displacements associated with the I-4m2 crystal
structure that was previously thought to be its origin. Within the same
formalism, we suggest that the primary order parameter detected in the SHG
experiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the
general mechanism driving the phase transition in our proposed framework, and
suggest experiments, particularly resonant X-ray scattering ones, that could
clarify this issue.
",0,1,0,0,0,0
73,Efficient and consistent inference of ancestral sequences in an evolutionary model with insertions and deletions under dense taxon sampling,"  In evolutionary biology, the speciation history of living organisms is
represented graphically by a phylogeny, that is, a rooted tree whose leaves
correspond to current species and branchings indicate past speciation events.
Phylogenies are commonly estimated from molecular sequences, such as DNA
sequences, collected from the species of interest. At a high level, the idea
behind this inference is simple: the further apart in the Tree of Life are two
species, the greater is the number of mutations to have accumulated in their
genomes since their most recent common ancestor. In order to obtain accurate
estimates in phylogenetic analyses, it is standard practice to employ
statistical approaches based on stochastic models of sequence evolution on a
tree. For tractability, such models necessarily make simplifying assumptions
about the evolutionary mechanisms involved. In particular, commonly omitted are
insertions and deletions of nucleotides -- also known as indels.
Properly accounting for indels in statistical phylogenetic analyses remains a
major challenge in computational evolutionary biology. Here we consider the
problem of reconstructing ancestral sequences on a known phylogeny in a model
of sequence evolution incorporating nucleotide substitutions, insertions and
deletions, specifically the classical TKF91 process. We focus on the case of
dense phylogenies of bounded height, which we refer to as the taxon-rich
setting, where statistical consistency is achievable. We give the first
polynomial-time ancestral reconstruction algorithm with provable guarantees
under constant rates of mutation. Our algorithm succeeds when the phylogeny
satisfies the ""big bang"" condition, a necessary and sufficient condition for
statistical consistency in this context.
",1,0,1,1,0,0
74,Flow Characteristics and Cores of Complex Network and Multiplex Type Systems,"  Subject of research is complex networks and network systems. The network
system is defined as a complex network in which flows are moved. Classification
of flows in the network is carried out on the basis of ordering and continuity.
It is shown that complex networks with different types of flows generate
various network systems. Flow analogues of the basic concepts of the theory of
complex networks are introduced and the main problems of this theory in terms
of flow characteristics are formulated. Local and global flow characteristics
of networks bring closer the theory of complex networks to the systems theory
and systems analysis. Concept of flow core of network system is introduced and
defined how it simplifies the process of its investigation. Concepts of kernel
and flow core of multiplex are determined. Features of operation of multiplex
type systems are analyzed.
",1,1,0,0,0,0
75,Pattern-forming fronts in a Swift-Hohenberg equation with directional quenching - parallel and oblique stripes,"  We study the effect of domain growth on the orientation of striped phases in
a Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter
dependence that allows stripe formation in a half plane, and suppresses
patterns in the complement, while the boundary of the pattern-forming region is
propagating with fixed normal velocity. We construct front solutions that leave
behind stripes in the pattern-forming region that are parallel to or at a small
oblique angle to the boundary.
Technically, the construction of stripe formation parallel to the boundary
relies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at
a small oblique angle are constructed using a functional-analytic, perturbative
approach. Here, the main difficulties are the presence of continuous spectrum
and the fact that small oblique angles appear as a singular perturbation in a
traveling-wave problem. We resolve the former difficulty using a farfield-core
decomposition and Fredholm theory in weighted spaces. The singular perturbation
problem is resolved using preconditioners and boot-strapping.
",0,1,0,0,0,0
76,Generalized Minimum Distance Estimators in Linear Regression with Dependent Errors,"  This paper discusses minimum distance estimation method in the linear
regression model with dependent errors which are strongly mixing. The
regression parameters are estimated through the minimum distance estimation
method, and asymptotic distributional properties of the estimators are
discussed. A simulation study compares the performance of the minimum distance
estimator with other well celebrated estimator. This simulation study shows the
superiority of the minimum distance estimator over another estimator. KoulMde
(R package) which was used for the simulation study is available online. See
section 4 for the detail.
",0,0,1,1,0,0
77,Live Service Migration in Mobile Edge Clouds,"  Mobile edge clouds (MECs) bring the benefits of the cloud closer to the user,
by installing small cloud infrastructures at the network edge. This enables a
new breed of real-time applications, such as instantaneous object recognition
and safety assistance in intelligent transportation systems, that require very
low latency. One key issue that comes with proximity is how to ensure that
users always receive good performance as they move across different locations.
Migrating services between MECs is seen as the means to achieve this. This
article presents a layered framework for migrating active service applications
that are encapsulated either in virtual machines (VMs) or containers. This
layering approach allows a substantial reduction in service downtime. The
framework is easy to implement using readily available technologies, and one of
its key advantages is that it supports containers, which is a promising
emerging technology that offers tangible benefits over VMs. The migration
performance of various real applications is evaluated by experiments under the
presented framework. Insights drawn from the experimentation results are
discussed.
",1,0,0,0,0,0
78,Induced density correlations in a sonic black hole condensate,"  Analog black/white hole pairs, consisting of a region of supersonic flow,
have been achieved in a recent experiment by J. Steinhauer using an elongated
Bose-Einstein condensate. A growing standing density wave, and a checkerboard
feature in the density-density correlation function, were observed in the
supersonic region. We model the density-density correlation function, taking
into account both quantum fluctuations and the shot-to-shot variation of atom
number normally present in ultracold-atom experiments. We find that quantum
fluctuations alone produce some, but not all, of the features of the
correlation function, whereas atom-number fluctuation alone can produce all the
observed features, and agreement is best when both are included. In both cases,
the density-density correlation is not intrinsic to the fluctuations, but
rather is induced by modulation of the standing wave caused by the
fluctuations.
",0,1,0,0,0,0
79,Genus growth in $\mathbb{Z}_p$-towers of function fields,"  Let $K$ be a function field over a finite field $k$ of characteristic $p$ and
let $K_{\infty}/K$ be a geometric extension with Galois group $\mathbb{Z}_p$.
Let $K_n$ be the corresponding subextension with Galois group
$\mathbb{Z}/p^n\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple
explicit formula $g_n$ in terms of an explicit Witt vector construction of the
$\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which
is quadratic in $p^n$. Furthermore, we determine all $\mathbb{Z}_p$-towers for
which the genus sequence is stable, in the sense that there are $a,b,c \in
\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus
stable towers are expected to have strong stable arithmetic properties for
their zeta functions. A key technical contribution of this work is a new
simplified formula for the Schmid-Witt symbol coming from local class field
theory.
",0,0,1,0,0,0
80,Topological Phases emerging from Spin-Orbital Physics,"  We study the evolution of spin-orbital correlations in an inhomogeneous
quantum system with an impurity replacing a doublon by a holon orbital degree
of freedom. Spin-orbital entanglement is large when spin correlations are
antiferromagnetic, while for a ferromagnetic host we obtain a pure orbital
description. In this regime the orbital model can be mapped on spinless
fermions and we uncover topological phases with zero energy modes at the edge
or at the domain between magnetically inequivalent regions.
",0,1,0,0,0,0
81,"Accurate and Diverse Sampling of Sequences based on a ""Best of Many"" Sample Objective","  For autonomous agents to successfully operate in the real world, anticipation
of future events and states of their environment is a key competence. This
problem has been formalized as a sequence extrapolation problem, where a number
of observations are used to predict the sequence into the future. Real-world
scenarios demand a model of uncertainty of such predictions, as predictions
become increasingly uncertain -- in particular on long time horizons. While
impressive results have been shown on point estimates, scenarios that induce
multi-modal distributions over future sequences remain challenging. Our work
addresses these challenges in a Gaussian Latent Variable model for sequence
prediction. Our core contribution is a ""Best of Many"" sample objective that
leads to more accurate and more diverse predictions that better capture the
true variations in real-world sequence data. Beyond our analysis of improved
model fit, our models also empirically outperform prior work on three diverse
tasks ranging from traffic scenes to weather data.
",0,0,0,1,0,0
82,Exploring RNN-Transducer for Chinese Speech Recognition,"  End-to-end approaches have drawn much attention recently for significantly
simplifying the construction of an automatic speech recognition (ASR) system.
RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous
studies have shown that RNN-T is difficult to train and a very complex training
process is needed for a reasonable performance. In this paper, we explore RNN-T
for a Chinese large vocabulary continuous speech recognition (LVCSR) task and
aim to simplify the training process while maintaining performance. First, a
new strategy of learning rate decay is proposed to accelerate the model
convergence. Second, we find that adding convolutional layers at the beginning
of the network and using ordered data can discard the pre-training process of
the encoder without loss of performance. Besides, we design experiments to find
a balance among the usage of GPU memory, training circle and model performance.
Finally, we achieve 16.9% character error rate (CER) on our test set which is
2% absolute improvement from a strong BLSTM CE system with language model
trained on the same text corpus.
",1,0,0,0,0,0
83,A Debt-Aware Learning Approach for Resource Adaptations in Cloud Elasticity Management,"  Elasticity is a cloud property that enables applications and its execution
systems to dynamically acquire and release shared computational resources on
demand. Moreover, it unfolds the advantage of economies of scale in the cloud
through a drop in the average costs of these shared resources. However, it is
still an open challenge to achieve a perfect match between resource demand and
provision in autonomous elasticity management. Resource adaptation decisions
essentially involve a trade-off between economics and performance, which
produces a gap between the ideal and actual resource provisioning. This gap, if
not properly managed, can negatively impact the aggregate utility of a cloud
customer in the long run. To address this limitation, we propose a technical
debt-aware learning approach for autonomous elasticity management based on a
reinforcement learning of elasticity debts in resource provisioning; the
adaptation pursues strategic decisions that trades off economics against
performance. We extend CloudSim and Burlap to evaluate our approach. The
evaluation shows that a reinforcement learning of technical debts in elasticity
obtains a higher utility for a cloud customer, while conforming expected levels
of performance.
",1,0,0,0,0,0
84,Semi-simplicial spaces,"  This is an exposition of homotopical results on the geometric realization of
semi-simplicial spaces. We then use these to derive basic foundational results
about classifying spaces of topological categories, possibly without units. The
topics considered include: fibrancy conditions on topological categories; the
effect on classifying spaces of freely adjoining units; approximate notions of
units; Quillen's Theorems A and B for non-unital topological categories; the
effect on classifying spaces of changing the topology on the space of objects;
the Group-Completion Theorem.
",0,0,1,0,0,0
85,"Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis","  Answer Set Programming (ASP) is a well-established declarative paradigm. One
of the successes of ASP is the availability of efficient systems.
State-of-the-art systems are based on the ground+solve approach. In some
applications this approach is infeasible because the grounding of one or few
constraints is expensive. In this paper, we systematically compare alternative
strategies to avoid the instantiation of problematic constraints, that are
based on custom extensions of the solver. Results on real and synthetic
benchmarks highlight some strengths and weaknesses of the different strategies.
(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)
",1,0,0,0,0,0
86,A Unified Approach to Nonlinear Transformation Materials,"  The advances in geometric approaches to optical devices due to transformation
optics has led to the development of cloaks, concentrators, and other devices.
It has also been shown that transformation optics can be used to gravitational
fields from general relativity. However, the technique is currently constrained
to linear devices, as a consistent approach to nonlinearity (including both the
case of a nonlinear background medium and a nonlinear transformation) remains
an open question. Here we show that nonlinearity can be incorporated into
transformation optics in a consistent way. We use this to illustrate a number
of novel effects, including cloaking an optical soliton, modeling nonlinear
solutions to Einstein's field equations, controlling transport in a Debye
solid, and developing a set of constitutive to relations for relativistic
cloaks in arbitrary nonlinear backgrounds.
",0,1,0,0,0,0
87,Stationary crack propagation in a two-dimensional visco-elastic network model,"  We investigate crack propagation in a simple two-dimensional visco-elastic
model and find a scaling regime in the relation between the propagation
velocity and energy release rate or fracture energy, together with lower and
upper bounds of the scaling regime. On the basis of our result, the existence
of the lower and upper bounds is expected to be universal or model-independent:
the present simple simulation model provides generic insight into the physics
of crack propagation, and the model will be a first step towards the
development of a more refined coarse-grained model. Relatively abrupt changes
of velocity are predicted near the lower and upper bounds for the scaling
regime and the positions of the bounds could be good markers for the
development of tough polymers, for which we provide simple views that could be
useful as guiding principles for toughening polymer-based materials.
",0,1,0,0,0,0
88,A note on the fundamental group of Kodaira fibrations,"  The fundamental group $\pi$ of a Kodaira fibration is, by definition, the
extension of a surface group $\Pi_b$ by another surface group $\Pi_g$, i.e. \[
1 \rightarrow \Pi_g \rightarrow \pi \rightarrow \Pi_b \rightarrow 1. \]
Conversely, we can inquire about what conditions need to be satisfied by a
group of that sort in order to be the fundamental group of a Kodaira fibration.
In this short note we collect some restriction on the image of the classifying
map $m \colon \Pi_b \to \Gamma_g$ in terms of the coinvariant homology of
$\Pi_g$. In particular, we observe that if $\pi$ is the fundamental group of a
Kodaira fibration with relative irregularity $g-s$, then $g \leq 1+ 6s$, and we
show that this effectively constrains the possible choices for $\pi$, namely
that there are group extensions as above that fail to satisfy this bound, hence
cannot be the fundamental group of a Kodaira fibration. In particular this
provides examples of symplectic $4$--manifolds that fail to admit a Kähler
structure for reasons that eschew the usual obstructions.
",0,0,1,0,0,0
89,Photo-Chemically Directed Self-Assembly of Carbon Nanotubes on Surfaces,"  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel
material are used in a variety of electronics applications. However, a
competitive CNT-based technology requires the precise placement of CNTs at
predefined locations of a substrate. One promising placement approach is to use
chemical recognition to bind CNTs from solution at the desired locations on a
surface. Producing the chemical pattern on the substrate is challenging. Here
we describe a one-step patterning approach based on a highly photosensitive
surface monolayer. The monolayer contains chromophopric group as light
sensitive body with heteroatoms as high quantum yield photolysis center. As
deposited, the layer will bind CNTs from solution. However, when exposed to
ultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for
conventional photoresists, the monolayer cleaves and no longer binds CNTs.
These features allow standard, wafer-scale UV lithography processes to be used
to form a patterned chemical monolayer without the need for complex substrate
patterning or monolayer stamping.
",0,1,0,0,0,0
90,Split-and-augmented Gibbs sampler - Application to large-scale inference problems,"  This paper derives two new optimization-driven Monte Carlo algorithms
inspired from variable splitting and data augmentation. In particular, the
formulation of one of the proposed approaches is closely related to the
alternating direction method of multipliers (ADMM) main steps. The proposed
framework enables to derive faster and more efficient sampling schemes than the
current state-of-the-art methods and can embed the latter. By sampling
efficiently the parameter to infer as well as the hyperparameters of the
problem, the generated samples can be used to approximate Bayesian estimators
of the parameters to infer. Additionally, the proposed approach brings
confidence intervals at a low cost contrary to optimization methods.
Simulations on two often-studied signal processing problems illustrate the
performance of the two proposed samplers. All results are compared to those
obtained by recent state-of-the-art optimization and MCMC algorithms used to
solve these problems.
",0,0,0,1,0,0
91,Does a generalized Chaplygin gas correctly describe the cosmological dark sector?,"  Yes, but only for a parameter value that makes it almost coincide with the
standard model. We reconsider the cosmological dynamics of a generalized
Chaplygin gas (gCg) which is split into a cold dark matter (CDM) part and a
dark energy (DE) component with constant equation of state. This model, which
implies a specific interaction between CDM and DE, has a $\Lambda$CDM limit and
provides the basis for studying deviations from the latter. Including matter
and radiation, we use the (modified) CLASS code \cite{class} to construct the
CMB and matter power spectra in order to search for a gCg-based concordance
model that is in agreement with the SNIa data from the JLA sample and with
recent Planck data. The results reveal that the gCg parameter $\alpha$ is
restricted to $|\alpha|\lesssim 0.05$, i.e., to values very close to the
$\Lambda$CDM limit $\alpha =0$. This excludes, in particular, models in which
DE decays linearly with the Hubble rate.
",0,1,0,0,0,0
92,The effects of subdiffusion on the NTA size measurements of extracellular vesicles in biological samples,"  The interest in the extracellular vesicles (EVs) is rapidly growing as they
became reliable biomarkers for many diseases. For this reason, fast and
accurate techniques of EVs size characterization are the matter of utmost
importance. One increasingly popular technique is the Nanoparticle Tracking
Analysis (NTA), in which the diameters of EVs are calculated from their
diffusion constants. The crucial assumption here is that the diffusion in NTA
follows the Stokes-Einstein relation, i.e. that the Mean Square Displacement
(MSD) of a particle grows linearly in time (MSD $\propto t$). However, we show
that NTA violates this assumption in both artificial and biological samples,
i.e. a large population of particles show a strongly sub-diffusive behaviour
(MSD $\propto t^\alpha$, $0<\alpha<1$). To support this observation we present
a range of experimental results for both polystyrene beads and EVs. This is
also related to another problem: for the same samples there exists a huge
discrepancy (by the factor of 2-4) between the sizes measured with NTA and with
the direct imaging methods, such as AFM. This can be remedied by e.g. the
Finite Track Length Adjustment (FTLA) method in NTA, but its applicability is
limited in the biological and poly-disperse samples. On the other hand, the
models of sub-diffusion rarely provide the direct relation between the size of
a particle and the generalized diffusion constant. However, we solve this last
problem by introducing the logarithmic model of sub-diffusion, aimed at
retrieving the size data. In result, we propose a novel protocol of NTA data
analysis. The accuracy of our method is on par with FTLA for small
($\simeq$200nm) particles. We apply our method to study the EVs samples and
corroborate the results with AFM.
",0,1,0,0,0,0
93,Empirical regression quantile process with possible application to risk analysis,"  The processes of the averaged regression quantiles and of their modifications
provide useful tools in the regression models when the covariates are not fully
under our control. As an application we mention the probabilistic risk
assessment in the situation when the return depends on some exogenous
variables. The processes enable to evaluate the expected $\alpha$-shortfall
($0\leq\alpha\leq 1$) and other measures of the risk, recently generally
accepted in the financial literature, but also help to measure the risk in
environment analysis and elsewhere.
",0,0,1,1,0,0
94,Primordial perturbations from inflation with a hyperbolic field-space,"  We study primordial perturbations from hyperinflation, proposed recently and
based on a hyperbolic field-space. In the previous work, it was shown that the
field-space angular momentum supported by the negative curvature modifies the
background dynamics and enhances fluctuations of the scalar fields
qualitatively, assuming that the inflationary background is almost de Sitter.
In this work, we confirm and extend the analysis based on the standard approach
of cosmological perturbation in multi-field inflation. At the background level,
to quantify the deviation from de Sitter, we introduce the slow-varying
parameters and show that steep potentials, which usually can not drive
inflation, can drive inflation. At the linear perturbation level, we obtain the
power spectrum of primordial curvature perturbation and express the spectral
tilt and running in terms of the slow-varying parameters. We show that
hyperinflation with power-law type potentials has already been excluded by the
recent Planck observations, while exponential-type potential with the exponent
of order unity can be made consistent with observations as far as the power
spectrum is concerned. We also argue that, in the context of a simple $D$-brane
inflation, the hyperinflation requires exponentially large hyperbolic extra
dimensions but that masses of Kaluza-Klein gravitons can be kept relatively
heavy.
",0,1,0,0,0,0
95,Role of Vanadyl Oxygen in Understanding Metallic Behavior of V2O5(001) Nanorods,"  Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,
exhibits interesting semiconductor to metal transition in the temperature range
of 530-560 K. The metallic behavior originates because of the reduction of V2O5
through oxygen vacancies. In the present report, V2O5 nanorods in the
orthorhombic phase with crystal orientation of (001) are grown using vapor
transport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal
formula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the
formation of metallic phase above the transition temperature is established
from the temperature-dependent Raman spectroscopic studies. The origin of the
metallic behavior of V2O5 is also understood due to the breakdown of pdpi bond
between OI and nearest V atom instigated by the formation of vanadyl OI
vacancy, confirmed from the downward shift of the bottom most split-off
conduction bands in the material with increasing temperature.
",0,1,0,0,0,0
96,Graph Convolution: A High-Order and Adaptive Approach,"  In this paper, we presented a novel convolutional neural network framework
for graph modeling, with the introduction of two new modules specially designed
for graph-structured data: the $k$-th order convolution operator and the
adaptive filtering module. Importantly, our framework of High-order and
Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed
architecture that fits various applications on both node and graph centrics, as
well as graph generative models. We conducted extensive experiments on
demonstrating the advantages of our framework. Particularly, our HA-GCN
outperforms the state-of-the-art models on node classification and molecule
property prediction tasks. It also generates 32% more real molecules on the
molecule generation task, both of which will significantly benefit real-world
applications such as material design and drug screening.
",1,0,0,1,0,0
97,Learning Sparse Representations in Reinforcement Learning with Sparse Coding,"  A variety of representation learning approaches have been investigated for
reinforcement learning; much less attention, however, has been given to
investigating the utility of sparse coding. Outside of reinforcement learning,
sparse coding representations have been widely used, with non-convex objectives
that result in discriminative representations. In this work, we develop a
supervised sparse coding objective for policy evaluation. Despite the
non-convexity of this objective, we prove that all local minima are global
minima, making the approach amenable to simple optimization strategies. We
empirically show that it is key to use a supervised objective, rather than the
more straightforward unsupervised sparse coding approach. We compare the
learned representations to a canonical fixed sparse representation, called
tile-coding, demonstrating that the sparse coding representation outperforms a
wide variety of tilecoding representations.
",1,0,0,1,0,0
98,Almost euclidean Isoperimetric Inequalities in spaces satisfying local Ricci curvature lower bounds,"  Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove
that if a Riemannian manifold has Ricci curvature bounded below in a metric
ball which moreover has almost maximal volume, then in a smaller ball (in a
quantified sense) it holds an almost-euclidean isoperimetric inequality. The
result is actually established in the more general framework of non-smooth
spaces satisfying local Ricci curvature lower bounds in a synthetic sense via
optimal transportation.
",0,0,1,0,0,0
99,Exponential Sums and Riesz energies,"  We bound an exponential sum that appears in the study of irregularities of
distribution (the low-frequency Fourier energy of the sum of several Dirac
measures) by geometric quantities: a special case is that for all $\left\{ x_1,
\dots, x_N\right\} \subset \mathbb{T}^2$, $X \geq 1$ and a universal $c>0$ $$
\sum_{i,j=1}^{N}{ \frac{X^2}{1 + X^4 \|x_i -x_j\|^4}} \lesssim \sum_{k \in
\mathbb{Z}^2 \atop \|k\| \leq X}{ \left| \sum_{n=1}^{N}{ e^{2 \pi i
\left\langle k, x_n \right\rangle}}\right|^2} \lesssim \sum_{i,j=1}^{N}{ X^2
e^{-c X^2\|x_i -x_j\|^2}}.$$ Since this exponential sum is intimately tied to
rather subtle distribution properties of the points, we obtain nonlocal
structural statements for near-minimizers of the Riesz-type energy. In the
regime $X \gtrsim N^{1/2}$ both upper and lower bound match for
maximally-separated point sets satisfying $\|x_i -x_j\| \gtrsim N^{-1/2}$.
",0,0,1,0,0,0
100,One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice,"  We investigate the effect of dimensional crossover in the ground state of the
antiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular
lattice that interpolates between the regime of weakly coupled Haldane chains
($J^{\prime}\! \!\ll\!\! J$) and the isotropic triangular lattice
($J^{\prime}\!\!=\!\!J$). We use the density-matrix renormalization group
(DMRG) and Schwinger boson theory performed at the Gaussian correction level
above the saddle-point solution. Our DMRG results show an abrupt transition
between decoupled spin chains and the spirally ordered regime at
$(J^{\prime}/J)_c\sim 0.42$, signaled by the sudden closing of the spin gap.
Coming from the magnetically ordered side, the computation of the spin
stiffness within Schwinger boson theory predicts the instability of the spiral
magnetic order toward a magnetically disordered phase with one-dimensional
features at $(J^{\prime}/J)_c \sim 0.43$. The agreement of these complementary
methods, along with the strong difference found between the intra- and the
interchain DMRG short spin-spin correlations; for sufficiently large values of
the interchain coupling, suggests that the interplay between the quantum
fluctuations and the dimensional crossover effects gives rise to the
one-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.
",0,1,0,0,0,0
101,Memory Aware Synapses: Learning what (not) to forget,"  Humans can learn in a continuous manner. Old rarely utilized knowledge can be
overwritten by new incoming information while important, frequently used
knowledge is prevented from being erased. In artificial learning systems,
lifelong learning so far has focused mainly on accumulating knowledge over
tasks and overcoming catastrophic forgetting. In this paper, we argue that,
given the limited model capacity and the unlimited new information to be
learned, knowledge has to be preserved or erased selectively. Inspired by
neuroplasticity, we propose a novel approach for lifelong learning, coined
Memory Aware Synapses (MAS). It computes the importance of the parameters of a
neural network in an unsupervised and online manner. Given a new sample which
is fed to the network, MAS accumulates an importance measure for each parameter
of the network, based on how sensitive the predicted output function is to a
change in this parameter. When learning a new task, changes to important
parameters can then be penalized, effectively preventing important knowledge
related to previous tasks from being overwritten. Further, we show an
interesting connection between a local version of our method and Hebb's
rule,which is a model for the learning process in the brain. We test our method
on a sequence of object recognition tasks and on the challenging problem of
learning an embedding for predicting $<$subject, predicate, object$>$ triplets.
We show state-of-the-art performance and, for the first time, the ability to
adapt the importance of the parameters based on unlabeled data towards what the
network needs (not) to forget, which may vary depending on test conditions.
",1,0,0,1,0,0
102,Uniform Spectral Convergence of the Stochastic Galerkin Method for the Linear Semiconductor Boltzmann Equation with Random Inputs and Diffusive Scalings,"  In this paper, we study the generalized polynomial chaos (gPC) based
stochastic Galerkin method for the linear semiconductor Boltzmann equation
under diffusive scaling and with random inputs from an anisotropic collision
kernel and the random initial condition. While the numerical scheme and the
proof of uniform-in-Knudsen-number regularity of the distribution function in
the random space has been introduced in [Jin-Liu-16'], the main goal of this
paper is to first obtain a sharper estimate on the regularity of the
solution-an exponential decay towards its local equilibrium, which then lead to
the uniform spectral convergence of the stochastic Galerkin method for the
problem under study.
",0,0,1,0,0,0
103,On Improving the Capacity of Solving Large-scale Wireless Network Design Problems by Genetic Algorithms,"  Over the last decade, wireless networks have experienced an impressive growth
and now play a main role in many telecommunications systems. As a consequence,
scarce radio resources, such as frequencies, became congested and the need for
effective and efficient assignment methods arose. In this work, we present a
Genetic Algorithm for solving large instances of the Power, Frequency and
Modulation Assignment Problem, arising in the design of wireless networks. To
our best knowledge, this is the first Genetic Algorithm that is proposed for
such problem. Compared to previous works, our approach allows a wider
exploration of the set of power solutions, while eliminating sources of
numerical problems. The performance of the algorithm is assessed by tests over
a set of large realistic instances of a Fixed WiMAX Network.
",1,0,1,0,0,0
104,Quasi two-dimensional Fermi surface topography of the delafossite PdRhO$_2$,"  We report on a combined study of the de Haas-van Alphen effect and angle
resolved photoemission spectroscopy on single crystals of the metallic
delafossite PdRhO$_2$ rounded off by \textit{ab initio} band structure
calculations. A high sensitivity torque magnetometry setup with SQUID readout
and synchrotron-based photoemission with a light spot size of
$~50\,\mu\mathrm{m}$ enabled high resolution data to be obtained from samples
as small as $150\times100\times20\,(\mu\mathrm{m})^3$. The Fermi surface shape
is nearly cylindrical with a rounded hexagonal cross section enclosing a
Luttinger volume of 1.00(1) electrons per formula unit.
",0,1,0,0,0,0
105,A Variational Characterization of Rényi Divergences,"  Atar, Chowdhary and Dupuis have recently exhibited a variational formula for
exponential integrals of bounded measurable functions in terms of Rényi
divergences. We develop a variational characterization of the Rényi
divergences between two probability distributions on a measurable sace in terms
of relative entropies. When combined with the elementary variational formula
for exponential integrals of bounded measurable functions in terms of relative
entropy, this yields the variational formula of Atar, Chowdhary and Dupuis as a
corollary. We also develop an analogous variational characterization of the
Rényi divergence rates between two stationary finite state Markov chains in
terms of relative entropy rates. When combined with Varadhan's variational
characterization of the spectral radius of square matrices with nonnegative
entries in terms of relative entropy, this yields an analog of the variational
formula of Atar, Chowdary and Dupuis in the framework of finite state Markov
chains.
",1,0,1,1,0,0
106,Interlayer coupling and gate-tunable excitons in transition metal dichalcogenide heterostructures,"  Bilayer van der Waals (vdW) heterostructures such as MoS2/WS2 and MoSe2/WSe2
have attracted much attention recently, particularly because of their type II
band alignments and the formation of interlayer exciton as the lowest-energy
excitonic state. In this work, we calculate the electronic and optical
properties of such heterostructures with the first-principles GW+Bethe-Salpeter
Equation (BSE) method and reveal the important role of interlayer coupling in
deciding the excited-state properties, including the band alignment and
excitonic properties. Our calculation shows that due to the interlayer
coupling, the low energy excitons can be widely tunable by a vertical gate
field. In particular, the dipole oscillator strength and radiative lifetime of
the lowest energy exciton in these bilayer heterostructures is varied by over
an order of magnitude within a practical external gate field. We also build a
simple model that captures the essential physics behind this tunability and
allows the extension of the ab initio results to a large range of electric
fields. Our work clarifies the physical picture of interlayer excitons in
bilayer vdW heterostructures and predicts a wide range of gate-tunable
excited-state properties of 2D optoelectronic devices.
",0,1,0,0,0,0
107,Enumeration of singular varieties with tangency conditions,"  We construct the algebraic cobordism theory of bundles and divisors on
varieties. It has a simple basis (over Q) from projective spaces and its rank
is equal to the number of Chern numbers. An application of this algebraic
cobordism theory is the enumeration of singular subvarieties with give tangent
conditions with a fixed smooth divisor, where the subvariety is the zero locus
of a section of a vector bundle. We prove that the generating series of numbers
of such subvarieties gives a homomorphism from the algebraic cobordism group to
the power series ring. This implies that the enumeration of singular
subvarieties with tangency conditions is governed by universal polynomials of
Chern numbers, when the vector bundle is sufficiently ample. This result
combines and generalizes the Caporaso-Harris recursive formula, Gottsche's
conjecture, classical De Jonquiere's Formula and node polynomials from tropical
geometry.
",0,0,1,0,0,0
108,In-home and remote use of robotic body surrogates by people with profound motor deficits,"  People with profound motor deficits could perform useful physical tasks for
themselves by controlling robots that are comparable to the human body. Whether
this is possible without invasive interfaces has been unclear, due to the
robot's complexity and the person's limitations. We developed a novel,
augmented reality interface and conducted two studies to evaluate the extent to
which it enabled people with profound motor deficits to control robotic body
surrogates. 15 novice users achieved meaningful improvements on a clinical
manipulation assessment when controlling the robot in Atlanta from locations
across the United States. Also, one expert user performed 59 distinct tasks in
his own home over seven days, including self-care tasks such as feeding. Our
results demonstrate that people with profound motor deficits can effectively
control robotic body surrogates without invasive interfaces.
",1,0,0,0,0,0
109,ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information,"  Object detection in wide area motion imagery (WAMI) has drawn the attention
of the computer vision research community for a number of years. WAMI proposes
a number of unique challenges including extremely small object sizes, both
sparse and densely-packed objects, and extremely large search spaces (large
video frames). Nearly all state-of-the-art methods in WAMI object detection
report that appearance-based classifiers fail in this challenging data and
instead rely almost entirely on motion information in the form of background
subtraction or frame-differencing. In this work, we experimentally verify the
failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a
heatmap-based fully convolutional neural network (CNN), and propose a novel
two-stage spatio-temporal CNN which effectively and efficiently combines both
appearance and motion information to significantly surpass the state-of-the-art
in WAMI object detection. To reduce the large search space, the first stage
(ClusterNet) takes in a set of extremely large video frames, combines the
motion and appearance information within the convolutional architecture, and
proposes regions of objects of interest (ROOBI). These ROOBI can contain from
one to clusters of several hundred objects due to the large video frame size
and varying object density in WAMI. The second stage (FoveaNet) then estimates
the centroid location of all objects in that given ROOBI simultaneously via
heatmap estimation. The proposed method exceeds state-of-the-art results on the
WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped
objects, as well as being the first proposed method in wide area motion imagery
to detect completely stationary objects.
",1,0,0,0,0,0
110,Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds,"  Monte Carlo Tree Search (MCTS), most famously used in game-play artificial
intelligence (e.g., the game of Go), is a well-known strategy for constructing
approximate solutions to sequential decision problems. Its primary innovation
is the use of a heuristic, known as a default policy, to obtain Monte Carlo
estimates of downstream values for states in a decision tree. This information
is used to iteratively expand the tree towards regions of states and actions
that an optimal policy might visit. However, to guarantee convergence to the
optimal action, MCTS requires the entire tree to be expanded asymptotically. In
this paper, we propose a new technique called Primal-Dual MCTS that utilizes
sampled information relaxation upper bounds on potential actions, creating the
possibility of ""ignoring"" parts of the tree that stem from highly suboptimal
choices. This allows us to prove that despite converging to a partial decision
tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The
new approach shows significant promise when used to optimize the behavior of a
single driver navigating a graph while operating on a ride-sharing platform.
Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest
that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision
trees and exhibits a reduced sensitivity to the size of the action space.
",1,0,1,0,0,0
111,Fermi-edge singularity and the functional renormalization group,"  We study the Fermi-edge singularity, describing the response of a degenerate
electron system to optical excitation, in the framework of the functional
renormalization group (fRG). Results for the (interband) particle-hole
susceptibility from various implementations of fRG (one- and two-
particle-irreducible, multi-channel Hubbard-Stratonovich, flowing
susceptibility) are compared to the summation of all leading logarithmic (log)
diagrams, achieved by a (first-order) solution of the parquet equations. For
the (zero-dimensional) special case of the X-ray-edge singularity, we show that
the leading log formula can be analytically reproduced in a consistent way from
a truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic
structure, we show that this derivation relies on fortuitous partial
cancellations special to the form of and accuracy applied to the X-ray-edge
singularity and does not generalize.
",0,1,0,0,0,0
112,"Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies","  Retrosynthesis is a technique to plan the chemical synthesis of organic
molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a
search tree is built by analysing molecules recursively and dissecting them
into simpler molecular building blocks until one obtains a set of known
building blocks. The search space is intractably large, and it is difficult to
determine the value of retrosynthetic positions. Here, we propose to model
retrosynthesis as a Markov Decision Process. In combination with a Deep Neural
Network policy learned from essentially the complete published knowledge of
chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In
exploratory studies, we demonstrate that MCTS with neural network policies
outperforms the traditionally used best-first search with hand-coded
heuristics.
",1,1,0,0,0,0
113,The quasi-Assouad dimension for stochastically self-similar sets,"  The class of stochastically self-similar sets contains many famous examples
of random sets, e.g. Mandelbrot percolation and general fractal percolation.
Under the assumption of the uniform open set condition and some mild
assumptions on the iterated function systems used, we show that the
quasi-Assouad dimension of self-similar random recursive sets is almost surely
equal to the almost sure Hausdorff dimension of the set. We further comment on
random homogeneous and $V$-variable sets and the removal of overlap conditions.
",0,0,1,0,0,0
114,Influence of Spin Orbit Coupling in the Iron-Based Superconductors,"  We report on the influence of spin-orbit coupling (SOC) in the Fe-based
superconductors (FeSCs) via application of circularly-polarized spin and
angle-resolved photoemission spectroscopy. We combine this technique in
representative members of both the Fe-pnictides and Fe-chalcogenides with ab
initio density functional theory and tight-binding calculations to establish an
ubiquitous modification of the electronic structure in these materials imbued
by SOC. The influence of SOC is found to be concentrated on the hole pockets
where the superconducting gap is generally found to be largest. This result
contests descriptions of superconductivity in these materials in terms of pure
spin-singlet eigenstates, raising questions regarding the possible pairing
mechanisms and role of SOC therein.
",0,1,0,0,0,0
115,Effect of Meltdown and Spectre Patches on the Performance of HPC Applications,"  In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
",1,0,0,0,0,0
116,Gene regulatory network inference: an introductory survey,"  Gene regulatory networks are powerful abstractions of biological systems.
Since the advent of high-throughput measurement technologies in biology in the
late 90s, reconstructing the structure of such networks has been a central
computational problem in systems biology. While the problem is certainly not
solved in its entirety, considerable progress has been made in the last two
decades, with mature tools now available. This chapter aims to provide an
introduction to the basic concepts underpinning network inference tools,
attempting a categorisation which highlights commonalities and relative
strengths. While the chapter is meant to be self-contained, the material
presented should provide a useful background to the later, more specialised
chapters of this book.
",0,0,0,0,1,0
117,Optic Disc and Cup Segmentation Methods for Glaucoma Detection with Modification of U-Net Convolutional Neural Network,"  Glaucoma is the second leading cause of blindness all over the world, with
approximately 60 million cases reported worldwide in 2010. If undiagnosed in
time, glaucoma causes irreversible damage to the optic nerve leading to
blindness. The optic nerve head examination, which involves measurement of
cup-to-disc ratio, is considered one of the most valuable methods of structural
diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation
of optic disc and optic cup on eye fundus images and can be performed by modern
computer vision algorithms. This work presents universal approach for automatic
optic disc and cup segmentation, which is based on deep learning, namely,
modification of U-Net convolutional neural network. Our experiments include
comparison with the best known methods on publicly available databases
DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,
our method achieves quality comparable to current state-of-the-art methods,
outperforming them in terms of the prediction time.
",1,0,0,1,0,0
118,"Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks","  The life of the modern world essentially depends on the work of the large
artificial homogeneous networks, such as wired and wireless communication
systems, networks of roads and pipelines. The support of their effective
continuous functioning requires automatic screening and permanent optimization
with processing of the huge amount of data by high-performance distributed
systems. We propose new meta-algorithm of large homogeneous network analysis,
its decomposition into alternative sets of loosely connected subnets, and
parallel optimization of the most independent elements. This algorithm is based
on a network-specific correlation function, Simulated Annealing technique, and
is adapted to work in the computer cluster. On the example of large wireless
network, we show that proposed algorithm essentially increases speed of
parallel optimization. The elaborated general approach can be used for analysis
and optimization of the wide range of networks, including such specific types
as artificial neural networks or organized in networks physiological systems of
living organisms.
",1,0,1,0,0,0
119,Robust Contextual Bandit via the Capped-$\ell_{2}$ norm,"  This paper considers the actor-critic contextual bandit for the mobile health
(mHealth) intervention. The state-of-the-art decision-making methods in mHealth
generally assume that the noise in the dynamic system follows the Gaussian
distribution. Those methods use the least-square-based algorithm to estimate
the expected reward, which is prone to the existence of outliers. To deal with
the issue of outliers, we propose a novel robust actor-critic contextual bandit
method for the mHealth intervention. In the critic updating, the
capped-$\ell_{2}$ norm is used to measure the approximation error, which
prevents outliers from dominating our objective. A set of weights could be
achieved from the critic updating. Considering them gives a weighted objective
for the actor updating. It provides the badly noised sample in the critic
updating with zero weights for the actor updating. As a result, the robustness
of both actor-critic updating is enhanced. There is a key parameter in the
capped-$\ell_{2}$ norm. We provide a reliable method to properly set it by
making use of one of the most fundamental definitions of outliers in
statistics. Extensive experiment results demonstrate that our method can
achieve almost identical results compared with the state-of-the-art methods on
the dataset without outliers and dramatically outperform them on the datasets
noised by outliers.
",1,0,0,1,0,0
120,Improper posteriors are not improper,"  In 1933 Kolmogorov constructed a general theory that defines the modern
concept of conditional expectation. In 1955 Renyi fomulated a new axiomatic
theory for probability motivated by the need to include unbounded measures. We
introduce a general concept of conditional expectation in Renyi spaces. In this
theory improper priors are allowed, and the resulting posterior can also be
improper.
In 1965 Lindley published his classic text on Bayesian statistics using the
theory of Renyi, but retracted this idea in 1973 due to the appearance of
marginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes
are investigated, and the seemingly conflicting results are explained. The
theory of Renyi can hence be used as an axiomatic basis for statistics that
allows use of unbounded priors.
Keywords: Haldane's prior; Poisson intensity; Marginalization paradox;
Measure theory; conditional probability space; axioms for statistics;
conditioning on a sigma field; improper prior
",0,0,1,1,0,0
121,Fault Tolerant Consensus Agreement Algorithm,"  Recently a new fault tolerant and simple mechanism was designed for solving
commit consensus problem. It is based on replicated validation of messages sent
between transaction participants and a special dispatcher validator manager
node. This paper presents a correctness, safety proofs and performance analysis
of this algorithm.
",1,0,0,0,0,0
122,Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology,"  This work presents a new method to quantify connectivity in transportation
networks. Inspired by the field of topological data analysis, we propose a
novel approach to explore the robustness of road network connectivity in the
presence of congestion on the roadway. The robustness of the pattern is
summarized in a congestion barcode, which can be constructed directly from
traffic datasets commonly used for navigation. As an initial demonstration, we
illustrate the main technique on a publicly available traffic dataset in a
neighborhood in New York City.
",1,0,1,0,0,0
123,Once in a blue moon: detection of 'bluing' during debris transits in the white dwarf WD1145+017,"  The first transiting planetesimal orbiting a white dwarf was recently
detected in K2 data of WD1145+017 and has been followed up intensively. The
multiple, long, and variable transits suggest the transiting objects are dust
clouds, probably produced by a disintegrating asteroid. In addition, the system
contains circumstellar gas, evident by broad absorption lines, mostly in the
u'-band, and a dust disc, indicated by an infrared excess. Here we present the
first detection of a change in colour of WD1145+017 during transits, using
simultaneous multi-band fast-photometry ULTRACAM measurements over the
u'g'r'i'-bands. The observations reveal what appears to be 'bluing' during
transits; transits are deeper in the redder bands, with a u'-r' colour
difference of up to ~-0.05 mag. We explore various possible explanations for
the bluing. 'Spectral' photometry obtained by integrating over bandpasses in
the spectroscopic data in- and out-of-transit, compared to the photometric
data, shows that the observed colour difference is most likely the result of
reduced circumstellar absorption in the spectrum during transits. This
indicates that the transiting objects and the gas share the same line-of-sight,
and that the gas covers the white dwarf only partially, as would be expected if
the gas, the transiting debris, and the dust emitting the infrared excess, are
part of the same general disc structure (although possibly at different radii).
In addition, we present the results of a week-long monitoring campaign of the
system.
",0,1,0,0,0,0
124,"Viscous dynamics of drops and bubbles in Hele-Shaw cells: drainage, drag friction, coalescence, and bursting","  In this review article, we discuss recent studies on drops and bubbles in
Hele-Shaw cells, focusing on how scaling laws exhibit crossovers from the
three-dimensional counterparts and focusing on topics in which viscosity plays
an important role. By virtue of progresses in analytical theory and high-speed
imaging, dynamics of drops and bubbles have actively been studied with the aid
of scaling arguments. However, compared with three dimensional problems,
studies on the corresponding problems in Hele-Shaw cells are still limited.
This review demonstrates that the effect of confinement in the Hele-Shaw cell
introduces new physics allowing different scaling regimes to appear. For this
purpose, we discuss various examples that are potentially important for
industrial applications handling drops and bubbles in confined spaces by
showing agreement between experiments and scaling theories. As a result, this
review provides a collection of problems in hydrodynamics that may be
analytically solved or that may be worth studying numerically in the near
future.
",0,1,0,0,0,0
125,Stacking-based Deep Neural Network: Deep Analytic Network on Convolutional Spectral Histogram Features,"  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural
network (DNN) resemblance in terms of its very deep, feedforward network
architecture. The typical S-DNN aggregates a variable number of individually
learnable modules in series to assemble a DNN-alike alternative to the targeted
object recognition tasks. This work likewise devises an S-DNN instantiation,
dubbed deep analytic network (DAN), on top of the spectral histogram (SH)
features. The DAN learning principle relies on ridge regression, and some key
DNN constituents, specifically, rectified linear unit, fine-tuning, and
normalization. The DAN aptitude is scrutinized on three repositories of varying
domains, including FERET (faces), MNIST (handwritten digits), and CIFAR10
(natural objects). The empirical results unveil that DAN escalates the SH
baseline performance over a sufficiently deep layer.
",1,0,0,0,0,0
126,Superconductivity and Frozen Electronic States at the (111) LaAlO$_3$/SrTiO$_3$ Interface,"  In spite of Anderson's theorem, disorder is known to affect superconductivity
in conventional s-wave superconductors. In most superconductors, the degree of
disorder is fixed during sample preparation. Here we report measurements of the
superconducting properties of the two-dimensional gas that forms at the
interface between LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) in the (111) crystal
orientation, a system that permits \emph{in situ} tuning of carrier density and
disorder by means of a back gate voltage $V_g$. Like the (001) oriented LAO/STO
interface, superconductivity at the (111) LAO/STO interface can be tuned by
$V_g$. In contrast to the (001) interface, superconductivity in these (111)
samples is anisotropic, being different along different interface crystal
directions, consistent with the strong anisotropy already observed other
transport properties at the (111) LAO/STO interface. In addition, we find that
the (111) interface samples ""remember"" the backgate voltage $V_F$ at which they
are cooled at temperatures near the superconducting transition temperature
$T_c$, even if $V_g$ is subsequently changed at lower temperatures. The low
energy scale and other characteristics of this memory effect ($<1$ K)
distinguish it from charge-trapping effects previously observed in (001)
interface samples.
",0,1,0,0,0,0
127,Emittance preservation of an electron beam in a loaded quasi-linear plasma wakefield,"  We investigate beam loading and emittance preservation for a high-charge
electron beam being accelerated in quasi-linear plasma wakefields driven by a
short proton beam. The structure of the studied wakefields are similar to those
of a long, modulated proton beam, such as the AWAKE proton driver. We show that
by properly choosing the electron beam parameters and exploiting two well known
effects, beam loading of the wakefield and full blow out of plasma electrons by
the accelerated beam, the electron beam can gain large amounts of energy with a
narrow final energy spread (%-level) and without significant emittance growth.
",0,1,0,0,0,0
128,Detection of Nonlinearly Distorted OFDM Signals via Generalized Approximate Message Passing,"  In this paper, we propose a practical receiver for multicarrier signals
subjected to a strong memoryless nonlinearity. The receiver design is based on
a generalized approximate message passing (GAMP) framework, and this allows
real-time algorithm implementation in software or hardware with moderate
complexity. We demonstrate that the proposed receiver can provide more than a
2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range
$10^{-4}\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to
clipping nonlinearity and the crest-factor of the clipped waveform is only
1.9dB. Simulation results also demonstrate that the proposed receiver provides
significant performance gain in frequency-selective multipath channels
",1,0,0,0,0,0
129,Nonlinear fractal meaning of the Hubble constant,"  According to astrophysical observations value of recession velocity in a
certain point is proportional to a distance to this point. The proportionality
coefficient is the Hubble constant measured with 5% accuracy. It is used in
many cosmological theories describing dark energy, dark matter, baryons, and
their relation with the cosmological constant introduced by Einstein.
In the present work we have determined a limit value of the global Hubble
constant (in a big distance from a point of observations) theoretically without
using any empirical constants on the base of our own fractal model used for the
description a relation between distance to an observed galaxy and coordinate of
its center. The distance has been defined as a nonlinear fractal measure with
scale of measurement corresponding to a deviation of the measure from its fixed
value (zero-gravity radius). We have suggested a model of specific anisotropic
fractal for simulation a radial Universe expansion. Our theoretical results
have shown existence of an inverse proportionality between accuracy of
determination the Hubble constant and accuracy of calculation a coordinates of
galaxies leading to ambiguity results obtained at cosmological observations.
",0,1,0,0,0,0
130,SEA: String Executability Analysis by Abstract Interpretation,"  Dynamic languages often employ reflection primitives to turn dynamically
generated text into executable code at run-time. These features make standard
static analysis extremely hard if not impossible because its essential data
structures, i.e., the control-flow graph and the system of recursive equations
associated with the program to analyse, are themselves dynamically mutating
objects. We introduce SEA, an abstract interpreter for automatic sound string
executability analysis of dynamic languages employing bounded (i.e, finitely
nested) reflection and dynamic code generation. Strings are statically
approximated in an abstract domain of finite state automata with basic
operations implemented as symbolic transducers. SEA combines standard program
analysis together with string executability analysis. The analysis of a call to
reflection determines a call to the same abstract interpreter over a code which
is synthesised directly from the result of the static string executability
analysis at that program point. The use of regular languages for approximating
dynamically generated code structures allows SEA to soundly approximate safety
properties of self modifying programs yet maintaining efficiency. Soundness
here means that the semantics of the code synthesised by the analyser to
resolve reflection over-approximates the semantics of the code dynamically
built at run-rime by the program at that point.
",1,0,0,0,0,0
131,On the trade-off between labels and weights in quantitative bisimulation,"  Reductions for transition systems have been recently introduced as a uniform
and principled method for comparing the expressiveness of system models with
respect to a range of properties, especially bisimulations. In this paper we
study the expressiveness (w.r.t. bisimulations) of models for quantitative
computations such as weighted labelled transition systems (WLTSs), uniform
labelled transition systems (ULTraSs), and state-to-function transition systems
(FuTSs). We prove that there is a trade-off between labels and weights: at one
extreme lays the class of (unlabelled) weighted transition systems where
information is presented using weights only; at the other lays the class of
labelled transition systems (LTSs) where information is shifted on labels.
These categories of systems cannot be further reduced in any significant way
and subsume all the aforementioned models.
",1,0,0,0,0,0
132,Poynting's theorem in magnetic turbulence,"  Poynting's theorem is used to obtain an expression for the turbulent
power-spectral density as function of frequency and wavenumber in low-frequency
magnetic turbulence. No reference is made to Elsasser variables as is usually
done in magnetohydrodynamic turbulence mixing mechanical and electromagnetic
turbulence. We rather stay with an implicit form of the mechanical part of
turbulence as suggested by electromagnetic theory in arbitrary media. All of
mechanics and flows is included into a turbulent response function which by
appropriate observations can be determined from knowledge of the turbulent
fluctuation spectra. This approach is not guided by the wish of developing a
complete theory of turbulence. It aims on the identification of the response
function from observations as input into a theory which afterwards attempts its
interpretation. Combination of both the magnetic and electric power spectral
densities leads to a representation of the turbulent response function, i.e.
the turbulent conductivity spectrum $\sigma_{\omega k}$ as function of
frequency $\omega$ and wavenumber $k$. {It is given as the ratio of magnetic to
electric power spectral densities in frequency space. This knowledge allows for
formally writing down a turbulent dispersion relation. Power law inertial range
spectra result in a power law turbulent conductivity spectrum. These can be
compared with observations in the solar wind. Keywords: MHD turbulence,
turbulent dispersion relation, turbulent response function, solar wind
turbulence
",0,1,0,0,0,0
133,Polar factorization of conformal and projective maps of the sphere in the sense of optimal mass transport,"  Let M be a compact Riemannian manifold and let $\mu$,d be the associated
measure and distance on M. Robert McCann obtained, generalizing results for the
Euclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->
M pushing forward $\mu$ to a measure $\nu$: each S factors uniquely a.e. into
the composition S = T \circ U, where U : M -> M is volume preserving and T : M
-> M is the optimal map transporting $\mu$ to $\nu$ with respect to the cost
function d^2/2.
In this article we study the polar factorization of conformal and projective
maps of the sphere S^n. For conformal maps, which may be identified with
elements of the identity component of O(1,n+1), we prove that the polar
factorization in the sense of optimal mass transport coincides with the
algebraic polar factorization (Cartan decomposition) of this Lie group. For the
projective case, where the group GL_+(n+1) is involved, we find necessary and
sufficient conditions for these two factorizations to agree.
",0,0,1,0,0,0
134,Representing numbers as the sum of squares and powers in the ring $\mathbb{Z}_n$,"  We examine the representation of numbers as the sum of two squares in
$\mathbb{Z}_n$ for a general positive integer $n$. Using this information we
make some comments about the density of positive integers which can be
represented as the sum of two squares and powers of $2$ in $\mathbb{N}$.
",0,0,1,0,0,0
135,Spatial Regression and the Bayesian Filter,"  Regression for spatially dependent outcomes poses many challenges, for
inference and for computation. Non-spatial models and traditional spatial
mixed-effects models each have their advantages and disadvantages, making it
difficult for practitioners to determine how to carry out a spatial regression
analysis. We discuss the data-generating mechanisms implicitly assumed by
various popular spatial regression models, and discuss the implications of
these assumptions. We propose Bayesian spatial filtering as an approximate
middle way between non-spatial models and traditional spatial mixed models. We
show by simulation that our Bayesian spatial filtering model has several
desirable properties and hence may be a useful addition to a spatial
statistician's toolkit.
",0,0,0,1,0,0
136,Behaviour of electron content in the ionospheric D-region during solar X-ray flares,"  One of the most important parameters in ionospheric plasma research also
having a wide practical application in wireless satellite telecommunications is
the total electron content (TEC) representing the columnal electron number
density. The F region with high electron density provides the biggest
contribution to TEC while the relatively weakly ionized plasma of the D region
(60 km - 90 km above Earths surface) is often considered as a negligible cause
of satellite signal disturbances. However, sudden intensive ionization
processes like those induced by solar X ray flares can cause relative increases
of electron density that are significantly larger in the D-region than in
regions at higher altitudes. Therefore, one cannot exclude a priori the D
region from investigations of ionospheric influences on propagation of
electromagnetic signals emitted by satellites. We discuss here this problem
which has not been sufficiently treated in literature so far. The obtained
results are based on data collected from the D region monitoring by very low
frequency radio waves and on vertical TEC calculations from the Global
Navigation Satellite System (GNSS) signal analyses, and they show noticeable
variations in the D region electron content (TECD) during activity of a solar X
ray flare (it rises by a factor of 136 in the considered case) when TECD
contribution to TEC can reach several percent and which cannot be neglected in
practical applications like global positioning procedures by satellites.
",0,1,0,0,0,0
137,Fractional compound Poisson processes with multiple internal states,"  For the particles undergoing the anomalous diffusion with different waiting
time distributions for different internal states, we derive the Fokker-Planck
and Feymann-Kac equations, respectively, describing positions of the particles
and functional distributions of the trajectories of particles; in particular,
the equations governing the functional distribution of internal states are also
obtained. The dynamics of the stochastic processes are analyzed and the
applications, calculating the distribution of the first passage time and the
distribution of the fraction of the occupation time, of the equations are
given.
",0,0,1,1,0,0
138,Zero-point spin-fluctuations of single adatoms,"  Stabilizing the magnetic signal of single adatoms is a crucial step towards
their successful usage in widespread technological applications such as
high-density magnetic data storage devices. The quantum mechanical nature of
these tiny objects, however, introduces intrinsic zero-point spin-fluctuations
that tend to destabilize the local magnetic moment of interest by dwindling the
magnetic anisotropy potential barrier even at absolute zero temperature. Here,
we elucidate the origins and quantify the effect of the fundamental ingredients
determining the magnitude of the fluctuations, namely the ($i$) local magnetic
moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner
excitations. Based on a systematic first-principles study of 3d and 4d adatoms,
we demonstrate that the transverse contribution of the fluctuations is
comparable in size to the magnetic moment itself, leading to a remarkable
$\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives
rise to a comprehensible diagram relating the fluctuation magnitude to
characteristic features of adatoms, providing practical guidelines for
designing magnetically stable nanomagnets with minimal quantum fluctuations.
",0,1,0,0,0,0
139,Exploration-exploitation tradeoffs dictate the optimal distributions of phenotypes for populations subject to fitness fluctuations,"  We study a minimal model for the growth of a phenotypically heterogeneous
population of cells subject to a fluctuating environment in which they can
replicate (by exploiting available resources) and modify their phenotype within
a given landscape (thereby exploring novel configurations). The model displays
an exploration-exploitation trade-off whose specifics depend on the statistics
of the environment. Most notably, the phenotypic distribution corresponding to
maximum population fitness (i.e. growth rate) requires a non-zero exploration
rate when the magnitude of environmental fluctuations changes randomly over
time, while a purely exploitative strategy turns out to be optimal in two-state
environments, independently of the statistics of switching times. We obtain
analytical insight into the limiting cases of very fast and very slow
exploration rates by directly linking population growth to the features of the
environment.
",0,0,0,0,1,0
140,Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms,"  Electronic Health Records (EHR) are data generated during routine clinical
care. EHR offer researchers unprecedented phenotypic breadth and depth and have
the potential to accelerate the pace of precision medicine at scale. A main EHR
use-case is creating phenotyping algorithms to define disease status, onset and
severity. Currently, no common machine-readable standard exists for defining
phenotyping algorithms which often are stored in human-readable formats. As a
result, the translation of algorithms to implementation code is challenging and
sharing across the scientific community is problematic. In this paper, we
evaluate openEHR, a formal EHR data specification, for computable
representations of EHR phenotyping algorithms.
",1,0,0,0,0,0
141,Optimizing Mission Critical Data Dissemination in Massive IoT Networks,"  Mission critical data dissemination in massive Internet of things (IoT)
networks imposes constraints on the message transfer delay between devices. Due
to low power and communication range of IoT devices, data is foreseen to be
relayed over multiple device-to-device (D2D) links before reaching the
destination. The coexistence of a massive number of IoT devices poses a
challenge in maximizing the successful transmission capacity of the overall
network alongside reducing the multi-hop transmission delay in order to support
mission critical applications. There is a delicate interplay between the
carrier sensing threshold of the contention based medium access protocol and
the choice of packet forwarding strategy selected at each hop by the devices.
The fundamental problem in optimizing the performance of such networks is to
balance the tradeoff between conflicting performance objectives such as the
spatial frequency reuse, transmission quality, and packet progress towards the
destination. In this paper, we use a stochastic geometry approach to quantify
the performance of multi-hop massive IoT networks in terms of the spatial
frequency reuse and the transmission quality under different packet forwarding
schemes. We also develop a comprehensive performance metric that can be used to
optimize the system to achieve the best performance. The results can be used to
select the best forwarding scheme and tune the carrier sensing threshold to
optimize the performance of the network according to the delay constraints and
transmission quality requirements.
",1,0,0,0,0,0
142,Interference of two co-directional exclusion processes in the presence of a static bottleneck: a biologically motivated model,"  We develope a two-species exclusion process with a distinct pair of entry and
exit sites for each species of rigid rods. The relatively slower forward
stepping of the rods in an extended bottleneck region, located in between the
two entry sites, controls the extent of interference of the co-directional flow
of the two species of rods. The relative positions of the sites of entry of the
two species of rods with respect to the location of the bottleneck are
motivated by a biological phenomenon. However, the primary focus of the study
here is to explore the effects of the interference of the flow of the two
species of rods on their spatio-temporal organization and the regulations of
this interference by the extended bottleneck. By a combination of mean-field
theory and computer simulation we calculate the flux of both species of rods
and their density profiles as well as the composite phase diagrams of the
system. If the bottleneck is sufficiently stringent some of the phases become
practically unrealizable although not ruled out on the basis of any fundamental
physical principle. Moreover the extent of suppression of flow of the
downstream entrants by the flow of the upstream entrants can also be regulated
by the strength of the bottleneck. We speculate on the possible implications of
the results in the context of the biological phenomenon that motivated the
formulation of the theoretical model.
",0,1,0,0,0,0
143,Gaussian fluctuations of Jack-deformed random Young diagrams,"  We introduce a large class of random Young diagrams which can be regarded as
a natural one-parameter deformation of some classical Young diagram ensembles;
a deformation which is related to Jack polynomials and Jack characters. We show
that each such a random Young diagram converges asymptotically to some limit
shape and that the fluctuations around the limit are asymptotically Gaussian.
",0,0,1,0,0,0
144,Revisiting (logarithmic) scaling relations using renormalization group,"  We explicitly compute the critical exponents associated with logarithmic
corrections (the so-called hatted exponents) starting from the renormalization
group equations and the mean field behavior for a wide class of models at the
upper critical behavior (for short and long range $\phi^n$-theories) and below
it. This allows us to check the scaling relations among these critical
exponents obtained by analysing the complex singularities (Lee-Yang and Fisher
zeroes) of these models. Moreover, we have obtained an explicit method to
compute the $\hat{\coppa}$ exponent [defined by $\xi\sim L (\log
L)^{\hat{\coppa}}$] and, finally, we have found a new derivation of the scaling
law associated with it.
",0,1,0,0,0,0
145,Concentration of weakly dependent Banach-valued sums and applications to statistical learning methods,"  We obtain a Bernstein-type inequality for sums of Banach-valued random
variables satisfying a weak dependence assumption of general type and under
certain smoothness assumptions of the underlying Banach norm. We use this
inequality in order to investigate in the asymptotical regime the error upper
bounds for the broad family of spectral regularization methods for reproducing
kernel decision rules, when trained on a sample coming from a $\tau-$mixing
process.
",0,0,1,1,0,0
146,Evolution of the Kondo lattice electronic structure above the transport coherence temperature,"  The temperature-dependent evolution of the Kondo lattice is a long-standing
topic of theoretical and experimental investigation and yet it lacks a truly
microscopic description of the relation of the basic $f$-$d$ hybridization
processes to the fundamental temperature scales of Kondo screening and
Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$
hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo
lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved
photoemission (ARPES) with sufficient detail to allow direct comparison to
first principles dynamical mean field theory (DMFT) calculations containing
full realism of crystalline electric field states. The ARPES results, for two
orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$
hybridization scenarios, with additional microscopic insight provided by DMFT,
reveal $f$ participation in the Fermi surface at temperatures much higher than
the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be
the onset for such behavior. The identification of a $T$-dependent crystalline
electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is
specifically highlighted.
",0,1,0,0,0,0
147,On A Conjecture Regarding Permutations Which Destroy Arithmetic Progressions,"  Hegarty conjectured for $n\neq 2, 3, 5, 7$ that $\mathbb{Z}/n\mathbb{Z}$ has
a permutation which destroys all arithmetic progressions mod $n$. For $n\ge
n_0$, Hegarty and Martinsson demonstrated that $\mathbb{Z}/n\mathbb{Z}$ has an
arithmetic-progression destroying permutation. However $n_0\approx 1.4\times
10^{14}$ and thus resolving the conjecture in full remained out of reach of any
computational techniques. However, this paper using constructions modeled after
those used by Elkies and Swaminathan for the case of $\mathbb{Z}/p\mathbb{Z}$
with $p$ being prime, establish the conjecture in full. Furthermore our results
do not rely on the fact that it suffices to study when $n<n_0$ and thus our
results completely independent of the proof given by Hegarty and Martinsson.
",0,0,1,0,0,0
148,Inverse monoids and immersions of cell complexes,"  An immersion $f : {\mathcal D} \rightarrow \mathcal C$ between cell complexes
is a local homeomorphism onto its image that commutes with the characteristic
maps of the cell complexes. We study immersions between finite-dimensional
connected $\Delta$-complexes by replacing the fundamental group of the base
space by an appropriate inverse monoid. We show how conjugacy classes of the
closed inverse submonoids of this inverse monoid may be used to classify
connected immersions into the complex. This extends earlier results of Margolis
and Meakin for immersions between graphs and of Meakin and Szakács on
immersions into $2$-dimensional $CW$-complexes.
",0,0,1,0,0,0
149,Not even wrong: The spurious link between biodiversity and ecosystem functioning,"  Resolving the relationship between biodiversity and ecosystem functioning has
been one of the central goals of modern ecology. Early debates about the
relationship were finally resolved with the advent of a statistical
partitioning scheme that decomposed the biodiversity effect into a ""selection""
effect and a ""complementarity"" effect. We prove that both the biodiversity
effect and its statistical decomposition into selection and complementarity are
fundamentally flawed because these methods use a naïve null expectation based
on neutrality, likely leading to an overestimate of the net biodiversity
effect, and they fail to account for the nonlinear abundance-ecosystem
functioning relationships observed in nature. Furthermore, under such
nonlinearity no statistical scheme can be devised to partition the biodiversity
effects. We also present an alternative metric providing a more reasonable
estimate of biodiversity effect. Our results suggest that all studies conducted
since the early 1990s likely overestimated the positive effects of biodiversity
on ecosystem functioning.
",0,0,0,0,1,0
150,Evidence of Fraud in Brazil's Electoral Campaigns Via the Benford's Law,"  The principle of democracy is that the people govern through elected
representatives. Therefore, a democracy is healthy as long as the elected
politicians do represent the people. We have analyzed data from the Brazilian
electoral court (Tribunal Superior Eleitoral, TSE) concerning money donations
for the electoral campaigns and the election results. Our work points to two
disturbing conclusions: money is a determining factor on whether a candidate is
elected or not (as opposed to representativeness); secondly, the use of
Benford's Law to analyze the declared donations received by the parties and
electoral campaigns shows evidence of fraud in the declarations. A better term
to define Brazil's government system is what we define as chrimatocracy (govern
by money).
",1,0,0,1,0,0
151,A Berkeley View of Systems Challenges for AI,"  With the increasing commoditization of computer vision, speech recognition
and machine translation systems and the widespread deployment of learning-based
back-end technologies such as digital advertising and intelligent
infrastructures, AI (Artificial Intelligence) has moved from research labs to
production. These changes have been made possible by unprecedented levels of
data and computation, by methodological advances in machine learning, by
innovations in systems software and architectures, and by the broad
accessibility of these technologies.
The next generation of AI systems promises to accelerate these developments
and increasingly impact our lives via frequent interactions and making (often
mission-critical) decisions on our behalf, often in highly personalized
contexts. Realizing this promise, however, raises daunting challenges. In
particular, we need AI systems that make timely and safe decisions in
unpredictable environments, that are robust against sophisticated adversaries,
and that can process ever increasing amounts of data across organizations and
individuals without compromising confidentiality. These challenges will be
exacerbated by the end of the Moore's Law, which will constrain the amount of
data these technologies can store and process. In this paper, we propose
several open research directions in systems, architectures, and security that
can address these challenges and help unlock AI's potential to improve lives
and society.
",1,0,0,0,0,0
152,"Equivariant infinite loop space theory, I. The space level story","  We rework and generalize equivariant infinite loop space theory, which shows
how to construct G-spectra from G-spaces with suitable structure. There is a
naive version which gives naive G-spectra for any topological group G, but our
focus is on the construction of genuine G-spectra when G is finite.
We give new information about the Segal and operadic equivariant infinite
loop space machines, supplying many details that are missing from the
literature, and we prove by direct comparison that the two machines give
equivalent output when fed equivalent input. The proof of the corresponding
nonequivariant uniqueness theorem, due to May and Thomason, works for naive
G-spectra for general G but fails hopelessly for genuine G-spectra when G is
finite. Even in the nonequivariant case, our comparison theorem is considerably
more precise, giving a direct point-set level comparison.
We have taken the opportunity to update this general area, equivariant and
nonequivariant, giving many new proofs, filling in some gaps, and giving some
corrections to results in the literature.
",0,0,1,0,0,0
153,Arithmetic purity of strong approximation for homogeneous spaces,"  We prove that any open subset $U$ of a semi-simple simply connected
quasi-split linear algebraic group $G$ with ${codim} (G\setminus U, G)\geq 2$
over a number field satisfies strong approximation by establishing a fibration
of $G$ over a toric variety. We also prove a similar result of strong
approximation with Brauer-Manin obstruction for a partial equivariant smooth
compactification of a homogeneous space where all invertible functions are
constant and the semi-simple part of the linear algebraic group is quasi-split.
Some semi-abelian varieties of any given dimension where the complements of a
rational point do not satisfy strong approximation with Brauer-Manin
obstruction are given.
",0,0,1,0,0,0
154,Flatness results for nonlocal minimal cones and subgraphs,"  We show that nonlocal minimal cones which are non-singular subgraphs outside
the origin are necessarily halfspaces.
The proof is based on classical ideas of~\cite{DG1} and on the computation of
the linearized nonlocal mean curvature operator, which is proved to satisfy a
suitable maximum principle.
With this, we obtain new, and somehow simpler, proofs of the Bernstein-type
results for nonlocal minimal surfaces which have been recently established
in~\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type
result which classifies Lipschitz nonlocal minimal subgraphs outside a ball.
",0,0,1,0,0,0
155,Effective Asymptotic Formulae for Multilinear Averages of Multiplicative Functions,"  Let $f_1,\ldots,f_k : \mathbb{N} \rightarrow \mathbb{C}$ be multiplicative
functions taking values in the closed unit disc. Using an analytic approach in
the spirit of Halász' mean value theorem, we compute multidimensional
averages of the shape $$x^{-l} \sum_{\mathbf{n} \in [x]^l} \prod_{1 \leq j \leq
k} f_j(L_j(\mathbf{n}))$$ as $x \rightarrow \infty$, where $[x] := [1,x]$ and
$L_1,\ldots, L_k$ are affine linear forms that satisfy some natural conditions.
Our approach gives a new proof of a result of Frantzikinakis and Host that is
distinct from theirs, with \emph{explicit} main and error terms. \\ As an
application of our formulae, we establish a \emph{local-to-global} principle
for Gowers norms of multiplicative functions. We also compute the asymptotic
densities of the sets of integers $n$ such that a given multiplicative function
$f: \mathbb{N} \rightarrow \{-1, 1\}$ yields a fixed sign pattern of length 3
or 4 on almost all 3- and 4-term arithmetic progressions, respectively, with
first term $n$.
",0,0,1,0,0,0
156,On the apparent permeability of porous media in rarefied gas flows,"  The apparent gas permeability of the porous medium is an important parameter
in the prediction of unconventional gas production, which was first
investigated systematically by Klinkenberg in 1941 and found to increase with
the reciprocal mean gas pressure (or equivalently, the Knudsen number).
Although the underlying rarefaction effects are well-known, the reason that the
correction factor in Klinkenberg's famous equation decreases when the Knudsen
number increases has not been fully understood. Most of the studies idealize
the porous medium as a bundle of straight cylindrical tubes, however, according
to the gas kinetic theory, this only results in an increase of the correction
factor with the Knudsen number, which clearly contradicts Klinkenberg's
experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
in simplified (but not simple) porous media, we identify, for the first time,
two key factors that can explain Klinkenberg's experimental results: the
tortuous flow path and the non-unitary tangential momentum accommodation
coefficient for the gas-surface interaction. Moreover, we find that
Klinkenberg's results can only be observed when the ratio between the apparent
and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
numbers) the correction factor increases with the Knudsen number. Our numerical
results could also serve as benchmarking cases to assess the accuracy of
macroscopic models and/or numerical schemes for the modeling/simulation of
rarefied gas flows in complex geometries over a wide range of gas rarefaction.
",0,1,0,0,0,0
157,Small subgraphs and their extensions in a random distance graph,"  In previous papers, threshold probabilities for the properties of a random
distance graph to contain strictly balanced graphs were found. We extend this
result to arbitrary graphs and prove that the number of copies of a strictly
balanced graph has asymptotically Poisson distribution at the threshold.
",0,0,1,0,0,0
158,Increasing the Reusability of Enforcers with Lifecycle Events,"  Runtime enforcement can be effectively used to improve the reliability of
software applications. However, it often requires the definition of ad hoc
policies and enforcement strategies, which might be expensive to identify and
implement. This paper discusses how to exploit lifecycle events to obtain
useful enforcement strategies that can be easily reused across applications,
thus reducing the cost of adoption of the runtime enforcement technology. The
paper finally sketches how this idea can be used to define libraries that can
automatically overcome problems related to applications misusing them.
",1,0,0,0,0,0
159,A Fast Interior Point Method for Atomic Norm Soft Thresholding,"  The atomic norm provides a generalization of the $\ell_1$-norm to continuous
parameter spaces. When applied as a sparse regularizer for line spectral
estimation the solution can be obtained by solving a convex optimization
problem. This problem is known as atomic norm soft thresholding (AST). It can
be cast as a semidefinite program and solved by standard methods. In the
semidefinite formulation there are $O(N^2)$ dual variables and a standard
primal-dual interior point method requires at least $O(N^6)$ flops per
iteration. That has lead researcher to consider alternating direction method of
multipliers (ADMM) for the solution of AST, but this method is still somewhat
slow for large problem sizes. To obtain a faster algorithm we reformulate AST
as a non-symmetric conic program. That has two properties of key importance to
its numerical solution: the conic formulation has only $O(N)$ dual variables
and the Toeplitz structure inherent to AST is preserved. Based on it we derive
FastAST which is a primal-dual interior point method for solving AST. Two
variants are considered with the fastest one requiring only $O(N^2)$ flops per
iteration. Extensive numerical experiments demonstrate that FastAST solves AST
significantly faster than a state-of-the-art solver based on ADMM.
",1,0,0,0,0,0
160,Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments,"  We study the problem of causal structure learning over a set of random
variables when the experimenter is allowed to perform at most $M$ experiments
in a non-adaptive manner. We consider the optimal learning strategy in terms of
minimizing the portions of the structure that remains unknown given the limited
number of experiments in both Bayesian and minimax setting. We characterize the
theoretical optimal solution and propose an algorithm, which designs the
experiments efficiently in terms of time complexity. We show that for bounded
degree graphs, in the minimax case and in the Bayesian case with uniform
priors, our proposed algorithm is a $\rho$-approximation algorithm, where
$\rho$ is independent of the order of the underlying graph. Simulations on both
synthetic and real data show that the performance of our algorithm is very
close to the optimal solution.
",1,0,0,1,0,0
161,Economically Efficient Combined Plant and Controller Design Using Batch Bayesian Optimization: Mathematical Framework and Airborne Wind Energy Case Study,"  We present a novel data-driven nested optimization framework that addresses
the problem of coupling between plant and controller optimization. This
optimization strategy is tailored towards instances where a closed-form
expression for the system dynamic response is unobtainable and simulations or
experiments are necessary. Specifically, Bayesian Optimization, which is a
data-driven technique for finding the optimum of an unknown and
expensive-to-evaluate objective function, is employed to solve a nested
optimization problem. The underlying objective function is modeled by a
Gaussian Process (GP); then, Bayesian Optimization utilizes the predictive
uncertainty information from the GP to determine the best subsequent control or
plant parameters. The proposed framework differs from the majority of co-design
literature where there exists a closed-form model of the system dynamics.
Furthermore, we utilize the idea of Batch Bayesian Optimization at the plant
optimization level to generate a set of plant designs at each iteration of the
overall optimization process, recognizing that there will exist economies of
scale in running multiple experiments in each iteration of the plant design
process. We validate the proposed framework for a Buoyant Airborne Turbine
(BAT). We choose the horizontal stabilizer area, longitudinal center of mass
relative to center of buoyancy (plant parameters), and the pitch angle
set-point (controller parameter) as our decision variables. Our results
demonstrate that these plant and control parameters converge to their
respective optimal values within only a few iterations.
",1,0,0,0,0,0
162,The 10 phases of spin chains with two Ising symmetries,"  We explore the topological properties of quantum spin-1/2 chains with two
Ising symmetries. This class of models does not possess any of the symmetries
that are required to protect the Haldane phase. Nevertheless, we show that
there are 4 symmetry-protected topological phases, in addition to 6 phases that
spontaneously break one or both Ising symmetries. By mapping the model to
one-dimensional interacting fermions with particle-hole and time-reversal
symmetry, we obtain integrable parent Hamiltonians for the conventional and
topological phases of the spin model. We use these Hamiltonians to characterize
the physical properties of all 10 phases, identify their local and nonlocal
order parameters, and understand the effects of weak perturbations that respect
the Ising symmetries. Our study provides the first explicit example of a class
of spin chains with several topologically non-trivial phases, and binds
together the topological classifications of interacting bosons and fermions.
",0,1,0,0,0,0
163,Generalized subspace subcodes with application in cryptology,"  Most of the codes that have an algebraic decoding algorithm are derived from
the Reed Solomon codes. They are obtained by taking equivalent codes, for
example the generalized Reed Solomon codes, or by using the so-called subfield
subcode method, which leads to Alternant codes and Goppa codes over the
underlying prime field, or over some intermediate subfield. The main advantages
of these constructions is to preserve both the minimum distance and the
decoding algorithm of the underlying Reed Solomon code. In this paper, we
propose a generalization of the subfield subcode construction by introducing
the notion of subspace subcodes and a generalization of the equivalence of
codes which leads to the notion of generalized subspace subcodes. When the
dimension of the selected subspaces is equal to one, we show that our approach
gives exactly the family of the codes obtained by equivalence and subfield
subcode technique. However, our approach highlights the links between the
subfield subcode of a code defined over an extension field and the operation of
puncturing the $q$-ary image of this code. When the dimension of the subspaces
is greater than one, we obtain codes whose alphabet is no longer a finite
field, but a set of r-uples. We explain why these codes are practically as
efficient for applications as the codes defined on an extension of degree r. In
addition, they make it possible to obtain decodable codes over a large alphabet
having parameters previously inaccessible. As an application, we give some
examples that can be used in public key cryptosystems such as McEliece.
",1,0,0,0,0,0
164,Lagrangian fibers of Gelfand-Cetlin systems,"  Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of
Gelfand-Cetlin systems over complex partial flag manifolds, we provide a
complete description of the topology of Gelfand-Cetlin fibers. We prove that
all fibers are \emph{smooth} isotropic submanifolds and give a complete
description of the fiber to be Lagrangian in terms of combinatorics of
Gelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian
fibers. After a few combinatorial and numercal tests for the displaceability,
using the bulk-deformation of Floer cohomology by Schubert cycles, we prove
that every full flag manifold $\mathcal{F}(n)$ ($n \geq 3$) with a monotone
Kirillov-Kostant-Souriau symplectic form carries a continuum of
non-displaceable Lagrangian tori which degenerates to a non-torus fiber in the
Hausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\mathcal{F}(3)$
is non-displaceable the question of which was raised by Nohara-Ueda who
computed its Floer cohomology to be vanishing.
",0,0,1,0,0,0
165,A local ensemble transform Kalman particle filter for convective scale data assimilation,"  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)
are a key component of probabilistic weather forecasting. They represent the
uncertainty in the initial conditions by an ensemble which incorporates
information coming from the physical model with the latest observations.
High-resolution numerical weather prediction models ran at operational centers
are able to resolve non-linear and non-Gaussian physical phenomena such as
convection. There is therefore a growing need to develop ensemble assimilation
algorithms able to deal with non-Gaussianity while staying computationally
feasible. In the present paper we address some of these needs by proposing a
new hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully
formulated in ensemble space and uses a deterministic scheme such that it has
the ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a
limiting case. A new criterion for choosing the proportion of particle filter
and ETKF update is also proposed. The new algorithm is implemented in the COSMO
framework and numerical experiments in a quasi-operational convective-scale
setup are conducted. The results show the feasibility of the new algorithm in
practice and indicate a strong potential for such local hybrid methods, in
particular for forecasting non-Gaussian variables such as wind and hourly
precipitation.
",0,1,0,1,0,0
166,Tensor Robust Principal Component Analysis with A New Tensor Nuclear Norm,"  In this paper, we consider the Tensor Robust Principal Component Analysis
(TRPCA) problem, which aims to exactly recover the low-rank and sparse
components from their sum. Our model is based on the recently proposed
tensor-tensor product (or t-product) [13]. Induced by the t-product, we first
rigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor
average rank, and show that the tensor nuclear norm is the convex envelope of
the tensor average rank within the unit ball of the tensor spectral norm. These
definitions, their relationships and properties are consistent with matrix
cases. Equipped with the new tensor nuclear norm, we then solve the TRPCA
problem by solving a convex program and provide the theoretical guarantee for
the exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA
as a special case. Numerical experiments verify our results, and the
applications to image recovery and background modeling problems demonstrate the
effectiveness of our method.
",0,0,0,1,0,0
167,Resolving the age bimodality of galaxy stellar populations on kpc scales,"  Galaxies in the local Universe are known to follow bimodal distributions in
the global stellar populations properties. We analyze the distribution of the
local average stellar-population ages of 654,053 sub-galactic regions resolved
on ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the
CALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.
We find a bimodal local-age distribution, with an old and a young peak
primarily due to regions in early-type galaxies and star-forming regions of
spirals, respectively. Within spiral galaxies, the older ages of bulges and
inter-arm regions relative to spiral arms support an internal age bimodality.
Although regions of higher stellar-mass surface-density, mu*, are typically
older, mu* alone does not determine the stellar population age and a bimodal
distribution is found at any fixed mu*. We identify an ""old ridge"" of regions
of age ~9 Gyr, independent of mu*, and a ""young sequence"" of regions with age
increasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as
regions containing only old stars, and the latter as regions where the relative
contamination of old stellar populations by young stars decreases as mu*
increases. The reason why this bimodal age distribution is not inconsistent
with the unimodal shape of the cosmic-averaged star-formation history is that
i) the dominating contribution by young stars biases the age low with respect
to the average epoch of star formation, and ii) the use of a single average age
per region is unable to represent the full time-extent of the star-formation
history of ""young-sequence"" regions.
",0,1,0,0,0,0
168,Hidden long evolutionary memory in a model biochemical network,"  We introduce a minimal model for the evolution of functional
protein-interaction networks using a sequence-based mutational algorithm, and
apply the model to study neutral drift in networks that yield oscillatory
dynamics. Starting with a functional core module, random evolutionary drift
increases network complexity even in the absence of specific selective
pressures. Surprisingly, we uncover a hidden order in sequence space that gives
rise to long-term evolutionary memory, implying strong constraints on network
evolution due to the topology of accessible sequence space.
",0,1,0,0,0,0
169,On Study of the Reliable Fully Convolutional Networks with Tree Arranged Outputs (TAO-FCN) for Handwritten String Recognition,"  The handwritten string recognition is still a challengeable task, though the
powerful deep learning tools were introduced. In this paper, based on TAO-FCN,
we proposed an end-to-end system for handwritten string recognition. Compared
with the conventional methods, there is no preprocess nor manually designed
rules employed. With enough labelled data, it is easy to apply the proposed
method to different applications. Although the performance of the proposed
method may not be comparable with the state-of-the-art approaches, it's
usability and robustness are more meaningful for practical applications.
",1,0,0,0,0,0
170,Marcel Riesz on Nörlund Means,"  We note that the necessary and sufficient conditions established by Marcel
Riesz for the inclusion of regular Nörlund summation methods are in fact
applicable quite generally.
",0,0,1,0,0,0
171,Mathematics of Isogeny Based Cryptography,"  These lectures notes were written for a summer school on Mathematics for
post-quantum cryptography in Thiès, Senegal. They try to provide a guide for
Masters' students to get through the vast literature on elliptic curves,
without getting lost on their way to learning isogeny based cryptography. They
are by no means a reference text on the theory of elliptic curves, nor on
cryptography; students are encouraged to complement these notes with some of
the books recommended in the bibliography.
The presentation is divided in three parts, roughly corresponding to the
three lectures given. In an effort to keep the reader interested, each part
alternates between the fundamental theory of elliptic curves, and applications
in cryptography. We often prefer to have the main ideas flow smoothly, rather
than having a rigorous presentation as one would have in a more classical book.
The reader will excuse us for the inaccuracies and the omissions.
",1,0,0,0,0,0
172,Modeling of drug diffusion in a solid tumor leading to tumor cell death,"  It has been shown recently that changing the fluidic properties of a drug can
improve its efficacy in ablating solid tumors. We develop a modeling framework
for tumor ablation, and present the simplest possible model for drug diffusion
in a spherical tumor with leaky boundaries and assuming cell death eventually
leads to ablation of that cell effectively making the two quantities
numerically equivalent. The death of a cell after a given exposure time depends
on both the concentration of the drug and the amount of oxygen available to the
cell. Higher oxygen availability leads to cell death at lower drug
concentrations. It can be assumed that a minimum concentration is required for
a cell to die, effectively connecting diffusion with efficacy. The
concentration threshold decreases as exposure time increases, which allows us
to compute dose-response curves. Furthermore, these curves can be plotted at
much finer time intervals compared to that of experiments, which is used to
produce a dose-threshold-response surface giving an observer a complete picture
of the drug's efficacy for an individual. In addition, since the diffusion,
leak coefficients, and the availability of oxygen is different for different
individuals and tumors, we produce artificial replication data through
bootstrapping to simulate error. While the usual data-driven model with
Sigmoidal curves use 12 free parameters, our mechanistic model only has two
free parameters, allowing it to be open to scrutiny rather than forcing
agreement with data. Even so, the simplest model in our framework, derived
here, shows close agreement with the bootstrapped curves, and reproduces well
established relations, such as Haber's rule.
",0,0,0,0,1,0
173,Sensitivity analysis for inverse probability weighting estimators via the percentile bootstrap,"  To identify the estimand in missing data problems and observational studies,
it is common to base the statistical estimation on the ""missing at random"" and
""no unmeasured confounder"" assumptions. However, these assumptions are
unverifiable using empirical data and pose serious threats to the validity of
the qualitative conclusions of the statistical inference. A sensitivity
analysis asks how the conclusions may change if the unverifiable assumptions
are violated to a certain degree. In this paper we consider a marginal
sensitivity model which is a natural extension of Rosenbaum's sensitivity model
that is widely used for matched observational studies. We aim to construct
confidence intervals based on inverse probability weighting estimators, such
that asymptotically the intervals have at least nominal coverage of the
estimand whenever the data generating distribution is in the collection of
marginal sensitivity models. We use a percentile bootstrap and a generalized
minimax/maximin inequality to transform this intractable problem to a linear
fractional programming problem, which can be solved very efficiently. We
illustrate our method using a real dataset to estimate the causal effect of
fish consumption on blood mercury level.
",0,0,1,1,0,0
174,From 4G to 5G: Self-organized Network Management meets Machine Learning,"  In this paper, we provide an analysis of self-organized network management,
with an end-to-end perspective of the network. Self-organization as applied to
cellular networks is usually referred to Self-organizing Networks (SONs), and
it is a key driver for improving Operations, Administration, and Maintenance
(OAM) activities. SON aims at reducing the cost of installation and management
of 4G and future 5G networks, by simplifying operational tasks through the
capability to configure, optimize and heal itself. To satisfy 5G network
management requirements, this autonomous management vision has to be extended
to the end to end network. In literature and also in some instances of products
available in the market, Machine Learning (ML) has been identified as the key
tool to implement autonomous adaptability and take advantage of experience when
making decisions. In this paper, we survey how network management can
significantly benefit from ML solutions. We review and provide the basic
concepts and taxonomy for SON, network management and ML. We analyse the
available state of the art in the literature, standardization, and in the
market. We pay special attention to 3rd Generation Partnership Project (3GPP)
evolution in the area of network management and to the data that can be
extracted from 3GPP networks, in order to gain knowledge and experience in how
the network is working, and improve network performance in a proactive way.
Finally, we go through the main challenges associated with this line of
research, in both 4G and in what 5G is getting designed, while identifying new
directions for research.
",1,0,0,0,0,0
175,Cyber Risk Analysis of Combined Data Attacks Against Power System State Estimation,"  Understanding smart grid cyber attacks is key for developing appropriate
protection and recovery measures. Advanced attacks pursue maximized impact at
minimized costs and detectability. This paper conducts risk analysis of
combined data integrity and availability attacks against the power system state
estimation. We compare the combined attacks with pure integrity attacks - false
data injection (FDI) attacks. A security index for vulnerability assessment to
these two kinds of attacks is proposed and formulated as a mixed integer linear
programming problem. We show that such combined attacks can succeed with fewer
resources than FDI attacks. The combined attacks with limited knowledge of the
system model also expose advantages in keeping stealth against the bad data
detection. Finally, the risk of combined attacks to reliable system operation
is evaluated using the results from vulnerability assessment and attack impact
analysis. The findings in this paper are validated and supported by a detailed
case study.
",1,0,0,0,0,0
176,A New Family of Near-metrics for Universal Similarity,"  We propose a family of near-metrics based on local graph diffusion to capture
similarity for a wide class of data sets. These quasi-metametrics, as their
names suggest, dispense with one or two standard axioms of metric spaces,
specifically distinguishability and symmetry, so that similarity between data
points of arbitrary type and form could be measured broadly and effectively.
The proposed near-metric family includes the forward k-step diffusion and its
reverse, typically on the graph consisting of data objects and their features.
By construction, this family of near-metrics is particularly appropriate for
categorical data, continuous data, and vector representations of images and
text extracted via deep learning approaches. We conduct extensive experiments
to evaluate the performance of this family of similarity measures and compare
and contrast with traditional measures of similarity used for each specific
application and with the ground truth when available. We show that for
structured data including categorical and continuous data, the near-metrics
corresponding to normalized forward k-step diffusion (k small) work as one of
the best performing similarity measures; for vector representations of text and
images including those extracted from deep learning, the near-metrics derived
from normalized and reverse k-step graph diffusion (k very small) exhibit
outstanding ability to distinguish data points from different classes.
",1,0,0,1,0,0
177,Poisoning Attacks to Graph-Based Recommender Systems,"  Recommender system is an important component of many web services to help
users locate items that match their interests. Several studies showed that
recommender systems are vulnerable to poisoning attacks, in which an attacker
injects fake data to a given system such that the system makes recommendations
as the attacker desires. However, these poisoning attacks are either agnostic
to recommendation algorithms or optimized to recommender systems that are not
graph-based. Like association-rule-based and matrix-factorization-based
recommender systems, graph-based recommender system is also deployed in
practice, e.g., eBay, Huawei App Store. However, how to design optimized
poisoning attacks for graph-based recommender systems is still an open problem.
In this work, we perform a systematic study on poisoning attacks to graph-based
recommender systems. Due to limited resources and to avoid detection, we assume
the number of fake users that can be injected into the system is bounded. The
key challenge is how to assign rating scores to the fake users such that the
target item is recommended to as many normal users as possible. To address the
challenge, we formulate the poisoning attacks as an optimization problem,
solving which determines the rating scores for the fake users. We also propose
techniques to solve the optimization problem. We evaluate our attacks and
compare them with existing attacks under white-box (recommendation algorithm
and its parameters are known), gray-box (recommendation algorithm is known but
its parameters are unknown), and black-box (recommendation algorithm is
unknown) settings using two real-world datasets. Our results show that our
attack is effective and outperforms existing attacks for graph-based
recommender systems. For instance, when 1% fake users are injected, our attack
can make a target item recommended to 580 times more normal users in certain
scenarios.
",0,0,0,1,0,0
178,SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models,"  This paper describes the Stockholm University/University of Groningen
(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological
inflection. Our system is based on an attentional sequence-to-sequence neural
network model using Long Short-Term Memory (LSTM) cells, with joint training of
morphological inflection and the inverse transformation, i.e. lemmatization and
morphological analysis. Our system outperforms the baseline with a large
margin, and our submission ranks as the 4th best team for the track we
participate in (task 1, high-resource).
",1,0,0,0,0,0
179,"Neural system identification for large populations separating ""what"" and ""where""","  Neuroscientists classify neurons into different types that perform similar
computations at different locations in the visual field. Traditional methods
for neural system identification do not capitalize on this separation of 'what'
and 'where'. Learning deep convolutional feature spaces that are shared among
many neurons provides an exciting path forward, but the architectural design
needs to account for data limitations: While new experimental techniques enable
recordings from thousands of neurons, experimental time is limited so that one
can sample only a small fraction of each neuron's response space. Here, we show
that a major bottleneck for fitting convolutional neural networks (CNNs) to
neural data is the estimation of the individual receptive field locations, a
problem that has been scratched only at the surface thus far. We propose a CNN
architecture with a sparse readout layer factorizing the spatial (where) and
feature (what) dimensions. Our network scales well to thousands of neurons and
short recordings and can be trained end-to-end. We evaluate this architecture
on ground-truth data to explore the challenges and limitations of CNN-based
system identification. Moreover, we show that our network model outperforms
current state-of-the art system identification models of mouse primary visual
cortex.
",1,0,0,1,0,0
180,On the Deployment of Distributed Antennas for Wireless Power Transfer with Safety Electromagnetic Radiation Level Requirement,"  The extremely low efficiency is regarded as the bottleneck of Wireless Power
Transfer (WPT) technology. To tackle this problem, either enlarging the
transfer power or changing the infrastructure of WPT system could be an
intuitively proposed way. However, the drastically important issue on the user
exposure of electromagnetic radiation is rarely considered while we try to
improve the efficiency of WPT. In this paper, a Distributed Antenna Power
Beacon (DA-PB) based WPT system where these antennas are uniformly distributed
on a circle is analyzed and optimized with the safety electromagnetic radiation
level (SERL) requirement. In this model, three key questions are intended to be
answered: 1) With the SERL, what is the performance of the harvested power at
the users ? 2) How do we configure the parameters to maximize the efficiency of
WPT? 3) Under the same constraints, does the DA-PB still have performance gain
than the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of
DA-PB is derived to make the radio frequency (RF) electromagnetic radiation
power density at any location of the charging cell lower than the SERL
published by the Federal Communications Commission (FCC). Second, the
closed-form expressions of average harvested Direct Current (DC) power per user
in the charging cell for pass-loss exponent 2 and 4 are also provided. In order
to maximize the average efficiency of WPT, the optimal radii for distributed
antennas elements (DAEs) are derived when the pass-loss exponent takes the
typical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a
benchmark. Simulation results verify our derived theoretical results. And it is
shown that the proposed DA-PB indeed achieves larger average harvested DC power
than CA-PB and can improve the efficiency of WPT.
",1,0,0,0,0,0
181,A simulation technique for slurries interacting with moving parts and deformable solids with applications,"  A numerical method for particle-laden fluids interacting with a deformable
solid domain and mobile rigid parts is proposed and implemented in a full
engineering system. The fluid domain is modeled with a lattice Boltzmann
representation, the particles and rigid parts are modeled with a discrete
element representation, and the deformable solid domain is modeled using a
Lagrangian mesh. The main issue of this work, since separately each of these
methods is a mature tool, is to develop coupling and model-reduction approaches
in order to efficiently simulate coupled problems of this nature, as occur in
various geological and engineering applications. The lattice Boltzmann method
incorporates a large-eddy simulation technique using the Smagorinsky turbulence
model. The discrete element method incorporates spherical and polyhedral
particles for stiff contact interactions. A neo-Hookean hyperelastic model is
used for the deformable solid. We provide a detailed description of how to
couple the three solvers within a unified algorithm. The technique we propose
for rubber modeling/coupling exploits a simplification that prevents having to
solve a finite-element problem each time step. We also develop a technique to
reduce the domain size of the full system by replacing certain zones with
quasi-analytic solutions, which act as effective boundary conditions for the
lattice Boltzmann method. The major ingredients of the routine are are
separately validated. To demonstrate the coupled method in full, we simulate
slurry flows in two kinds of piston-valve geometries. The dynamics of the valve
and slurry are studied and reported over a large range of input parameters.
",1,0,0,0,0,0
182,Dissipative hydrodynamics in superspace,"  We construct a Schwinger-Keldysh effective field theory for relativistic
hydrodynamics for charged matter in a thermal background using a superspace
formalism. Superspace allows us to efficiently impose the symmetries of the
problem and to obtain a simple expression for the effective action. We show
that the theory we obtain is compatible with the Kubo-Martin-Schwinger
condition, which in turn implies that Green's functions obey the
fluctuation-dissipation theorem. Our approach complements and extends existing
formulations found in the literature.
",0,1,0,0,0,0
183,The Two-fold Role of Observables in Classical and Quantum Kinematics,"  Observables have a dual nature in both classical and quantum kinematics: they
are at the same time \emph{quantities}, allowing to separate states by means of
their numerical values, and \emph{generators of transformations}, establishing
relations between different states. In this work, we show how this two-fold
role of observables constitutes a key feature in the conceptual analysis of
classical and quantum kinematics, shedding a new light on the distinguishing
feature of the quantum at the kinematical level. We first take a look at the
algebraic description of both classical and quantum observables in terms of
Jordan-Lie algebras and show how the two algebraic structures are the precise
mathematical manifestation of the two-fold role of observables. Then, we turn
to the geometric reformulation of quantum kinematics in terms of Kähler
manifolds. A key achievement of this reformulation is to show that the two-fold
role of observables is the constitutive ingredient defining what an observable
is. Moreover, it points to the fact that, from the restricted point of view of
the transformational role of observables, classical and quantum kinematics
behave in exactly the same way. Finally, we present Landsman's general
framework of Poisson spaces with transition probability, which highlights with
unmatched clarity that the crucial difference between the two kinematics lies
in the way the two roles of observables are related to each other.
",0,1,0,0,0,0
184,On the isoperimetric quotient over scalar-flat conformal classes,"  Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with
smooth boundary $\partial M$. Suppose that $(M,g)$ admits a scalar-flat
conformal metric. We prove that the supremum of the isoperimetric quotient over
the scalar-flat conformal class is strictly larger than the best constant of
the isoperimetric inequality in the Euclidean space, and consequently is
achieved, if either (i) $n\ge 12$ and $\partial M$ has a nonumbilic point; or
(ii) $n\ge 10$, $\partial M$ is umbilic and the Weyl tensor does not vanish at
some boundary point.
",0,0,1,0,0,0
185,On the Spectrum of Random Features Maps of High Dimensional Data,"  Random feature maps are ubiquitous in modern statistical machine learning,
where they generalize random projections by means of powerful, yet often
difficult to analyze nonlinear operators. In this paper, we leverage the
""concentration"" phenomenon induced by random matrix theory to perform a
spectral analysis on the Gram matrix of these random feature maps, here for
Gaussian mixture models of simultaneously large dimension and size. Our results
are instrumental to a deeper understanding on the interplay of the nonlinearity
and the statistics of the data, thereby allowing for a better tuning of random
feature-based techniques.
",0,0,0,1,0,0
186,Minimum energy path calculations with Gaussian process regression,"  The calculation of minimum energy paths for transitions such as atomic and/or
spin re-arrangements is an important task in many contexts and can often be
used to determine the mechanism and rate of transitions. An important challenge
is to reduce the computational effort in such calculations, especially when ab
initio or electron density functional calculations are used to evaluate the
energy since they can require large computational effort. Gaussian process
regression is used here to reduce significantly the number of energy
evaluations needed to find minimum energy paths of atomic rearrangements. By
using results of previous calculations to construct an approximate energy
surface and then converge to the minimum energy path on that surface in each
Gaussian process iteration, the number of energy evaluations is reduced
significantly as compared with regular nudged elastic band calculations. For a
test problem involving rearrangements of a heptamer island on a crystal
surface, the number of energy evaluations is reduced to less than a fifth. The
scaling of the computational effort with the number of degrees of freedom as
well as various possible further improvements to this approach are discussed.
",0,1,0,1,0,0
187,Evaluating Roles of Central Users in Online Communication Networks: A Case Study of #PanamaLeaks,"  Social media has changed the ways of communication, where everyone is
equipped with the power to express their opinions to others in online
discussion platforms. Previously, a number of stud- ies have been presented to
identify opinion leaders in online discussion networks. Feng (""Are you
connected? Evaluating information cascade in online discussion about the
#RaceTogether campaign"", Computers in Human Behavior, 2016) identified five
types of central users and their communication patterns in an online
communication network of a limited time span. However, to trace the change in
communication pattern, a long-term analysis is required. In this study, we
critically analyzed framework presented by Feng based on five types of central
users in online communication network and their communication pattern in a
long-term manner. We take another case study presented by Udnor et al.
(""Determining social media impact on the politics of developing countries using
social network analytics"", Program, 2016) to further understand the dynamics as
well as to perform validation . Results indicate that there may not exist all
of these central users in an online communication network in a long-term
manner. Furthermore, we discuss the changing positions of opinion leaders and
their power to keep isolates interested in an online discussion network.
",1,1,0,1,0,0
188,Best polynomial approximation on the triangle,"  Let $E_n(f)_{\alpha,\beta,\gamma}$ denote the error of best approximation by
polynomials of degree at most $n$ in the space
$L^2(\varpi_{\alpha,\beta,\gamma})$ on the triangle $\{(x,y): x, y \ge 0, x+y
\le 1\}$, where $\varpi_{\alpha,\beta,\gamma}(x,y) := x^\alpha y ^\beta
(1-x-y)^\gamma$ for $\alpha,\beta,\gamma > -1$. Our main result gives a sharp
estimate of $E_n(f)_{\alpha,\beta,\gamma}$ in terms of the error of best
approximation for higher order derivatives of $f$ in appropriate Sobolev
spaces. The result also leads to a characterization of
$E_n(f)_{\alpha,\beta,\gamma}$ by a weighted $K$-functional.
",0,0,1,0,0,0
189,SecureTime: Secure Multicast Time Synchronization,"  Due to the increasing dependency of critical infrastructure on synchronized
clocks, network time synchronization protocols have become an attractive target
for attackers. We identify data origin authentication as the key security
objective and suggest to employ recently proposed high-performance digital
signature schemes (Ed25519 and MQQ-SIG)) as foundation of a novel set of
security measures to secure multicast time synchronization. We conduct
experiments to verify the computational and communication efficiency for using
these signatures in the standard time synchronization protocols NTP and PTP. We
propose additional security measures to prevent replay attacks and to mitigate
delay attacks. Our proposed solutions cover 1-step mode for NTP and PTP and we
extend our security measures specifically to 2-step mode (PTP) and show that
they have no impact on time synchronization's precision.
",1,0,0,0,0,0
190,Solving the multi-site and multi-orbital Dynamical Mean Field Theory using Density Matrix Renormalization,"  We implement an efficient numerical method to calculate response functions of
complex impurities based on the Density Matrix Renormalization Group (DMRG) and
use it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This
method uses the correction vector to obtain precise Green's functions on the
real frequency axis at zero temperature. By using a self-consistent bath
configuration with very low entanglement, we take full advantage of the DMRG to
calculate dynamical response functions paving the way to treat large effective
impurities such as those corresponding to multi-orbital interacting models and
multi-site or multi-momenta clusters. This method leads to reliable
calculations of non-local self energies at arbitrary dopings and interactions
and at any energy scale.
",0,1,0,0,0,0
191,Topologically Invariant Double Dirac States in Bismuth based Perovskites: Consequence of Ambivalent Charge States and Covalent Bonding,"  Bulk and surface electronic structures, calculated using density functional
theory and a tight-binding model Hamiltonian, reveal the existence of two
topologically invariant (TI) surface states in the family of cubic Bi
perovskites (ABiO$_3$; A = Na, K, Rb, Cs, Mg, Ca, Sr and Ba). The two TI
states, one lying in the valence band (TI-V) and other lying in the conduction
band (TI-C) are formed out of bonding and antibonding states of the
Bi-$\{$s,p$\}$ - O-$\{$p$\}$ coordinated covalent interaction. Below a certain
critical thickness of the film, which varies with A, TI states of top and
bottom surfaces couple to destroy the Dirac type linear dispersion and
consequently to open surface energy gaps. The origin of s-p band inversion,
necessary to form a TI state, classifies the family of ABiO$_3$ into two. For
class-I (A = Na, K, Rb, Cs and Mg) the band inversion, leading to TI-C state,
is induced by spin-orbit coupling of the Bi-p states and for class-II (A = Ca,
Sr and Ba) the band inversion is induced through weak but sensitive second
neighbor Bi-Bi interactions.
",0,1,0,0,0,0
192,Identitas: A Better Way To Be Meaningless,"  It is often recommended that identifiers for ontology terms should be
semantics-free or meaningless. In practice, ontology developers tend to use
numeric identifiers, starting at 1 and working upwards. In this paper we
present a critique of current ontology semantics-free identifiers;
monotonically increasing numbers have a number of significant usability flaws
which make them unsuitable as a default option, and we present a series of
alternatives. We have provide an implementation of these alternatives which can
be freely combined.
",1,0,0,0,0,0
193,Learning from Between-class Examples for Deep Sound Recognition,"  Deep learning methods have achieved high performance in sound recognition
tasks. Deciding how to feed the training data is important for further
performance improvement. We propose a novel learning method for deep sound
recognition: Between-Class learning (BC learning). Our strategy is to learn a
discriminative feature space by recognizing the between-class sounds as
between-class sounds. We generate between-class sounds by mixing two sounds
belonging to different classes with a random ratio. We then input the mixed
sound to the model and train the model to output the mixing ratio. The
advantages of BC learning are not limited only to the increase in variation of
the training data; BC learning leads to an enlargement of Fisher's criterion in
the feature space and a regularization of the positional relationship among the
feature distributions of the classes. The experimental results show that BC
learning improves the performance on various sound recognition networks,
datasets, and data augmentation schemes, in which BC learning proves to be
always beneficial. Furthermore, we construct a new deep sound recognition
network (EnvNet-v2) and train it with BC learning. As a result, we achieved a
performance surpasses the human level.
",1,0,0,1,0,0
194,DAGGER: A sequential algorithm for FDR control on DAGs,"  We propose a linear-time, single-pass, top-down algorithm for multiple
testing on directed acyclic graphs (DAGs), where nodes represent hypotheses and
edges specify a partial ordering in which hypotheses must be tested. The
procedure is guaranteed to reject a sub-DAG with bounded false discovery rate
(FDR) while satisfying the logical constraint that a rejected node's parents
must also be rejected. It is designed for sequential testing settings, when the
DAG structure is known a priori, but the $p$-values are obtained selectively
(such as in a sequence of experiments), but the algorithm is also applicable in
non-sequential settings when all $p$-values can be calculated in advance (such
as variable/model selection). Our DAGGER algorithm, shorthand for Greedily
Evolving Rejections on DAGs, provably controls the false discovery rate under
independence, positive dependence or arbitrary dependence of the $p$-values.
The DAGGER procedure specializes to known algorithms in the special cases of
trees and line graphs, and simplifies to the classical Benjamini-Hochberg
procedure when the DAG has no edges. We explore the empirical performance of
DAGGER using simulations, as well as a real dataset corresponding to a gene
ontology, showing favorable performance in terms of time and power.
",0,0,1,1,0,0
195,On nonlinear profile decompositions and scattering for a NLS-ODE model,"  In this paper, we consider a Hamiltonian system combining a nonlinear Schr\""
odinger equation (NLS) and an ordinary differential equation (ODE). This system
is a simplified model of the NLS around soliton solutions. Following Nakanishi
\cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.
The proof is based on Nakanishi's framework and Fermi Golden Rule estimates on
$L^4$ in time norms.
",0,0,1,0,0,0
196,Blockchain and human episodic memory,"  We relate the concepts used in decentralized ledger technology to studies of
episodic memory in the mammalian brain. Specifically, we introduce the standard
concepts of linked list, hash functions, and sharding, from computer science.
We argue that these concepts may be more relevant to studies of the neural
mechanisms of memory than has been previously appreciated. In turn, we also
highlight that certain phenomena studied in the brain, namely metacognition,
reality monitoring, and how perceptual conscious experiences come about, may
inspire development in blockchain technology too, specifically regarding
probabilistic consensus protocols.
",0,0,0,0,1,0
197,Epidemic Spreading and Aging in Temporal Networks with Memory,"  Time-varying network topologies can deeply influence dynamical processes
mediated by them. Memory effects in the pattern of interactions among
individuals are also known to affect how diffusive and spreading phenomena take
place. In this paper we analyze the combined effect of these two ingredients on
epidemic dynamics on networks. We study the susceptible-infected-susceptible
(SIS) and the susceptible-infected-removed (SIR) models on the recently
introduced activity-driven networks with memory. By means of an activity-based
mean-field approach we derive, in the long time limit, analytical predictions
for the epidemic threshold as a function of the parameters describing the
distribution of activities and the strength of the memory effects. Our results
show that memory reduces the threshold, which is the same for SIS and SIR
dynamics, therefore favouring epidemic spreading. The theoretical approach
perfectly agrees with numerical simulations in the long time asymptotic regime.
Strong aging effects are present in the preasymptotic regime and the epidemic
threshold is deeply affected by the starting time of the epidemics. We discuss
in detail the origin of the model-dependent preasymptotic corrections, whose
understanding could potentially allow for epidemic control on correlated
temporal networks.
",1,0,0,0,1,0
198,"The Shattered Gradients Problem: If resnets are the answer, then what is the question?","  A long-standing obstacle to progress in deep learning is the problem of
vanishing and exploding gradients. Although, the problem has largely been
overcome via carefully constructed initializations and batch normalization,
architectures incorporating skip-connections such as highway and resnets
perform much better than standard feedforward architectures despite well-chosen
initialization and batch normalization. In this paper, we identify the
shattered gradients problem. Specifically, we show that the correlation between
gradients in standard feedforward networks decays exponentially with depth
resulting in gradients that resemble white noise whereas, in contrast, the
gradients in architectures with skip-connections are far more resistant to
shattering, decaying sublinearly. Detailed empirical evidence is presented in
support of the analysis, on both fully-connected networks and convnets.
Finally, we present a new ""looks linear"" (LL) initialization that prevents
shattering, with preliminary experiments showing the new initialization allows
to train very deep networks without the addition of skip-connections.
",1,0,0,1,0,0
199,Pr$_2$Ir$_2$O$_7$: when Luttinger semimetal meets Melko-Hertog-Gingras spin ice state,"  We study the band structure topology and engineering from the interplay
between local moments and itinerant electrons in the context of pyrochlore
iridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction
electrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.
While the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned
into an ordered spin ice with a finite ordering wavevector, dubbed
""Melko-Hertog-Gingras"" state, by varying Ir and O contents. We point out that
the ordered spin ice of the Pr local moments generates an internal magnetic
field that reconstructs the band structure of the Luttinger semimetal. Besides
the broad existence of Weyl nodes, we predict that the magnetic translation of
the ""Melko-Hertog-Gingras"" state for the Pr moments protects the Dirac band
touching at certain time reversal invariant momenta for the Ir conduction
electrons. We propose the magnetic fields to control the Pr magnetic structure
and thereby indirectly influence the topological and other properties of the Ir
electrons. Our prediction may be immediately tested in the ordered
Pr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed
examination of the band structure, magneto-transport, and other properties of
Pr$_2$Ir$_2$O$_7$.
",0,1,0,0,0,0
200,A 2-edge partial inverse problem for the Sturm-Liouville operators with singular potentials on a star-shaped graph,"  Boundary value problems for Sturm-Liouville operators with potentials from
the class $W_2^{-1}$ on a star-shaped graph are considered. We assume that the
potentials are known on all the edges of the graph except two, and show that
the potentials on the remaining edges can be constructed by fractional parts of
two spectra. A uniqueness theorem is proved, and an algorithm for the
constructive solution of the partial inverse problem is provided. The main
ingredient of the proofs is the Riesz-basis property of specially constructed
systems of functions.
",0,0,1,0,0,0
201,Jastrow form of the Ground State Wave Functions for Fractional Quantum Hall States,"  The topological morphology--order of zeros at the positions of electrons with
respect to a specific electron--of Laughlin state at filling fractions $1/m$
($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the
positions of other electrons. Although fairly accurate ground state wave
functions for most of the other quantum Hall states in the lowest Landau level
are quite well-known, it had been an open problem in expressing the ground
state wave functions in terms of flux-attachment to particles, {\em a la}, this
morphology of Laughlin state. With a very general consideration of
flux-particle relations only, in spherical geometry, we here report a novel
method for determining morphologies of these states. Based on these, we
construct almost exact ground state wave-functions for the Coulomb interaction.
Although the form of interaction may change the ground state wave-function, the
same morphology constructs the latter irrespective of the nature of the
interaction between electrons.
",0,1,0,0,0,0
202,On a common refinement of Stark units and Gross-Stark units,"  The purpose of this paper is to formulate and study a common refinement of a
version of Stark's conjecture and its $p$-adic analogue, in terms of Fontaine's
$p$-adic period ring and $p$-adic Hodge theory. We construct period-ring-valued
functions under a generalization of Yoshida's conjecture on the transcendental
parts of CM-periods. Then we conjecture a reciprocity law on their special
values concerning the absolute Frobenius action. We show that our conjecture
implies a part of Stark's conjecture when the base field is an arbitrary real
field and the splitting place is its real place. It also implies a refinement
of the Gross-Stark conjecture under a certain assumption. When the base field
is the rational number field, our conjecture follows from Coleman's formula on
Fermat curves. We also prove some partial results in other cases.
",0,0,1,0,0,0
203,An Integrated Decision and Control Theoretic Solution to Multi-Agent Co-Operative Search Problems,"  This paper considers the problem of autonomous multi-agent cooperative target
search in an unknown environment using a decentralized framework under a
no-communication scenario. The targets are considered as static targets and the
agents are considered to be homogeneous. The no-communication scenario
translates as the agents do not exchange either the information about the
environment or their actions among themselves. We propose an integrated
decision and control theoretic solution for a search problem which generates
feasible agent trajectories. In particular, a perception based algorithm is
proposed which allows an agent to estimate the probable strategies of other
agents' and to choose a decision based on such estimation. The algorithm shows
robustness with respect to the estimation accuracy to a certain degree. The
performance of the algorithm is compared with random strategies and numerical
simulation shows considerable advantages.
",1,0,0,0,0,0
204,Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan,"  This study explores the validity of chain effects of clean water, which are
known as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan.
Recent studies have reported that water purifications systems are responsible
for huge contributions to human capital. Although a few studies have
investigated the short-term effects of water-supply systems in pre-war Japan,
little is known about the benefits associated with these systems. By analyzing
city-level cause-specific mortality data from the years 1922-1940, we found
that eliminating typhoid fever infections decreased the risk of deaths due to
non-waterborne diseases. Our estimates show that for one additional typhoid
death, there were approximately one to three deaths due to other causes, such
as tuberculosis and pneumonia. This suggests that the observed Mills-Reincke
phenomenon could have resulted from the prevention typhoid fever in a
previously-developing Asian country.
",0,0,0,1,1,0
205,Learning Transferable Architectures for Scalable Image Recognition,"  Developing neural network image classification models often requires
significant architecture engineering. In this paper, we study a method to learn
the model architectures directly on the dataset of interest. As this approach
is expensive when the dataset is large, we propose to search for an
architectural building block on a small dataset and then transfer the block to
a larger dataset. The key contribution of this work is the design of a new
search space (the ""NASNet search space"") which enables transferability. In our
experiments, we search for the best convolutional layer (or ""cell"") on the
CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking
together more copies of this cell, each with their own parameters to design a
convolutional architecture, named ""NASNet architecture"". We also introduce a
new regularization technique called ScheduledDropPath that significantly
improves generalization in the NASNet models. On CIFAR-10 itself, NASNet
achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet
achieves, among the published works, state-of-the-art accuracy of 82.7% top-1
and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than
the best human-invented architectures while having 9 billion fewer FLOPS - a
reduction of 28% in computational demand from the previous state-of-the-art
model. When evaluated at different levels of computational cost, accuracies of
NASNets exceed those of the state-of-the-art human-designed models. For
instance, a small version of NASNet also achieves 74% top-1 accuracy, which is
3.1% better than equivalently-sized, state-of-the-art models for mobile
platforms. Finally, the learned features by NASNet used with the Faster-RCNN
framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO
dataset.
",1,0,0,0,0,0
206,Fast Multi-frame Stereo Scene Flow with Motion Segmentation,"  We propose a new multi-frame method for efficiently computing scene flow
(dense depth and optical flow) and camera ego-motion for a dynamic scene
observed from a moving stereo camera rig. Our technique also segments out
moving objects from the rigid scene. In our method, we first estimate the
disparity map and the 6-DOF camera motion using stereo matching and visual
odometry. We then identify regions inconsistent with the estimated camera
motion and compute per-pixel optical flow only at these regions. This flow
proposal is fused with the camera motion-based flow proposal using fusion moves
to obtain the final optical flow and motion segmentation. This unified
framework benefits all four tasks - stereo, optical flow, visual odometry and
motion segmentation leading to overall higher accuracy and efficiency. Our
method is currently ranked third on the KITTI 2015 scene flow benchmark.
Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3
orders of magnitude faster than the top six methods. We also report a thorough
evaluation on challenging Sintel sequences with fast camera and object motion,
where our method consistently outperforms OSF [Menze and Geiger, 2015], which
is currently ranked second on the KITTI benchmark.
",1,0,0,0,0,0
207,Pointed $p^2q$-dimensional Hopf algebras in positive characteristic,"  Let $\K$ be an algebraically closed field of positive characteristic $p$. We
mainly classify pointed Hopf algebras over $\K$ of dimension $p^2q$, $pq^2$ and
$pqr$ where $p,q,r$ are distinct prime numbers. We obtain a complete
classification of such Hopf algebras except two subcases when they are not
generated by the first terms of coradical filtration. In particular, we obtain
many new examples of non-commutative and non-cocommutative finite-dimensional
Hopf algebras.
",0,0,1,0,0,0
208,Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems,"  We present the mixed Galerkin discretization of distributed parameter
port-Hamiltonian systems. On the prototypical example of hyperbolic systems of
two conservation laws in arbitrary spatial dimension, we derive the main
contributions: (i) A weak formulation of the underlying geometric
(Stokes-Dirac) structure with a segmented boundary according to the causality
of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac
structure by a finite-dimensional Dirac structure is realized using a mixed
Galerkin approach and power-preserving linear maps, which define minimal
discrete power variables. (iii) With a consistent approximation of the
Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.
By the degrees of freedom in the power-preserving maps, the resulting family of
structure-preserving schemes allows for trade-offs between centered
approximations and upwinding. We illustrate the method on the example of
Whitney finite elements on a 2D simplicial triangulation and compare the
eigenvalue approximation in 1D with a related approach.
",1,0,0,0,0,0
209,Clamped seismic metamaterials: Ultra-low broad frequency stop-bands,"  The regularity of earthquakes, their destructive power, and the nuisance of
ground vibration in urban environments, all motivate designs of defence
structures to lessen the impact of seismic and ground vibration waves on
buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and
up to a few tens of Hz for vibrations generated by human activities, cause a
large amount of damage, or inconvenience, depending on the geological
conditions they can travel considerable distances and may match the resonant
fundamental frequency of buildings. The ultimate aim of any seismic
metamaterial, or any other seismic shield, is to protect over this entire range
of frequencies, the long wavelengths involved, and low frequency, have meant
this has been unachievable to date.
Elastic flexural waves, applicable in the mechanical vibrations of thin
elastic plates, can be designed to have a broad zero-frequency stop-band using
a periodic array of very small clamped circles. Inspired by this experimental
and theoretical observation, all be it in a situation far removed from seismic
waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)
and body (pressure P and shear S) wave reflectors at very large wavelengths in
structured soils modelled as a fully elastic layer periodically clamped to
bedrock.
We identify zero frequency stop-bands that only exist in the limit of columns
of concrete clamped at their base to the bedrock. In a realistic configuration
of a sedimentary basin 15 meters deep we observe a zero frequency stop-band
covering a broad frequency range of $0$ to $30$ Hz.
",0,1,0,0,0,0
210,Difference analogue of second main theorems for meromorphic mapping into algebraic variety,"  In this paper, we prove some difference analogue of second main theorems of
meromorphic mapping from Cm into an algebraic variety V intersecting a finite
set of fixed hypersurfaces in subgeneral position. As an application, we prove
a result on algebraically degenerate of holomorphic curves intersecting
hypersurfaces and difference analogue of Picard's theorem on holomorphic
curves. Furthermore, we obtain a second main theorem of meromorphic mappings
intersecting hypersurfaces in N-subgeneral position for Veronese embedding in
Pn(C) and a uniqueness theorem sharing hypersurfaces.
",0,0,1,0,0,0
211,An Effective Way to Improve YouTube-8M Classification Accuracy in Google Cloud Platform,"  Large-scale datasets have played a significant role in progress of neural
network and deep learning areas. YouTube-8M is such a benchmark dataset for
general multi-label video classification. It was created from over 7 million
YouTube videos (450,000 hours of video) and includes video labels from a
vocabulary of 4716 classes (3.4 labels/video on average). It also comes with
pre-extracted audio & visual features from every second of video (3.2 billion
feature vectors in total). Google cloud recently released the datasets and
organized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.
Competitors are challenged to develop classification algorithms that assign
video-level labels using the new and improved Youtube-8M V2 dataset. Inspired
by the competition, we started exploration of audio understanding and
classification using deep learning algorithms and ensemble methods. We built
several baseline predictions according to the benchmark paper and public github
tensorflow code. Furthermore, we improved global prediction accuracy (GAP) from
base level 77% to 80.7% through approaches of ensemble.
",1,0,0,1,0,0
212,Experimental Design of a Prescribed Burn Instrumentation,"  Observational data collected during experiments, such as the planned Fire and
Smoke Model Evaluation Experiment (FASMEE), are critical for progressing and
transitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM
into operational use. Historical meteorological data, representing typical
weather conditions for the anticipated burn locations and times, have been
processed to initialize and run a set of simulations representing the planned
experimental burns. Based on an analysis of these numerical simulations, this
paper provides recommendations on the experimental setup that include the
ignition procedures, size and duration of the burns, and optimal sensor
placement. New techniques are developed to initialize coupled fire-atmosphere
simulations with weather conditions typical of the planned burn locations and
time of the year. Analysis of variation and sensitivity analysis of simulation
design to model parameters by repeated Latin Hypercube Sampling are used to
assess the locations of the sensors. The simulations provide the locations of
the measurements that maximize the expected variation of the sensor outputs
with the model parameters.
",0,0,0,1,0,0
213,Seifert surgery on knots via Reidemeister torsion and Casson-Walker-Lescop invariant III,"  For a knot $K$ in a homology $3$-sphere $\Sigma$, let $M$ be the result of
$2/q$-surgery on $K$, and let $X$ be the universal abelian covering of $M$. Our
first theorem is that if the first homology of $X$ is finite cyclic and $M$ is
a Seifert fibered space with $N\ge 3$ singular fibers, then $N\ge 4$ if and
only if the first homology of the universal abelian covering of $X$ is
infinite. Our second theorem is that under an appropriate assumption on the
Alexander polynomial of $K$, if $M$ is a Seifert fibered space, then $q=\pm 1$
(i.e.\ integral surgery).
",0,0,1,0,0,0
214,Sparse mean localization by information theory,"  Sparse feature selection is necessary when we fit statistical models, we have
access to a large group of features, don't know which are relevant, but assume
that most are not. Alternatively, when the number of features is larger than
the available data the model becomes over parametrized and the sparse feature
selection task involves selecting the most informative variables for the model.
When the model is a simple location model and the number of relevant features
does not grow with the total number of features, sparse feature selection
corresponds to sparse mean estimation. We deal with a simplified mean
estimation problem consisting of an additive model with gaussian noise and mean
that is in a restricted, finite hypothesis space. This restriction simplifies
the mean estimation problem into a selection problem of combinatorial nature.
Although the hypothesis space is finite, its size is exponential in the
dimension of the mean. In limited data settings and when the size of the
hypothesis space depends on the amount of data or on the dimension of the data,
choosing an approximation set of hypotheses is a desirable approach. Choosing a
set of hypotheses instead of a single one implies replacing the bias-variance
trade off with a resolution-stability trade off. Generalization capacity
provides a resolution selection criterion based on allowing the learning
algorithm to communicate the largest amount of information in the data to the
learner without error. In this work the theory of approximation set coding and
generalization capacity is explored in order to understand this approach. We
then apply the generalization capacity criterion to the simplified sparse mean
estimation problem and detail an importance sampling algorithm which at once
solves the difficulty posed by large hypothesis spaces and the slow convergence
of uniform sampling algorithms.
",0,0,0,1,0,0
215,Joint Power and Admission Control based on Channel Distribution Information: A Novel Two-Timescale Approach,"  In this letter, we consider the joint power and admission control (JPAC)
problem by assuming that only the channel distribution information (CDI) is
available. Under this assumption, we formulate a new chance (probabilistic)
constrained JPAC problem, where the signal to interference plus noise ratio
(SINR) outage probability of the supported links is enforced to be not greater
than a prespecified tolerance. To efficiently deal with the chance SINR
constraint, we employ the sample approximation method to convert them into
finitely many linear constraints. Then, we propose a convex approximation based
deflation algorithm for solving the sample approximation JPAC problem. Compared
to the existing works, this letter proposes a novel two-timescale JPAC
approach, where admission control is performed by the proposed deflation
algorithm based on the CDI in a large timescale and transmission power is
adapted instantly with fast fadings in a small timescale. The effectiveness of
the proposed algorithm is illustrated by simulations.
",1,0,1,0,0,0
216,A Closer Look at the Alpha Persei Coronal Conundrum,"  A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest
star, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness
were similar to coronally active late-type dwarf members. Later, in 2010, a
Hubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found
far-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious
offset of the ROSAT source, suggested that a late-type companion might be
responsible for the X-rays. Recently, a multi-faceted program tested that
premise. Groundbased optical coronography, and near-UV imaging with HST Wide
Field Camera 3, searched for any close-in faint candidate coronal objects, but
without success. Then, a Chandra pointing found the X-ray source single and
coincident with the bright star. Significantly, the SiIV emissions of Alpha
Persei, in a deeper FUV spectrum collected by HST COS as part of the joint
program, aligned well with chromospheric atomic oxygen (which must be intrinsic
to the luminous star), within the context of cooler late-F and early-G
supergiants, including Cepheid variables. This pointed to the X-rays as the
fundamental anomaly. The over-luminous X-rays still support the case for a
hyperactive dwarf secondary, albeit now spatially unresolved. However, an
alternative is that Alpha Persei represents a novel class of coronal source.
Resolving the first possibility now has become more difficult, because the easy
solution -- a well separated companion -- has been eliminated. Testing the
other possibility will require a broader high-energy census of the early-F
supergiants.
",0,1,0,0,0,0
217,The challenge of realistic music generation: modelling raw audio at scale,"  Realistic music generation is a challenging task. When building generative
models of music that are learnt from data, typically high-level representations
such as scores or MIDI are used that abstract away the idiosyncrasies of a
particular performance. But these nuances are very important for our perception
of musicality and realism, so in this work we embark on modelling music in the
raw audio domain. It has been shown that autoregressive models excel at
generating raw audio waveforms of speech, but when applied to music, we find
them biased towards capturing local signal structure at the expense of
modelling long-range correlations. This is problematic because music exhibits
structure at many different timescales. In this work, we explore autoregressive
discrete autoencoders (ADAs) as a means to enable autoregressive models to
capture long-range correlations in waveforms. We find that they allow us to
unconditionally generate piano music directly in the raw audio domain, which
shows stylistic consistency across tens of seconds.
",0,0,0,1,0,0
218,Interpretations of family size distributions: The Datura example,"  Young asteroid families are unique sources of information about fragmentation
physics and the structure of their parent bodies, since their physical
properties have not changed much since their birth. Families have different
properties such as age, size, taxonomy, collision severity and others, and
understanding the effect of those properties on our observations of the
size-frequency distribution (SFD) of family fragments can give us important
insights into the hypervelocity collision processes at scales we cannot achieve
in our laboratories. Here we take as an example the very young Datura family,
with a small 8-km parent body, and compare its size distribution to other
families, with both large and small parent bodies, and created by both
catastrophic and cratering formation events. We conclude that most likely
explanation for the shallower size distribution compared to larger families is
a more pronounced observational bias because of its small size. Its size
distribution is perfectly normal when its parent body size is taken into
account. We also discuss some other possibilities. In addition, we study
another common feature: an offset or ""bump"" in the distribution occurring for a
few of the larger elements. We hypothesize that it can be explained by a newly
described regime of cratering, ""spall cratering"", which controls the majority
of impact craters on the surface of small asteroids like Datura.
",0,1,0,0,0,0
219,"Intersections of $ω$ classes in $\overline{\mathcal{M}}_{g,n}$","  We provide a graph formula which describes an arbitrary monomial in {\omega}
classes (also referred to as stable {\psi} classes) in terms of a simple family
of dual graphs (pinwheel graphs) with edges decorated by rational functions in
{\psi} classes. We deduce some numerical consequences and in particular a
combinatorial formula expressing top intersections of \k{appa} classes on Mg in
terms of top intersections of {\psi} classes.
",0,0,1,0,0,0
220,GENFIRE: A generalized Fourier iterative reconstruction algorithm for high-resolution 3D imaging,"  Tomography has made a radical impact on diverse fields ranging from the study
of 3D atomic arrangements in matter to the study of human health in medicine.
Despite its very diverse applications, the core of tomography remains the same,
that is, a mathematical method must be implemented to reconstruct the 3D
structure of an object from a number of 2D projections. In many scientific
applications, however, the number of projections that can be measured is
limited due to geometric constraints, tolerable radiation dose and/or
acquisition speed. Thus it becomes an important problem to obtain the
best-possible reconstruction from a limited number of projections. Here, we
present the mathematical implementation of a tomographic algorithm, termed
GENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between
real and reciprocal space, GENFIRE searches for a global solution that is
concurrently consistent with the measured data and general physical
constraints. The algorithm requires minimal human intervention and also
incorporates angular refinement to reduce the tilt angle error. We demonstrate
that GENFIRE can produce superior results relative to several other popular
tomographic reconstruction techniques by numerical simulations, and by
experimentally by reconstructing the 3D structure of a porous material and a
frozen-hydrated marine cyanobacterium. Equipped with a graphical user
interface, GENFIRE is freely available from our website and is expected to find
broad applications across different disciplines.
",0,1,0,0,0,0
221,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,"  Generative Adversarial Networks (GANs) excel at creating realistic images
with complex models for which maximum likelihood is infeasible. However, the
convergence of GAN training has still not been proved. We propose a two
time-scale update rule (TTUR) for training GANs with stochastic gradient
descent on arbitrary GAN loss functions. TTUR has an individual learning rate
for both the discriminator and the generator. Using the theory of stochastic
approximation, we prove that the TTUR converges under mild assumptions to a
stationary local Nash equilibrium. The convergence carries over to the popular
Adam optimization, for which we prove that it follows the dynamics of a heavy
ball with friction and thus prefers flat minima in the objective landscape. For
the evaluation of the performance of GANs at image generation, we introduce the
""Fréchet Inception Distance"" (FID) which captures the similarity of generated
images to real ones better than the Inception Score. In experiments, TTUR
improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)
outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN
Bedrooms, and the One Billion Word Benchmark.
",1,0,0,1,0,0
222,"SPIRou Input Catalog: Activity, Rotation and Magnetic Field of Cool Dwarfs","  Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we
present new measurements of activity and magnetic field proxies of 442 low-mass
K5-M7 dwarfs. The objects were analysed as potential targets to search for
planetary-mass companions with the new spectropolarimeter and high-precision
velocimeter, SPIRou. We have analysed their high-resolution spectra in an
homogeneous way: circular polarisation, chromospheric features, and Zeeman
broadening of the FeH infrared line. The complex relationship between these
activity indicators is analysed: while no strong connection is found between
the large-scale and small-scale magnetic fields, the latter relates with the
non-thermal flux originating in the chromosphere.
We then examine the relationship between various activity diagnostics and the
optical radial-velocity jitter available in the literature, especially for
planet host stars. We use this to derive for all stars an activity merit
function (higher for quieter stars) with the goal of identifying the most
favorable stars where the radial-velocity jitter is low enough for planet
searches. We find that the main contributors to the RV jitter are the
large-scale magnetic field and the chromospheric non-thermal emission.
In addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed
along their rotation using the spectropolarimetric mode, and we derive their
magnetic topology. These very slow rotators are good representatives of future
SPIRou targets. They are compared to other stars where the magnetic topology is
also known. The poloidal component of the magnetic field is predominent in all
three stars.
",0,1,0,0,0,0
223,Objective Procedure for Reconstructing Couplings in Complex Systems,"  Inferring directional connectivity from point process data of multiple
elements is desired in various scientific fields such as neuroscience,
geography, economics, etc. Here, we propose an inference procedure for this
goal based on the kinetic Ising model. The procedure is composed of two steps:
(1) determination of the time-bin size for transforming the point-process data
to discrete time binary data and (2) screening of relevant couplings from the
estimated networks. For these, we develop simple methods based on information
theory and computational statistics. Applications to data from artificial and
\textit{in vitro} neuronal networks show that the proposed procedure performs
fairly well when identifying relevant couplings, including the discrimination
of their signs, with low computational cost. These results highlight the
potential utility of the kinetic Ising model to analyze real interacting
systems with event occurrences.
",0,0,0,0,1,0
224,Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines: A Majorization--Minimization Algorithm Approach,"  Support vector machines (SVMs) are an important tool in modern data analysis.
Traditionally, support vector machines have been fitted via quadratic
programming, either using purpose-built or off-the-shelf algorithms. We present
an alternative approach to SVM fitting via the majorization--minimization (MM)
paradigm. Algorithms that are derived via MM algorithm constructions can be
shown to monotonically decrease their objectives at each iteration, as well as
be globally convergent to stationary points. We demonstrate the construction of
iteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,
for SVM risk minimization problems involving the hinge, least-square,
squared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net
penalizations. Successful implementations of our algorithms are presented via
some numerical examples.
",1,0,0,1,0,0
225,Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search Queries,"  Estimating vaccination uptake is an integral part of ensuring public health.
It was recently shown that vaccination uptake can be estimated automatically
from web data, instead of slowly collected clinical records or population
surveys. All prior work in this area assumes that features of vaccination
uptake collected from the web are temporally regular. We present the first ever
method to remove this assumption from vaccination uptake estimation: our method
dynamically adapts to temporal fluctuations in time series web data used to
estimate vaccination uptake. We show our method to outperform the state of the
art compared to competitive baselines that use not only web data but also
curated clinical data. This performance improvement is more pronounced for
vaccines whose uptake has been irregular due to negative media attention (HPV-1
and HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of
12 years old (whose vaccination is more irregular compared to younger
children).
",1,0,0,1,0,0
226,Over Recurrence for Mixing Transformations,"  We show that every invertible strong mixing transformation on a Lebesgue
space has strictly over-recurrent sets. Also, we give an explicit procedure for
constructing strong mixing transformations with no under-recurrent sets. This
answers both parts of a question of V. Bergelson.
We define $\epsilon$-over-recurrence and show that given $\epsilon > 0$, any
ergodic measure preserving invertible transformation (including discrete
spectrum) has $\epsilon$-over-recurrent sets of arbitrarily small measure.
Discrete spectrum transformations and rotations do not have over-recurrent
sets, but we construct a weak mixing rigid transformation with strictly
over-recurrent sets.
",0,0,1,0,0,0
227,Joint Atlas-Mapping of Multiple Histological Series combined with Multimodal MRI of Whole Marmoset Brains,"  Development of a mesoscale neural circuitry map of the common marmoset is an
essential task due to the ideal characteristics of the marmoset as a model
organism for neuroscience research. To facilitate this development there is a
need for new computational tools to cross-register multi-modal data sets
containing MRI volumes as well as multiple histological series, and to register
the combined data set to a common reference atlas. We present a fully automatic
pipeline for same-subject-MRI guided reconstruction of image volumes from a
series of histological sections of different modalities, followed by
diffeomorphic mapping to a reference atlas. We show registration results for
Nissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo
MRI as our reference and show that our method achieves accurate registration
and eliminates artifactual warping that may be result from the absence of a
reference MRI data set. Examination of the determinant of the local metric
tensor of the diffeomorphic mapping between each subject's ex-vivo MRI and
resultant Nissl reconstruction allows an unprecedented local quantification of
geometrical distortions resulting from the histological processing, showing a
slight shrinkage, a median linear scale change of ~-1% in going from the
ex-vivo MRI to the tape-transfer generated histological image data.
",0,0,0,0,1,0
228,A Practical Approach for Successive Omniscience,"  The system that we study in this paper contains a set of users that observe a
discrete memoryless multiple source and communicate via noise-free channels
with the aim of attaining omniscience, the state that all users recover the
entire multiple source. We adopt the concept of successive omniscience (SO),
i.e., letting the local omniscience in some user subset be attained before the
global omniscience in the entire system, and consider the problem of how to
efficiently attain omniscience in a successive manner. Based on the existing
results on SO, we propose a CompSetSO algorithm for determining a complimentary
set, a user subset in which the local omniscience can be attained first without
increasing the sum-rate, the total number of communications, for the global
omniscience. We also derive a sufficient condition for a user subset to be
complimentary so that running the CompSetSO algorithm only requires a lower
bound, instead of the exact value, of the minimum sum-rate for attaining global
omniscience. The CompSetSO algorithm returns a complimentary user subset in
polynomial time. We show by example how to recursively apply the CompSetSO
algorithm so that the global omniscience can be attained by multi-stages of SO.
",1,0,0,0,0,0
229,Scholars on Twitter: who and how many are they?,"  In this paper we present a novel methodology for identifying scholars with a
Twitter account. By combining bibliometric data from Web of Science and Twitter
users identified by Altmetric.com we have obtained the largest set of
individual scholars matched with Twitter users made so far. Our methodology
consists of a combination of matching algorithms, considering different
linguistic elements of both author names and Twitter names; followed by a
rule-based scoring system that weights the common occurrence of several
elements related with the names, individual elements and activities of both
Twitter users and scholars matched. Our results indicate that about 2% of the
overall population of scholars in the Web of Science is active on Twitter. By
domain we find a strong presence of researchers from the Social Sciences and
the Humanities. Natural Sciences is the domain with the lowest level of
scholars on Twitter. Researchers on Twitter also tend to be younger than those
that are not on Twitter. As this is a bibliometric-based approach, it is
important to highlight the reliance of the method on the number of publications
produced and tweeted by the scholars, thus the share of scholars on Twitter
ranges between 1% and 5% depending on their level of productivity. Further
research is suggested in order to improve and expand the methodology.
",1,0,0,0,0,0
230,General notions of regression depth function,"  As a measure for the centrality of a point in a set of multivariate data,
statistical depth functions play important roles in multivariate analysis,
because one may conveniently construct descriptive as well as inferential
procedures relying on them. Many depth notions have been proposed in the
literature to fit to different applications. However, most of them are mainly
developed for the location setting. In this paper, we discuss the possibility
of extending some of them into the regression setting. A general concept of
regression depth function is also provided.
",0,0,0,1,0,0
231,Photonic topological pumping through the edges of a dynamical four-dimensional quantum Hall system,"  When a two-dimensional electron gas is exposed to a perpendicular magnetic
field and an in-plane electric field, its conductance becomes quantized in the
transverse in-plane direction: this is known as the quantum Hall (QH) effect.
This effect is a result of the nontrivial topology of the system's electronic
band structure, where an integer topological invariant known as the first Chern
number leads to the quantization of the Hall conductance. Interestingly, it was
shown that the QH effect can be generalized mathematically to four spatial
dimensions (4D), but this effect has never been realized for the obvious reason
that experimental systems are bound to three spatial dimensions. In this work,
we harness the high tunability and control offered by photonic waveguide arrays
to experimentally realize a dynamically-generated 4D QH system using a 2D array
of coupled optical waveguides. The inter-waveguide separation is constructed
such that the propagation of light along the device samples over
higher-dimensional momenta in the directions orthogonal to the two physical
dimensions, thus realizing a 2D topological pump. As a result, the device's
band structure is associated with 4D topological invariants known as second
Chern numbers which support a quantized bulk Hall response with a 4D symmetry.
In a finite-sized system, the 4D topological bulk response is carried by
localized edges modes that cross the sample as a function of of the modulated
auxiliary momenta. We directly observe this crossing through photon pumping
from edge-to-edge and corner-to-corner of our system. These are equivalent to
the pumping of charge across a 4D system from one 3D hypersurface to the
opposite one and from one 2D hyperedge to another, and serve as first
experimental realization of higher-dimensional topological physics.
",0,1,0,0,0,0
232,On Scalable Inference with Stochastic Gradient Descent,"  In many applications involving large dataset or online updating, stochastic
gradient descent (SGD) provides a scalable way to compute parameter estimates
and has gained increasing popularity due to its numerical convenience and
memory efficiency. While the asymptotic properties of SGD-based estimators have
been established decades ago, statistical inference such as interval estimation
remains much unexplored. The traditional resampling method such as the
bootstrap is not computationally feasible since it requires to repeatedly draw
independent samples from the entire dataset. The plug-in method is not
applicable when there are no explicit formulas for the covariance matrix of the
estimator. In this paper, we propose a scalable inferential procedure for
stochastic gradient descent, which, upon the arrival of each observation,
updates the SGD estimate as well as a large number of randomly perturbed SGD
estimates. The proposed method is easy to implement in practice. We establish
its theoretical properties for a general class of models that includes
generalized linear models and quantile regression models as special cases. The
finite-sample performance and numerical utility is evaluated by simulation
studies and two real data applications.
",1,0,0,1,0,0
233,The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes,"  In the work of Peng et al. in 2012, a new measure was proposed for fault
diagnosis of systems: namely, g-good-neighbor conditional diagnosability, which
requires that any fault-free vertex has at least g fault-free neighbors in the
system. In this paper, we establish the g-good-neighbor conditional
diagnosability of locally twisted cubes under the PMC model and the MM^* model.
",1,0,1,0,0,0
234,Coherence for lenses and open games,"  Categories of polymorphic lenses in computer science, and of open games in
compositional game theory, have a curious structure that is reminiscent of
compact closed categories, but differs in some crucial ways. Specifically they
have a family of morphisms that behave like the counits of a compact closed
category, but have no corresponding units; and they have a `partial' duality
that behaves like transposition in a compact closed category when it is
defined. We axiomatise this structure, which we refer to as a `teleological
category'. We precisely define a diagrammatic language suitable for these
categories, and prove a coherence theorem for them. This underpins the use of
diagrammatic reasoning in compositional game theory, which has previously been
used only informally.
",1,0,0,0,0,0
235,Streaming Algorithm for Euler Characteristic Curves of Multidimensional Images,"  We present an efficient algorithm to compute Euler characteristic curves of
gray scale images of arbitrary dimension. In various applications the Euler
characteristic curve is used as a descriptor of an image.
Our algorithm is the first streaming algorithm for Euler characteristic
curves. The usage of streaming removes the necessity to store the entire image
in RAM. Experiments show that our implementation handles terabyte scale images
on commodity hardware. Due to lock-free parallelism, it scales well with the
number of processor cores. Our software---CHUNKYEuler---is available as open
source on Bitbucket.
Additionally, we put the concept of the Euler characteristic curve in the
wider context of computational topology. In particular, we explain the
connection with persistence diagrams.
",1,0,1,0,0,0
236,An automata group of intermediate growth and exponential activity,"  We give a new example of an automata group of intermediate growth. It is
generated by an automaton with 4 states on an alphabet with 8 letters. This
automata group has exponential activity and its limit space is not simply
connected.
",0,0,1,0,0,0
237,Tuning across the BCS-BEC crossover in the multiband superconductor Fe$_{1+y}$Se$_x$Te$_{1-x}$ : An angle-resolved photoemission study,"  The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to
Bose-Einstein condensation (BEC) is difficult to realize in quantum materials
because, unlike in ultracold atoms, one cannot tune the pairing interaction. We
realize the BCS-BEC crossover in a nearly compensated semimetal
Fe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\epsilon_F$, via
chemical doping, which permits us to systematically change $\Delta /
\epsilon_F$ from 0.16 to 0.5 were $\Delta$ is the superconducting (SC) gap. We
use angle-resolved photoemission spectroscopy to measure the Fermi energy, the
SC gap and characteristic changes in the SC state electronic dispersion as the
system evolves from a BCS to a BEC regime. Our results raise important
questions about the crossover in multiband superconductors which go beyond
those addressed in the context of cold atoms.
",0,1,0,0,0,0
238,GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking,"  Model compression is essential for serving large deep neural nets on devices
with limited resources or applications that require real-time responses. As a
case study, a state-of-the-art neural language model usually consists of one or
more recurrent layers sandwiched between an embedding layer used for
representing input tokens and a softmax layer for generating output tokens. For
problems with a very large vocabulary size, the embedding and the softmax
matrices can account for more than half of the model size. For instance, the
bigLSTM model achieves state-of- the-art performance on the One-Billion-Word
(OBW) dataset with around 800k vocabulary, and its word embedding and softmax
matrices use more than 6GBytes space, and are responsible for over 90% of the
model parameters. In this paper, we propose GroupReduce, a novel compression
method for neural language models, based on vocabulary-partition (block) based
low-rank matrix approximation and the inherent frequency distribution of tokens
(the power-law distribution of words). The experimental results show our method
can significantly outperform traditional compression methods such as low-rank
approximation and pruning. On the OBW dataset, our method achieved 6.6 times
compression rate for the embedding and softmax matrices, and when combined with
quantization, our method can achieve 26 times compression rate, which
translates to a factor of 12.8 times compression for the entire model with very
little degradation in perplexity.
",0,0,0,1,0,0
239,Morphological characterization of Ge ion implanted SiO2 matrix using multifractal technique,"  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy
were implanted into SiO2 matrix with Different fluences. The implanted samples
were annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of
implanted as well as annealed samples were captured by the atomic force
microscopy (AFM). Two dimension (2D) multifractal detrended fluctuation
analysis (MFDFA) based on the partition function approach has been used to
study the surfaces of ion implanted and annealed samples. The partition
function is used to calculate generalized Hurst exponent with the segment size.
Moreover, it is seen that the generalized Hurst exponents vary nonlinearly with
the moment, thereby exhibiting the multifractal nature. The multifractality of
surface is pronounced after annealing for the surface implanted with fluence
7.5X1016 ions/cm^2.
",0,1,0,0,0,0
240,Preliminary corrosion studies of IN-RAFM steel with stagnant Lead Lithium at 550 C,"  Corrosion of Indian RAFMS (reduced activation ferritic martensitic steel)
material with liquid metal, Lead Lithium ( Pb-Li) has been studied under static
condition, maintaining Pb-Li at 550 C for different time durations, 2500, 5000
and 9000 hours. Corrosion rate was calculated from weight loss measurements.
Microstructure analysis was carried out using SEM and chemical composition by
SEM-EDX measurements. Micro Vickers hardness and tensile testing were also
carried out. Chromium was found leaching from the near surface regions and
surface hardness was found to decrease in all the three cases. Grain boundaries
were affected. Some grains got detached from the surface giving rise to pebble
like structures in the surface micrographs. There was no significant reduction
in the tensile strength, after exposure to liquid metal. This paper discusses
the experimental details and the results obtained.
",0,1,0,0,0,0
241,Magnetocapillary self-assemblies: locomotion and micromanipulation along a liquid interface,"  This paper presents an overview and discussion of magnetocapillary
self-assemblies. New results are presented, in particular concerning the
possible development of future applications. These self-organizing structures
possess the notable ability to move along an interface when powered by an
oscillatory, uniform magnetic field. The system is constructed as follows. Soft
magnetic particles are placed on a liquid interface, and submitted to a
magnetic induction field. An attractive force due to the curvature of the
interface around the particles competes with an interaction between magnetic
dipoles. Ordered structures can spontaneously emerge from these conditions.
Furthermore, time-dependent magnetic fields can produce a wide range of dynamic
behaviours, including non-time-reversible deformation sequences that produce
translational motion at low Reynolds number. In other words, due to a
spontaneous breaking of time-reversal symmetry, the assembly can turn into a
surface microswimmer. Trajectories have been shown to be precisely
controllable. As a consequence, this system offers a way to produce microrobots
able to perform different tasks. This is illustrated in this paper by the
capture, transport and release of a floating cargo, and the controlled mixing
of fluids at low Reynolds number.
",0,1,0,0,0,0
242,On asymptotically minimax nonparametric detection of signal in Gaussian white noise,"  For the problem of nonparametric detection of signal in Gaussian white noise
we point out strong asymptotically minimax tests. The sets of alternatives are
a ball in Besov space $B^r_{2\infty}$ with ""small"" balls in $L_2$ removed.
",0,0,1,1,0,0
243,Bayesian Metabolic Flux Analysis reveals intracellular flux couplings,"  Metabolic flux balance analyses are a standard tool in analysing metabolic
reaction rates compatible with measurements, steady-state and the metabolic
reaction network stoichiometry. Flux analysis methods commonly place
unrealistic assumptions on fluxes due to the convenience of formulating the
problem as a linear programming model, and most methods ignore the notable
uncertainty in flux estimates. We introduce a novel paradigm of Bayesian
metabolic flux analysis that models the reactions of the whole genome-scale
cellular system in probabilistic terms, and can infer the full flux vector
distribution of genome-scale metabolic systems based on exchange and
intracellular (e.g. 13C) flux measurements, steady-state assumptions, and
target function assumptions. The Bayesian model couples all fluxes jointly
together in a simple truncated multivariate posterior distribution, which
reveals informative flux couplings. Our model is a plug-in replacement to
conventional metabolic balance methods, such as flux balance analysis (FBA).
Our experiments indicate that we can characterise the genome-scale flux
covariances, reveal flux couplings, and determine more intracellular unobserved
fluxes in C. acetobutylicum from 13C data than flux variability analysis. The
COBRA compatible software is available at github.com/markusheinonen/bamfa
",0,0,0,1,0,0
244,Robust Estimation of Change-Point Location,"  We introduce a robust estimator of the location parameter for the
change-point in the mean based on the Wilcoxon statistic and establish its
consistency for $L_1$ near epoch dependent processes. It is shown that the
consistency rate depends on the magnitude of change. A simulation study is
performed to evaluate finite sample properties of the Wilcoxon-type estimator
in standard cases, as well as under heavy-tailed distributions and disturbances
by outliers, and to compare it with a CUSUM-type estimator. It shows that the
Wilcoxon-type estimator is equivalent to the CUSUM-type estimator in standard
cases, but outperforms the CUSUM-type estimator in presence of heavy tails or
outliers in the data.
",0,0,1,1,0,0
245,Growing length scale accompanying the vitrification: A perspective based on non-singular density fluctuations,"  In glass forming liquids close to the glass transition point, even a very
slight increase in the macroscopic density results in a dramatic slowing down
of the macroscopic relaxation. Concomitantly, the local density itself
fluctuates in space. Therefore, one can imagine that even very small local
density variations control the local glassy nature. Based on this perspective,
a model for describing growing length scale accompanying the vitrification is
introduced, in which we assume that in a subsystem whose density is above a
certain threshold value, $\rho_{\rm c}$, owing to steric constraints, particle
rearrangements are highly suppressed for a sufficiently long time period
($\sim$ structural relaxation time). We regard such a subsystem as a glassy
cluster. Then, based on the statistics of the subsystem-density, we predict
that with compression (increasing average density $\rho$) at a fixed
temperature $T$ in supercooled states, the characteristic length of the
clusters, $\xi$, diverges as $\xi\sim(\rho_{\rm c}-\rho)^{-2/d}$, where $d$ is
the spatial dimensionality. This $\xi$ measures the average persistence length
of the steric constraints in blocking the rearrangement motions and is
determined by the subsystem density. Additionally, with decreasing $T$ at a
fixed $\rho$, the length scale diverges in the same manner as $\xi\sim(T-T_{\rm
c})^{-2/d}$, for which $\rho$ is identical to $\rho_{\rm c}$ at $T=T_{\rm c}$.
The exponent describing the diverging length scale is the same as the one
predicted by some theoretical models and indeed has been observed in some
simulations and experiments. However, the basic mechanism for this divergence
is different; that is, we do not invoke thermodynamic anomalies associated with
the thermodynamic phase transition as the origin of the growing length scale.
We further present arguements for the cooperative properties based on the
clusters.
",0,1,0,0,0,0
246,Many-Objective Pareto Local Search,"  We propose a new Pareto Local Search Algorithm for the many-objective
combinatorial optimization. Pareto Local Search proved to be a very effective
tool in the case of the bi-objective combinatorial optimization and it was used
in a number of the state-of-the-art algorithms for problems of this kind. On
the other hand, the standard Pareto Local Search algorithm becomes very
inefficient for problems with more than two objectives. We build an effective
Many-Objective Pareto Local Search algorithm using three new mechanisms: the
efficient update of large Pareto archives with ND-Tree data structure, a new
mechanism for the selection of the promising solutions for the neighborhood
exploration, and a partial exploration of the neighborhoods. We apply the
proposed algorithm to the instances of two different problems, i.e. the
traveling salesperson problem and the traveling salesperson problem with
profits with up to 5 objectives showing high effectiveness of the proposed
algorithm.
",1,0,0,0,0,0
247,From Natural to Artificial Camouflage: Components and Systems,"  We identify the components of bio-inspired artificial camouflage systems
including actuation, sensing, and distributed computation. After summarizing
recent results in understanding the physiology and system-level performance of
a variety of biological systems, we describe computational algorithms that can
generate similar patterns and have the potential for distributed
implementation. We find that the existing body of work predominately treats
component technology in an isolated manner that precludes a material-like
implementation that is scale-free and robust. We conclude with open research
challenges towards the realization of integrated camouflage solutions.
",1,0,0,0,1,0
248,Bayesian nonparametric inference for the M/G/1 queueing systems based on the marked departure process,"  In the present work we study Bayesian nonparametric inference for the
continuous-time M/G/1 queueing system. In the focus of the study is the
unobservable service time distribution. We assume that the only available data
of the system are the marked departure process of customers with the marks
being the queue lengths just after departure instants. These marks constitute
an embedded Markov chain whose distribution may be parametrized by stochastic
matrices of a special delta form. We develop the theory in order to obtain
integral mixtures of Markov measures with respect to suitable prior
distributions. We have found a sufficient statistic with a distribution of a
so-called S-structure sheding some new light on the inner statistical structure
of the M/G/1 queue. Moreover, it allows to update suitable prior distributions
to the posterior. Our inference methods are validated by large sample results
as posterior consistency and posterior normality.
",0,0,1,1,0,0
249,On some polynomials and series of Bloch-Polya Type,"  We will show that $(1-q)(1-q^2)\dots (1-q^m)$ is a polynomial in $q$ with
coefficients from $\{-1,0,1\}$ iff $m=1,\ 2,\ 3,$ or $5$ and explore some
interesting consequences of this result. We find explicit formulas for the
$q$-series coefficients of $(1-q^2)(1-q^3)(1-q^4)(1-q^5)\dots$ and
$(1-q^3)(1-q^4)(1-q^5)(1-q^6)\dots$. In doing so, we extend certain
observations made by Sudler in 1964. We also discuss the classification of the
products $(1-q)(1-q^2)\dots (1-q^m)$ and some related series with respect to
their absolute largest coefficients.
",0,0,1,0,0,0
250,"Improvement in the UAV position estimation with low-cost GPS, INS and vision-based system: Application to a quadrotor UAV","  In this paper, we develop a position estimation system for Unmanned Aerial
Vehicles formed by hardware and software. It is based on low-cost devices: GPS,
commercial autopilot sensors and dense optical flow algorithm implemented in an
onboard microcomputer. Comparative tests were conducted using our approach and
the conventional one, where only fusion of GPS and inertial sensors are used.
Experiments were conducted using a quadrotor in two flying modes: hovering and
trajectory tracking in outdoor environments. Results demonstrate the
effectiveness of the proposed approach in comparison with the conventional
approaches presented in the vast majority of commercial drones.
",1,0,0,0,0,0
251,Structured low rank decomposition of multivariate Hankel matrices,"  We study the decomposition of a multivariate Hankel matrix H\_$\sigma$ as a
sum of Hankel matrices of small rank in correlation with the decomposition of
its symbol $\sigma$ as a sum of polynomial-exponential series. We present a new
algorithm to compute the low rank decomposition of the Hankel operator and the
decomposition of its symbol exploiting the properties of the associated
Artinian Gorenstein quotient algebra A\_$\sigma$. A basis of A\_$\sigma$ is
computed from the Singular Value Decomposition of a sub-matrix of the Hankel
matrix H\_$\sigma$. The frequencies and the weights are deduced from the
generalized eigenvectors of pencils of shifted sub-matrices of H $\sigma$.
Explicit formula for the weights in terms of the eigenvectors avoid us to solve
a Vandermonde system. This new method is a multivariate generalization of the
so-called Pencil method for solving Prony-type decomposition problems. We
analyse its numerical behaviour in the presence of noisy input moments, and
describe a rescaling technique which improves the numerical quality of the
reconstruction for frequencies of high amplitudes. We also present a new Newton
iteration, which converges locally to the closest multivariate Hankel matrix of
low rank and show its impact for correcting errors on input moments.
",0,0,1,0,0,0
252,Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework,"  Linear time-periodic (LTP) dynamical systems frequently appear in the
modeling of phenomena related to fluid dynamics, electronic circuits, and
structural mechanics via linearization centered around known periodic orbits of
nonlinear models. Such LTP systems can reach orders that make repeated
simulation or other necessary analysis prohibitive, motivating the need for
model reduction.
We develop here an algorithmic framework for constructing reduced models that
retains the linear time-periodic structure of the original LTP system. Our
approach generalizes optimal approaches that have been established previously
for linear time-invariant (LTI) model reduction problems. We employ an
extension of the usual H2 Hardy space defined for the LTI setting to
time-periodic systems and within this broader framework develop an a posteriori
error bound expressible in terms of related LTI systems. Optimization of this
bound motivates our algorithm. We illustrate the success of our method on two
numerical examples.
",1,0,0,0,0,0
253,Software metadata: How much is enough?,"  Broad efforts are underway to capture metadata about research software and
retain it across services; notable in this regard is the CodeMeta project. What
metadata are important to have about (research) software? What metadata are
useful for searching for codes? What would you like to learn about astronomy
software? This BoF sought to gather information on metadata most desired by
researchers and users of astro software and others interested in registering,
indexing, capturing, and doing research on this software. Information from this
BoF could conceivably result in changes to the Astrophysics Source Code Library
(ASCL) or other resources for the benefit of the community or provide input
into other projects concerned with software metadata.
",1,1,0,0,0,0
254,A Categorical Approach for Recognizing Emotional Effects of Music,"  Recently, digital music libraries have been developed and can be plainly
accessed. Latest research showed that current organization and retrieval of
music tracks based on album information are inefficient. Moreover, they
demonstrated that people use emotion tags for music tracks in order to search
and retrieve them. In this paper, we discuss separability of a set of emotional
labels, proposed in the categorical emotion expression, using Fisher's
separation theorem. We determine a set of adjectives to tag music parts: happy,
sad, relaxing, exciting, epic and thriller. Temporal, frequency and energy
features have been extracted from the music parts. It could be seen that the
maximum separability within the extracted features occurs between relaxing and
epic music parts. Finally, we have trained a classifier using Support Vector
Machines to automatically recognize and generate emotional labels for a music
part. Accuracy for recognizing each label has been calculated; where the
results show that epic music can be recognized more accurately (77.4%),
comparing to the other types of music.
",1,0,0,1,0,0
255,Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores,"  One key requirement for effective supply chain management is the quality of
its inventory management. Various inventory management methods are typically
employed for different types of products based on their demand patterns,
product attributes, and supply network. In this paper, our goal is to develop
robust demand prediction methods for weather sensitive products at retail
stores. We employ historical datasets from Walmart, whose customers and markets
are often exposed to extreme weather events which can have a huge impact on
sales regarding the affected stores and products. We want to accurately predict
the sales of 111 potentially weather-sensitive products around the time of
major weather events at 45 of Walmart retails locations in the U.S.
Intuitively, we may expect an uptick in the sales of umbrellas before a big
thunderstorm, but it is difficult for replenishment managers to predict the
level of inventory needed to avoid being out-of-stock or overstock during and
after that storm. While they rely on a variety of vendor tools to predict sales
around extreme weather events, they mostly employ a time-consuming process that
lacks a systematic measure of effectiveness. We employ all the methods critical
to any analytics project and start with data exploration. Critical features are
extracted from the raw historical dataset for demand forecasting accuracy and
robustness. In particular, we employ Artificial Neural Network for forecasting
demand for each product sold around the time of major weather events. Finally,
we evaluate our model to evaluate their accuracy and robustness.
",1,0,0,1,0,0
256,Deformable Generator Network: Unsupervised Disentanglement of Appearance and Geometry,"  We propose a deformable generator model to disentangle the appearance and
geometric information from images into two independent latent vectors. The
appearance generator produces the appearance information, including color,
illumination, identity or category, of an image. The geometric generator
produces displacement of the coordinates of each pixel and performs geometric
warping, such as stretching and rotation, on the appearance generator to obtain
the final synthesized image. The proposed model can learn both representations
from image data in an unsupervised manner. The learned geometric generator can
be conveniently transferred to the other image datasets to facilitate
downstream AI tasks.
",0,0,0,1,0,0
257,Gaussian Kernel in Quantum Paradigm,"  The Gaussian kernel is a very popular kernel function used in many
machine-learning algorithms, especially in support vector machines (SVM). For
nonlinear training instances in machine learning, it often outperforms
polynomial kernels in model accuracy. We use Gaussian kernel profoundly in
formulating nonlinear classical SVM. In the recent research, P. Rebentrost
et.al. discuss a very elegant quantum version of least square support vector
machine using the quantum version of polynomial kernel, which is exponentially
faster than the classical counterparts. In this paper, we have demonstrated a
quantum version of the Gaussian kernel and analyzed its complexity in the
context of quantum SVM. Our analysis shows that the computational complexity of
the quantum Gaussian kernel is O(\epsilon^(-1)logN) with N-dimensional
instances and \epsilon with a Taylor remainder error term |R_m (\epsilon^(-1)
logN)|.
",1,0,0,0,0,0
258,Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems,"  Security, privacy, and fairness have become critical in the era of data
science and machine learning. More and more we see that achieving universally
secure, private, and fair systems is practically impossible. We have seen for
example how generative adversarial networks can be used to learn about the
expected private training data; how the exploitation of additional data can
reveal private information in the original one; and how what looks like
unrelated features can teach us about each other. Confronted with this
challenge, in this paper we open a new line of research, where the security,
privacy, and fairness is learned and used in a closed environment. The goal is
to ensure that a given entity (e.g., the company or the government), trusted to
infer certain information with our data, is blocked from inferring protected
information from it. For example, a hospital might be allowed to produce
diagnosis on the patient (the positive task), without being able to infer the
gender of the subject (negative task). Similarly, a company can guarantee that
internally it is not using the provided data for any undesired task, an
important goal that is not contradicting the virtually impossible challenge of
blocking everybody from the undesired task. We design a system that learns to
succeed on the positive task while simultaneously fail at the negative one, and
illustrate this with challenging cases where the positive task is actually
harder than the negative one being blocked. Fairness, to the information in the
negative task, is often automatically obtained as a result of this proposed
approach. The particular framework and examples open the door to security,
privacy, and fairness in very important closed scenarios, ranging from private
data accumulation companies like social networks to law-enforcement and
hospitals.
",1,0,0,1,0,0
259,Performance of Energy Harvesting Receivers with Power Optimization,"  The difficulty of modeling energy consumption in communication systems leads
to challenges in energy harvesting (EH) systems, in which nodes scavenge energy
from their environment. An EH receiver must harvest enough energy for
demodulating and decoding. The energy required depends upon factors, like code
rate and signal-to-noise ratio, which can be adjusted dynamically. We consider
a receiver which harvests energy from ambient sources and the transmitter,
meaning the received signal is used for both EH and information decoding.
Assuming a generalized function for energy consumption, we maximize the total
number of information bits decoded, under both average and peak power
constraints at the transmitter, by carefully optimizing the power used for EH,
power used for information transmission, fraction of time for EH, and code
rate. For transmission over a single block, we find there exist problem
parameters for which either maximizing power for information transmission or
maximizing power for EH is optimal. In the general case, the optimal solution
is a tradeoff of the two. For transmission over multiple blocks, we give an
upper bound on performance and give sufficient and necessary conditions to
achieve this bound. Finally, we give some numerical results to illustrate our
results and analysis.
",1,0,0,0,0,0
260,On Convergence Rate of a Continuous-Time Distributed Self-Appraisal Model with Time-Varying Relative Interaction Matrices,"  This paper studies a recently proposed continuous-time distributed
self-appraisal model with time-varying interactions among a network of $n$
individuals which are characterized by a sequence of time-varying relative
interaction matrices. The model describes the evolution of the
social-confidence levels of the individuals via a reflected appraisal mechanism
in real time. We first show by example that when the relative interaction
matrices are stochastic (not doubly stochastic), the social-confidence levels
of the individuals may not converge to a steady state. We then show that when
the relative interaction matrices are doubly stochastic, the $n$ individuals'
self-confidence levels will all converge to $1/n$, which indicates a democratic
state, exponentially fast under appropriate assumptions, and provide an
explicit expression of the convergence rate.
",0,0,1,0,0,0
261,Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration,"  When the brain receives input from multiple sensory systems, it is faced with
the question of whether it is appropriate to process the inputs in combination,
as if they originated from the same event, or separately, as if they originated
from distinct events. Furthermore, it must also have a mechanism through which
it can keep sensory inputs calibrated to maintain the accuracy of its internal
representations. We have developed a neural network architecture capable of i)
approximating optimal multisensory spatial integration, based on Bayesian
causal inference, and ii) recalibrating the spatial encoding of sensory
systems. The architecture is based on features of the dorsal processing
hierarchy, including the spatial tuning properties of unisensory neurons and
the convergence of different sensory inputs onto multisensory neurons.
Furthermore, we propose that these unisensory and multisensory neurons play
dual roles in i) encoding spatial location as separate or integrated estimates
and ii) accumulating evidence for the independence or relatedness of
multisensory stimuli. We further propose that top-down feedback connections
spanning the dorsal pathway play key a role in recalibrating spatial encoding
at the level of early unisensory cortices. Our proposed architecture provides
possible explanations for a number of human electrophysiological and
neuroimaging results and generates testable predictions linking neurophysiology
with behaviour.
",0,0,0,0,1,0
262,Block CUR: Decomposing Matrices using Groups of Columns,"  A common problem in large-scale data analysis is to approximate a matrix
using a combination of specifically sampled rows and columns, known as CUR
decomposition. Unfortunately, in many real-world environments, the ability to
sample specific individual rows or columns of the matrix is limited by either
system constraints or cost. In this paper, we consider matrix approximation by
sampling predefined \emph{blocks} of columns (or rows) from the matrix. We
present an algorithm for sampling useful column blocks and provide novel
guarantees for the quality of the approximation. This algorithm has application
in problems as diverse as biometric data analysis to distributed computing. We
demonstrate the effectiveness of the proposed algorithms for computing the
Block CUR decomposition of large matrices in a distributed setting with
multiple nodes in a compute cluster, where such blocks correspond to columns
(or rows) of the matrix stored on the same node, which can be retrieved with
much less overhead than retrieving individual columns stored across different
nodes. In the biometric setting, the rows correspond to different users and
columns correspond to users' biometric reaction to external stimuli, {\em
e.g.,}~watching video content, at a particular time instant. There is
significant cost in acquiring each user's reaction to lengthy content so we
sample a few important scenes to approximate the biometric response. An
individual time sample in this use case cannot be queried in isolation due to
the lack of context that caused that biometric reaction. Instead, collections
of time segments ({\em i.e.,} blocks) must be presented to the user. The
practical application of these algorithms is shown via experimental results
using real-world user biometric data from a content testing environment.
",1,0,0,1,0,0
263,Synchronous Observation on the Spontaneous Transformation of Liquid Metal under Free Falling Microgravity Situation,"  The unusually high surface tension of room temperature liquid metal is
molding it as unique material for diverse newly emerging areas. However, unlike
its practices on earth, such metal fluid would display very different behaviors
when working in space where gravity disappears and surface property dominates
the major physics. So far, few direct evidences are available to understand
such effect which would impede further exploration of liquid metal use for
space. Here to preliminarily probe into this intriguing issue, a low cost
experimental strategy to simulate microgravity environment on earth was
proposed through adopting bridges with high enough free falling distance as the
test platform. Then using digital cameras amounted along x, y, z directions on
outside wall of the transparent container with liquid metal and allied solution
inside, synchronous observations on the transient flow and transformational
activities of liquid metal were performed. Meanwhile, an unmanned aerial
vehicle was adopted to record the whole free falling dynamics of the test
capsule from the far end which can help justify subsequent experimental
procedures. A series of typical fundamental phenomena were thus observed as:
(a) A relatively large liquid metal object would spontaneously transform from
its original planar pool state into a sphere and float in the container if
initiating the free falling; (b) The liquid metal changes its three-dimensional
shape due to dynamic microgravity strength due to free falling and rebound of
the test capsule; and (c) A quick spatial transformation of liquid metal
immersed in the solution can easily be induced via external electrical fields.
The mechanisms of the surface tension driven liquid metal actuation in space
were interpreted. All these findings indicated that microgravity effect should
be fully treated in developing future generation liquid metal space
technologies.
",0,1,0,0,0,0
264,Continuously tempered Hamiltonian Monte Carlo,"  Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)
method for performing approximate inference in complex probabilistic models of
continuous variables. In common with many MCMC methods, however, the standard
HMC approach performs poorly in distributions with multiple isolated modes. We
present a method for augmenting the Hamiltonian system with an extra continuous
temperature control variable which allows the dynamic to bridge between
sampling a complex target distribution and a simpler unimodal base
distribution. This augmentation both helps improve mixing in multimodal targets
and allows the normalisation constant of the target distribution to be
estimated. The method is simple to implement within existing HMC code,
requiring only a standard leapfrog integrator. We demonstrate experimentally
that the method is competitive with annealed importance sampling and simulating
tempering methods at sampling from challenging multimodal distributions and
estimating their normalising constants.
",0,0,0,1,0,0
265,Automated Synthesis of Safe Digital Controllers for Sampled-Data Stochastic Nonlinear Systems,"  We present a new method for the automated synthesis of digital controllers
with formal safety guarantees for systems with nonlinear dynamics, noisy output
measurements, and stochastic disturbances. Our method derives digital
controllers such that the corresponding closed-loop system, modeled as a
sampled-data stochastic control system, satisfies a safety specification with
probability above a given threshold. The proposed synthesis method alternates
between two steps: generation of a candidate controller pc, and verification of
the candidate. pc is found by maximizing a Monte Carlo estimate of the safety
probability, and by using a non-validated ODE solver for simulating the system.
Such a candidate is therefore sub-optimal but can be generated very rapidly. To
rule out unstable candidate controllers, we prove and utilize Lyapunov's
indirect method for instability of sampled-data nonlinear systems. In the
subsequent verification step, we use a validated solver based on SMT
(Satisfiability Modulo Theories) to compute a numerically and statistically
valid confidence interval for the safety probability of pc. If the probability
so obtained is not above the threshold, we expand the search space for
candidates by increasing the controller degree. We evaluate our technique on
three case studies: an artificial pancreas model, a powertrain control model,
and a quadruple-tank process.
",1,0,0,0,0,0
266,Magnus integrators on multicore CPUs and GPUs,"  In the present paper we consider numerical methods to solve the discrete
Schrödinger equation with a time dependent Hamiltonian (motivated by problems
encountered in the study of spin systems). We will consider both short-range
interactions, which lead to evolution equations involving sparse matrices, and
long-range interactions, which lead to dense matrices. Both of these settings
show very different computational characteristics. We use Magnus integrators
for time integration and employ a framework based on Leja interpolation to
compute the resulting action of the matrix exponential. We consider both
traditional Magnus integrators (which are extensively used for these types of
problems in the literature) as well as the recently developed commutator-free
Magnus integrators and implement them on modern CPU and GPU (graphics
processing unit) based systems.
We find that GPUs can yield a significant speed-up (up to a factor of $10$ in
the dense case) for these types of problems. In the sparse case GPUs are only
advantageous for large problem sizes and the achieved speed-ups are more
modest. In most cases the commutator-free variant is superior but especially on
the GPU this advantage is rather small. In fact, none of the advantage of
commutator-free methods on GPUs (and on multi-core CPUs) is due to the
elimination of commutators. This has important consequences for the design of
more efficient numerical methods.
",1,1,0,0,0,0
267,High Dimensional Estimation and Multi-Factor Models,"  This paper re-investigates the estimation of multiple factor models relaxing
the convention that the number of factors is small and using a new approach for
identifying factors. We first obtain the collection of all possible factors and
then provide a simultaneous test, security by security, of which factors are
significant. Since the collection of risk factors is large and highly
correlated, high-dimension methods (including the LASSO and prototype
clustering) have to be used. The multi-factor model is shown to have a
significantly better fit than the Fama-French 5-factor model. Robustness tests
are also provided.
",0,0,0,1,0,1
268,Scaling Law for Three-body Collisions in Identical Fermions with $p$-wave Interactions,"  We experimentally confirmed the threshold behavior and scattering length
scaling law of the three-body loss coefficients in an ultracold spin-polarized
gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the
three-body loss coefficients as functions of temperature and scattering volume,
and found that the threshold law and the scattering length scaling law hold in
limited temperature and magnetic field regions. We also found that the
breakdown of the scaling laws is due to the emergence of the effective-range
term. This work is an important first step toward full understanding of the
loss of identical fermions with $p$-wave interactions.
",0,1,0,0,0,0
269,An Expanded Local Variance Gamma model,"  The paper proposes an expanded version of the Local Variance Gamma model of
Carr and Nadtochiy by adding drift to the governing underlying process. Still
in this new model it is possible to derive an ordinary differential equation
for the option price which plays a role of Dupire's equation for the standard
local volatility model. It is shown how calibration of multiple smiles (the
whole local volatility surface) can be done in such a case. Further, assuming
the local variance to be a piecewise linear function of strike and piecewise
constant function of time this ODE is solved in closed form in terms of
Confluent hypergeometric functions. Calibration of the model to market smiles
does not require solving any optimization problem and, in contrast, can be done
term-by-term by solving a system of non-linear algebraic equations for each
maturity, which is fast.
",0,0,0,0,0,1
270,An attentive neural architecture for joint segmentation and parsing and its application to real estate ads,"  In processing human produced text using natural language processing (NLP)
techniques, two fundamental subtasks that arise are (i) segmentation of the
plain text into meaningful subunits (e.g., entities), and (ii) dependency
parsing, to establish relations between subunits. In this paper, we develop a
relatively simple and effective neural joint model that performs both
segmentation and dependency parsing together, instead of one after the other as
in most state-of-the-art works. We will focus in particular on the real estate
ad setting, aiming to convert an ad to a structured description, which we name
property tree, comprising the tasks of (1) identifying important entities of a
property (e.g., rooms) from classifieds and (2) structuring them into a tree
format. In this work, we propose a new joint model that is able to tackle the
two tasks simultaneously and construct the property tree by (i) avoiding the
error propagation that would arise from the subtasks one after the other in a
pipelined fashion, and (ii) exploiting the interactions between the subtasks.
For this purpose, we perform an extensive comparative study of the pipeline
methods and the new proposed joint model, reporting an improvement of over
three percentage points in the overall edge F1 score of the property tree.
Also, we propose attention methods, to encourage our model to focus on salient
tokens during the construction of the property tree. Thus we experimentally
demonstrate the usefulness of attentive neural architectures for the proposed
joint model, showcasing a further improvement of two percentage points in edge
F1 score for our application.
",1,0,0,0,0,0
271,Multilevel maximum likelihood estimation with application to covariance matrices,"  The asymptotic variance of the maximum likelihood estimate is proved to
decrease when the maximization is restricted to a subspace that contains the
true parameter value. Maximum likelihood estimation allows a systematic fitting
of covariance models to the sample, which is important in data assimilation.
The hierarchical maximum likelihood approach is applied to the spectral
diagonal covariance model with different parameterizations of eigenvalue decay,
and to the sparse inverse covariance model with specified parameter values on
different sets of nonzero entries. It is shown computationally that using
smaller sets of parameters can decrease the sampling noise in high dimension
substantially.
",0,0,1,1,0,0
272,Auto-Meta: Automated Gradient Based Meta Learner Search,"  Fully automating machine learning pipelines is one of the key challenges of
current artificial intelligence research, since practical machine learning
often requires costly and time-consuming human-powered processes such as model
design, algorithm development, and hyperparameter tuning. In this paper, we
verify that automated architecture search synergizes with the effect of
gradient-based meta learning. We adopt the progressive neural architecture
search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal
architectures for meta-learners. The gradient based meta-learner whose
architecture was automatically found achieved state-of-the-art results on the
5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy,
which is $11.54\%$ improvement over the result obtained by the first
gradient-based meta-learner called MAML
\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is
the first successful neural architecture search implementation in the context
of meta learning.
",0,0,0,1,0,0
273,Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry,"  We provide a comprehensive study of the convergence of forward-backward
algorithm under suitable geometric conditions leading to fast rates. We present
several new results and collect in a unified view a variety of results
scattered in the literature, often providing simplified proofs. Novel
contributions include the analysis of infinite dimensional convex minimization
problems, allowing the case where minimizers might not exist. Further, we
analyze the relation between different geometric conditions, and discuss novel
connections with a priori conditions in linear inverse problems, including
source conditions, restricted isometry properties and partial smoothness.
",0,0,1,1,0,0
274,Calibration-Free Relaxation-Based Multi-Color Magnetic Particle Imaging,"  Magnetic Particle Imaging (MPI) is a novel imaging modality with important
applications such as angiography, stem cell tracking, and cancer imaging.
Recently, there have been efforts to increase the functionality of MPI via
multi-color imaging methods that can distinguish the responses of different
nanoparticles, or nanoparticles in different environmental conditions. The
proposed techniques typically rely on extensive calibrations that capture the
differences in the harmonic responses of the nanoparticles. In this work, we
propose a method to directly estimate the relaxation time constant of the
nanoparticles from the MPI signal, which is then used to generate a multi-color
relaxation map. The technique is based on the underlying mirror symmetry of the
adiabatic MPI signal when the same region is scanned back and forth. We
validate the proposed method via extensive simulations, and via experiments on
our in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our
in-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be
successfully distinguished with the proposed technique, without any calibration
or prior knowledge about the nanoparticles.
",0,1,0,0,0,0
275,Neural Machine Translation,"  Draft of textbook chapter on neural machine translation. a comprehensive
treatment of the topic, ranging from introduction to neural networks,
computation graphs, description of the currently dominant attentional
sequence-to-sequence model, recent refinements, alternative architectures and
challenges. Written as chapter for the textbook Statistical Machine
Translation. Used in the JHU Fall 2017 class on machine translation.
",1,0,0,0,0,0
276,On algebraically integrable domains in Euclidean spaces,"  Let $D$ be a bounded domain $D$ in $\mathbb R^n $ with infinitely smooth
boundary and $n$ is odd. We prove that if the volume cut off from the domain by
a hyperplane is an algebraic function of the hyperplane, free of real singular
points, then the domain is an ellipsoid. This partially answers a question of
V.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically
integrable domains?
",0,0,1,0,0,0
277,Vocabulary-informed Extreme Value Learning,"  The novel unseen classes can be formulated as the extreme values of known
classes. This inspired the recent works on open-set recognition
\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no
way of naming the novel unseen classes. To solve this problem, we propose the
Extreme Value Learning (EVL) formulation to learn the mapping from visual
feature to semantic space. To model the margin and coverage distributions of
each class, the Vocabulary-informed Learning (ViL) is adopted by using vast
open vocabulary in the semantic space. Essentially, by incorporating the EVL
and ViL, we for the first time propose a novel semantic embedding paradigm --
Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual
features into semantic space in a probabilistic way. The learned embedding can
be directly used to solve supervised learning, zero-shot and open set
recognition simultaneously. Experiments on two benchmark datasets demonstrate
the effectiveness of proposed frameworks.
",1,0,1,1,0,0
278,Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks,"  In this paper, we study the performance of two cross-layer optimized dynamic
routing techniques for radio interference mitigation across multiple coexisting
wireless body area networks (BANs), based on real-life measurements. At the
network layer, the best route is selected according to channel state
information from the physical layer, associated with low duty cycle TDMA at the
MAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel
cooperative multi-path routing (CMR) incorporating 3-branch selection
combining) perform real-time and reliable data transfer across BANs operating
near the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'
mixed-activities is used for analyzing the proposed cross-layer optimization.
We show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and
even 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage
probability at a realistic signal-to-interference-plus-noise ratio (SINR).
Acceptable packet delivery ratios (PDR) and spectral efficiencies are obtained
from SPR and CMR with reasonably sensitive receivers across a range of TDMA low
duty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The
distribution fits for received SINR through routing are also derived and
validated with theoretical analysis.
",1,0,0,0,0,0
279,A Team-Formation Algorithm for Faultline Minimization,"  In recent years, the proliferation of online resumes and the need to evaluate
large populations of candidates for on-site and virtual teams have led to a
growing interest in automated team-formation. Given a large pool of candidates,
the general problem requires the selection of a team of experts to complete a
given task. Surprisingly, while ongoing research has studied numerous
variations with different constraints, it has overlooked a factor with a
well-documented impact on team cohesion and performance: team faultlines.
Addressing this gap is challenging, as the available measures for faultlines in
existing teams cannot be efficiently applied to faultline optimization. In this
work, we meet this challenge with a new measure that can be efficiently used
for both faultline measurement and minimization. We then use the measure to
solve the problem of automatically partitioning a large population into
low-faultline teams. By introducing faultlines to the team-formation
literature, our work creates exciting opportunities for algorithmic work on
faultline optimization, as well as on work that combines and studies the
connection of faultlines with other influential team characteristics.
",1,0,0,0,0,0
280,A Survey of Model Compression and Acceleration for Deep Neural Networks,"  Deep convolutional neural networks (CNNs) have recently achieved great
success in many visual recognition tasks. However, existing deep neural network
models are computationally expensive and memory intensive, hindering their
deployment in devices with low memory resources or in applications with strict
latency requirements. Therefore, a natural thought is to perform model
compression and acceleration in deep networks without significantly decreasing
the model performance. During the past few years, tremendous progress has been
made in this area. In this paper, we survey the recent advanced techniques for
compacting and accelerating CNNs model developed. These techniques are roughly
categorized into four schemes: parameter pruning and sharing, low-rank
factorization, transferred/compact convolutional filters, and knowledge
distillation. Methods of parameter pruning and sharing will be described at the
beginning, after that the other techniques will be introduced. For each scheme,
we provide insightful analysis regarding the performance, related applications,
advantages, and drawbacks etc. Then we will go through a few very recent
additional successful methods, for example, dynamic capacity networks and
stochastic depths networks. After that, we survey the evaluation matrix, the
main datasets used for evaluating the model performance and recent benchmarking
efforts. Finally, we conclude this paper, discuss remaining challenges and
possible directions on this topic.
",1,0,0,0,0,0
281,Non-Parametric Calibration of Probabilistic Regression,"  The task of calibration is to retrospectively adjust the outputs from a
machine learning model to provide better probability estimates on the target
variable. While calibration has been investigated thoroughly in classification,
it has not yet been well-established for regression tasks. This paper considers
the problem of calibrating a probabilistic regression model to improve the
estimated probability densities over the real-valued targets. We propose to
calibrate a regression model through the cumulative probability density, which
can be derived from calibrating a multi-class classifier. We provide three
non-parametric approaches to solve the problem, two of which provide empirical
estimates and the third providing smooth density estimates. The proposed
approaches are experimentally evaluated to show their ability to improve the
performance of regression models on the predictive likelihood.
",0,0,0,1,0,0
282,Cyclotron resonant scattering feature simulations. II. Description of the CRSF simulation process,"  Cyclotron resonant scattering features (CRSFs) are formed by scattering of
X-ray photons off quantized plasma electrons in the strong magnetic field (of
the order 10^12 G) close to the surface of an accreting X-ray pulsar. The line
profiles of CRSFs cannot be described by an analytic expression. Numerical
methods such as Monte Carlo (MC) simulations of the scattering processes are
required in order to predict precise line shapes for a given physical setup,
which can be compared to observations to gain information about the underlying
physics in these systems.
A versatile simulation code is needed for the generation of synthetic
cyclotron lines. Sophisticated geometries should be investigatable by making
their simulation possible for the first time.
The simulation utilizes the mean free path tables described in the first
paper of this series for the fast interpolation of propagation lengths. The
code is parallelized to make the very time consuming simulations possible on
convenient time scales. Furthermore, it can generate responses to
mono-energetic photon injections, producing Green's functions, which can be
used later to generate spectra for arbitrary continua.
We develop a new simulation code to generate synthetic cyclotron lines for
complex scenarios, allowing for unprecedented physical interpretation of the
observed data. An associated XSPEC model implementation is used to fit
synthetic line profiles to NuSTAR data of Cep X-4. The code has been developed
with the main goal of overcoming previous geometrical constraints in MC
simulations of CRSFs. By applying this code also to more simple, classic
geometries used in previous works, we furthermore address issues of code
verification and cross-comparison of various models. The XSPEC model and the
Green's function tables are available online at
this http URL .
",0,1,0,0,0,0
283,New quantum mds constacylıc codes,"  This paper is devoted to the study of the construction of new quantum MDS
codes. Based on constacyclic codes over Fq2 , we derive four new families of
quantum MDS codes, one of which is an explicit generalization of the
construction given in Theorem 7 in [22]. We also extend the result of Theorem
3:3 given in [17].
",1,0,0,0,0,0
284,Infinitary first-order categorical logic,"  We present a unified categorical treatment of completeness theorems for
several classical and intuitionistic infinitary logics with a proposed
axiomatization. This provides new completeness theorems and subsumes previous
ones by Gödel, Kripke, Beth, Karp, Joyal, Makkai and Fourman/Grayson. As an
application we prove, using large cardinals assumptions, the disjunction and
existence properties for infinitary intuitionistic first-order logics.
",0,0,1,0,0,0
285,Stochastic Gradient Monomial Gamma Sampler,"  Recent advances in stochastic gradient techniques have made it possible to
estimate posterior distributions from large datasets via Markov Chain Monte
Carlo (MCMC). However, when the target posterior is multimodal, mixing
performance is often poor. This results in inadequate exploration of the
posterior distribution. A framework is proposed to improve the sampling
efficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A
generalized kinetic function is leveraged, delivering superior stationary
mixing, especially for multimodal distributions. Techniques are also discussed
to overcome the practical issues introduced by this generalization. It is shown
that the proposed approach is better at exploring complex multimodal posterior
distributions, as demonstrated on multiple applications and in comparison with
other stochastic gradient MCMC methods.
",1,0,0,1,0,0
286,Gini estimation under infinite variance,"  We study the problems related to the estimation of the Gini index in presence
of a fat-tailed data generating process, i.e. one in the stable distribution
class with finite mean but infinite variance (i.e. with tail index
$\alpha\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be
reliably estimated using conventional nonparametric methods, because of a
downward bias that emerges under fat tails. This has important implications for
the ongoing discussion about economic inequality.
We start by discussing how the nonparametric estimator of the Gini index
undergoes a phase transition in the symmetry structure of its asymptotic
distribution, as the data distribution shifts from the domain of attraction of
a light-tailed distribution to that of a fat-tailed one, especially in the case
of infinite variance. We also show how the nonparametric Gini bias increases
with lower values of $\alpha$. We then prove that maximum likelihood estimation
outperforms nonparametric methods, requiring a much smaller sample size to
reach efficiency.
Finally, for fat-tailed data, we provide a simple correction mechanism to the
small sample bias of the nonparametric estimator based on the distance between
the mode and the mean of its asymptotic distribution.
",0,0,0,1,0,0
287,Training Neural Networks Using Features Replay,"  Training a neural network using backpropagation algorithm requires passing
error gradients sequentially through the network. The backward locking prevents
us from updating network layers in parallel and fully leveraging the computing
resources. Recently, there are several works trying to decouple and parallelize
the backpropagation algorithm. However, all of them suffer from severe accuracy
loss or memory explosion when the neural network is deep. To address these
challenging issues, we propose a novel parallel-objective formulation for the
objective function of the neural network. After that, we introduce features
replay algorithm and prove that it is guaranteed to converge to critical points
for the non-convex problem under certain conditions. Finally, we apply our
method to training deep convolutional neural networks, and the experimental
results show that the proposed method achieves {faster} convergence, {lower}
memory consumption, and {better} generalization error than compared methods.
",0,0,0,1,0,0
288,The anti-spherical category,"  We study a diagrammatic categorification (the ""anti-spherical category"") of
the anti-spherical module for any Coxeter group. We deduce that Deodhar's
(sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients,
and that a monotonicity conjecture of Brenti's holds. The main technical
observation is a localisation procedure for the anti-spherical category, from
which we construct a ""light leaves"" basis of morphisms. Our techniques may be
used to calculate many new elements of the $p$-canonical basis in the
anti-spherical module. The results use generators and relations for Soergel
bimodules (""Soergel calculus"") in a crucial way.
",0,0,1,0,0,0
289,Trajectories and orbital angular momentum of necklace beams in nonlinear colloidal suspensions,"  Recently, we have predicted that the modulation instability of optical vortex
solitons propagating in nonlinear colloidal suspensions with exponential
saturable nonlinearity leads to formation of necklace beams (NBs)
[S.~Z.~Silahli, W.~Walasik and N.~M.~Litchinitser, Opt.~Lett., \textbf{40},
5714 (2015)]. Here, we investigate the dynamics of NB formation and
propagation, and show that the distance at which the NB is formed depends on
the input power of the vortex beam. Moreover, we show that the NB trajectories
are not necessarily tangent to the initial vortex ring, and that their
velocities have components stemming both from the beam diffraction and from the
beam orbital angular momentum. We also demonstrate the generation of twisted
solitons and analyze the influence of losses on their propagation. Finally, we
investigate the conservation of the orbital angular momentum in necklace and
twisted beams. Our studies, performed in ideal lossless media and in realistic
colloidal suspensions with losses, provide a detailed description of NB
dynamics and may be useful in studies of light propagation in highly scattering
colloids and biological samples.
",0,1,0,0,0,0
290,Unified Treatment of Spin Torques using a Coupled Magnetisation Dynamics and Three-Dimensional Spin Current Solver,"  A three-dimensional spin current solver based on a generalised spin
drift-diffusion description, including the spin Hall effect, is integrated with
a magnetisation dynamics solver. The resulting model is shown to simultaneously
reproduce the spin-orbit torques generated using the spin Hall effect, spin
pumping torques generated by magnetisation dynamics in multilayers, as well as
the spin transfer torques acting on magnetisation regions with spatial
gradients, whilst field-like and spin-like torques are reproduced in a spin
valve geometry. Two approaches to modelling interfaces are analysed, one based
on the spin mixing conductance and the other based on continuity of spin
currents where the spin dephasing length governs the absorption of transverse
spin components. In both cases analytical formulas are derived for the
spin-orbit torques in a heavy metal / ferromagnet bilayer geometry, showing in
general both field-like and damping-like torques are generated. The limitations
of the analytical approach are discussed, showing that even in a simple bilayer
geometry, due to the non-uniformity of the spin currents, a full
three-dimensional treatment is required. Finally the model is applied to the
quantitative analysis of the spin Hall angle in Pt by reproducing published
experimental data on the ferromagnetic resonance linewidth in the bilayer
geometry.
",0,1,0,0,0,0
291,A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization,"  This paper aims to explore models based on the extreme gradient boosting
(XGBoost) approach for business risk classification. Feature selection (FS)
algorithms and hyper-parameter optimizations are simultaneously considered
during model training. The five most commonly used FS methods including weight
by Gini, weight by Chi-square, hierarchical variable clustering, weight by
correlation, and weight by information are applied to alleviate the effect of
redundant features. Two hyper-parameter optimization approaches, random search
(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
XGBoost. The effect of different FS and hyper-parameter optimization methods on
the model performance are investigated by the Wilcoxon Signed Rank Test. The
performance of XGBoost is compared to the traditionally utilized logistic
regression (LR) model in terms of classification accuracy, area under the curve
(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
show that hierarchical clustering is the optimal FS method for LR while weight
by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
optimization in XGBoost outperform LR significantly. TPE optimization shows a
superiority over RS since it results in a significantly higher accuracy and a
marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
tuning shows a lower variability than the RS method. Finally, the ranking of
feature importance based on XGBoost enhances the model interpretation.
Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
operative while powerful approach for business risk modeling.
",1,0,0,1,0,0
292,Rheology of High-Capillary Number Flow in Porous Media,"  Immiscible fluids flowing at high capillary numbers in porous media may be
characterized by an effective viscosity. We demonstrate that the effective
viscosity is well described by the Lichtenecker-Rother equation. The exponent
$\alpha$ in this equation takes either the value 1 or 0.6 in two- and 0.5 in
three-dimensional systems depending on the pore geometry. Our arguments are
based on analytical and numerical methods.
",0,1,0,0,0,0
293,Quantum Charge Pumps with Topological Phases in Creutz Ladder,"  Quantum charge pumping phenomenon connects band topology through the dynamics
of a one-dimensional quantum system. In terms of a microscopic model, the
Su-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful
starting point for many considerations of topological physics. Here we present
a generalized Creutz scheme as a distinct two-band quantum pump model. By
noting that it undergoes two kinds of topological band transitions accompanying
with a Zak-phase-difference of $\pi$ and $2\pi$, respectively, various charge
pumping schemes are studied by applying an elaborate Peierl's phase
substitution. Translating into real space, the transportation of quantized
charges is a result of cooperative quantum interference effect. In particular,
an all-flux quantum pump emerges which operates with time-varying fluxes only
and transports two charge units. This puts cold atoms with artificial gauge
fields as an unique system where this kind of phenomena can be realized.
",0,1,0,0,0,0
294,On Deep Neural Networks for Detecting Heart Disease,"  Heart disease is the leading cause of death, and experts estimate that
approximately half of all heart attacks and strokes occur in people who have
not been flagged as ""at risk."" Thus, there is an urgent need to improve the
accuracy of heart disease diagnosis. To this end, we investigate the potential
of using data analysis, and in particular the design and use of deep neural
networks (DNNs) for detecting heart disease based on routine clinical data. Our
main contribution is the design, evaluation, and optimization of DNN
architectures of increasing depth for heart disease diagnosis. This work led to
the discovery of a novel five layer DNN architecture - named Heart Evaluation
for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
best prediction accuracy. HEARO-5's design employs regularization optimization
and automatically deals with missing data and/or data outliers. To evaluate and
tune the architectures we use k-way cross-validation as well as Matthews
correlation coefficient (MCC) to measure the quality of our classifications.
The study is performed on the publicly available Cleveland dataset of medical
information, and we are making our developments open source, to further
facilitate openness and research on the use of DNNs in medicine. The HEARO-5
architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
currently published research in the area.
",1,0,0,1,0,0
295,Exponential Stability Analysis via Integral Quadratic Constraints,"  The theory of integral quadratic constraints (IQCs) allows verification of
stability and gain-bound properties of systems containing nonlinear or
uncertain elements. Gain bounds often imply exponential stability, but it can
be challenging to compute useful numerical bounds on the exponential decay
rate. This work presents a generalization of the classical IQC results of
Megretski and Rantzer that leads to a tractable computational procedure for
finding exponential rate certificates that are far less conservative than ones
computed from $L_2$ gain bounds alone. An expanded library of IQCs for
certifying exponential stability is also provided and the effectiveness of the
technique is demonstrated via numerical examples.
",1,0,1,0,0,0
296,Fermi acceleration of electrons inside foreshock transient cores,"  Foreshock transients upstream of Earth's bow shock have been recently
observed to accelerate electrons to many times their thermal energy. How such
acceleration occurs is unknown, however. Using THEMIS case studies, we examine
a subset of acceleration events (31 of 247 events) in foreshock transients with
cores that exhibit gradual electron energy increases accompanied by low
background magnetic field strength and large-amplitude magnetic fluctuations.
Using the evolution of electron distributions and the energy increase rates at
multiple spacecraft, we suggest that Fermi acceleration between a converging
foreshock transient's compressional boundary and the bow shock is responsible
for the observed electron acceleration. We then show that a one-dimensional
test particle simulation of an ideal Fermi acceleration model in fluctuating
fields prescribed by the observations can reproduce the observed evolution of
electron distributions, energy increase rate, and pitch-angle isotropy,
providing further support for our hypothesis. Thus, Fermi acceleration is
likely the principal electron acceleration mechanism in at least this subset of
foreshock transient cores.
",0,1,0,0,0,0
297,Quantum Speed Limit is Not Quantum,"  The quantum speed limit (QSL), or the energy-time uncertainty relation,
describes the fundamental maximum rate for quantum time evolution and has been
regarded as being unique in quantum mechanics. In this study, we obtain a
classical speed limit corresponding to the QSL using the Hilbert space for the
classical Liouville equation. Thus, classical mechanics has a fundamental speed
limit, and QSL is not a purely quantum phenomenon but a universal dynamical
property of the Hilbert space. Furthermore, we obtain similar speed limits for
the imaginary-time Schroedinger equations such as the master equation.
",0,1,0,0,0,0
298,Adaptive Diffusion Processes of Time-Varying Local Information on Networks,"  This paper mainly discusses the diffusion on complex networks with
time-varying couplings. We propose a model to describe the adaptive diffusion
process of local topological and dynamical information, and find that the
Barabasi-Albert scale-free network (BA network) is beneficial to the diffusion
and leads nodes to arrive at a larger state value than other networks do. The
ability of diffusion for a node is related to its own degree. Specifically,
nodes with smaller degrees are more likely to change their states and reach
larger values, while those with larger degrees tend to stick to their original
states. We introduce state entropy to analyze the thermodynamic mechanism of
the diffusion process, and interestingly find that this kind of diffusion
process is a minimization process of state entropy. We use the inequality
constrained optimization method to reveal the restriction function of the
minimization and find that it has the same form as the Gibbs free energy. The
thermodynamical concept allows us to understand dynamical processes on complex
networks from a brand-new perspective. The result provides a convenient means
of optimizing relevant dynamical processes on practical circuits as well as
related complex systems.
",1,0,0,0,0,0
299,ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder,"  This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to construct the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of the input speech into its converted version. Subjective evaluation
experiments revealed that this simple method worked reasonably well in a
non-parallel many-to-many speaker identity conversion task.
",1,0,0,1,0,0
300,Spectral analysis of jet turbulence,"  Informed by LES data and resolvent analysis of the mean flow, we examine the
structure of turbulence in jets in the subsonic, transonic, and supersonic
regimes. Spectral (frequency-space) proper orthogonal decomposition is used to
extract energy spectra and decompose the flow into energy-ranked coherent
structures. The educed structures are generally well predicted by the resolvent
analysis. Over a range of low frequencies and the first few azimuthal mode
numbers, these jets exhibit a low-rank response characterized by
Kelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer
up to the end of the potential core and that are excited by forcing in the
very-near-nozzle shear layer. These modes too the have been experimentally
observed before and predicted by quasi-parallel stability theory and other
approximations--they comprise a considerable portion of the total turbulent
energy. At still lower frequencies, particularly for the axisymmetric mode, and
again at high frequencies for all azimuthal wavenumbers, the response is not
low rank, but consists of a family of similarly amplified modes. These modes,
which are primarily active downstream of the potential core, are associated
with the Orr mechanism. They occur also as sub-dominant modes in the range of
frequencies dominated by the KH response. Our global analysis helps tie
together previous observations based on local spatial stability theory, and
explains why quasi-parallel predictions were successful at some frequencies and
azimuthal wavenumbers, but failed at others.
",0,1,0,0,0,0
301,A dual framework for low-rank tensor completion,"  One of the popular approaches for low-rank tensor completion is to use the
latent trace norm regularization. However, most existing works in this
direction learn a sparse combination of tensors. In this work, we fill this gap
by proposing a variant of the latent trace norm that helps in learning a
non-sparse combination of tensors. We develop a dual framework for solving the
low-rank tensor completion problem. We first show a novel characterization of
the dual solution space with an interesting factorization of the optimal
solution. Overall, the optimal solution is shown to lie on a Cartesian product
of Riemannian manifolds. Furthermore, we exploit the versatile Riemannian
optimization framework for proposing computationally efficient trust region
algorithm. The experiments illustrate the efficacy of the proposed algorithm on
several real-world datasets across applications.
",1,0,0,1,0,0
302,La notion d'involution dans le Brouillon Project de Girard Desargues,"  Nous tentons dans cet article de proposer une thèse cohérente concernant
la formation de la notion d'involution dans le Brouillon Project de Desargues.
Pour cela, nous donnons une analyse détaillée des dix premières pages
dudit Brouillon, comprenant les développements de cas particuliers qui aident
à comprendre l'intention de Desargues. Nous mettons cette analyse en regard
de la lecture qu'en fait Jean de Beaugrand et que l'on trouve dans les Advis
Charitables.
The purpose of this article is to propose a coherent thesis on how Girard
Desargues arrived at the notion of involution in his Brouillon Project of 1639.
To this purpose we give a detailed analysis of the ten first pages of the
Brouillon, including developments of particular cases which help to understand
the goal of Desargues, as well as to clarify the links between the notion of
involution and that of harmonic division. We compare the conclusions of this
analysis with the very critical reading Jean de Beaugrand made of the Brouillon
Project in the Advis Charitables of 1640.
",0,0,1,0,0,0
303,Framing U-Net via Deep Convolutional Framelets: Application to Sparse-view CT,"  X-ray computed tomography (CT) using sparse projection views is a recent
approach to reduce the radiation dose. However, due to the insufficient
projection views, an analytic reconstruction approach using the filtered back
projection (FBP) produces severe streaking artifacts. Recently, deep learning
approaches using large receptive field neural networks such as U-Net have
demonstrated impressive performance for sparse- view CT reconstruction.
However, theoretical justification is still lacking. Inspired by the recent
theory of deep convolutional framelets, the main goal of this paper is,
therefore, to reveal the limitation of U-Net and propose new multi-resolution
deep learning schemes. In particular, we show that the alternative U- Net
variants such as dual frame and the tight frame U-Nets satisfy the so-called
frame condition which make them better for effective recovery of high frequency
edges in sparse view- CT. Using extensive experiments with real patient data
set, we demonstrate that the new network architectures provide better
reconstruction performance.
",1,0,0,1,0,0
304,Lie $\infty$-algebroids and singular foliations,"  A singular (or Hermann) foliation on a smooth manifold $M$ can be seen as a
subsheaf of the sheaf $\mathfrak{X}$ of vector fields on $M$. We show that if
this singular foliation admits a resolution (in the sense of sheaves)
consisting of sections of a graded vector bundle of finite type, then one can
lift the Lie bracket of vector fields to a Lie $\infty$-algebroid structure on
this resolution, that we call a universal Lie $\infty$-algebroid associated to
the foliation. The name is justified because it is isomorphic (up to homotopy)
to any other Lie $\infty$-algebroid structure built on any other resolution of
the given singular foliation.
",0,0,1,0,0,0
305,Distinct evolutions of Weyl fermion quasiparticles and Fermi arcs with bulk band topology in Weyl semimetals,"  The Weyl semimetal phase is a recently discovered topological quantum state
of matter characterized by the presence of topologically protected degeneracies
near the Fermi level. These degeneracies are the source of exotic phenomena,
including the realization of chiral Weyl fermions as quasiparticles in the bulk
and the formation of Fermi arc states on the surfaces. Here, we demonstrate
that these two key signatures show distinct evolutions with the bulk band
topology by performing angle-resolved photoemission spectroscopy, supported by
first-principle calculations, on transition-metal monophosphides. While Weyl
fermion quasiparticles exist only when the chemical potential is located
between two saddle points of the Weyl cone features, the Fermi arc states
extend in a larger energy scale and are robust across the bulk Lifshitz
transitions associated with the recombination of two non-trivial Fermi surfaces
enclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl
points of opposite chirality. Therefore, in some systems (e.g. NbP),
topological Fermi arc states are preserved even if Weyl fermion quasiparticles
are absent in the bulk. Our findings not only provide insight into the
relationship between the exotic physical phenomena and the intrinsic bulk band
topology in Weyl semimetals, but also resolve the apparent puzzle of the
different magneto-transport properties observed in TaAs, TaP and NbP, where the
Fermi arc states are similar.
",0,1,0,0,0,0
306,Assessing inter-modal and inter-regional dependencies in prodromal Alzheimer's disease using multimodal MRI/PET and Gaussian graphical models,"  A sequence of pathological changes takes place in Alzheimer's disease, which
can be assessed in vivo using various brain imaging methods. Currently, there
is no appropriate statistical model available that can easily integrate
multiple imaging modalities, being able to utilize the additional information
provided from the combined data. We applied Gaussian graphical models (GGMs)
for analyzing the conditional dependency networks of multimodal neuroimaging
data and assessed alterations of the network structure in mild cognitive
impairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy
controls.
Data from N=667 subjects were obtained from the Alzheimer's Disease
Neuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism
(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.
Separate GGMs were estimated using a Bayesian framework for the combined
multimodal data for each diagnostic category. Graph-theoretical statistics were
calculated to determine network alterations associated with disease severity.
Network measures clustering coefficient, path length and small-world
coefficient were significantly altered across diagnostic groups, with a
biphasic u-shape trajectory, i.e. increased small-world coefficient in early
MCI, intermediate values in late MCI, and decreased values in AD patients
compared to controls. In contrast, no group differences were found for
clustering coefficient and small-world coefficient when estimating conditional
dependency networks on single imaging modalities.
GGMs provide a useful methodology to analyze the conditional dependency
networks of multimodal neuroimaging data.
",0,0,0,1,1,0
307,On the economics of knowledge creation and sharing,"  This work bridges the technical concepts underlying distributed computing and
blockchain technologies with their profound socioeconomic and sociopolitical
implications, particularly on academic research and the healthcare industry.
Several examples from academia, industry, and healthcare are explored
throughout this paper. The limiting factor in contemporary life sciences
research is often funding: for example, to purchase expensive laboratory
equipment and materials, to hire skilled researchers and technicians, and to
acquire and disseminate data through established academic channels. In the case
of the U.S. healthcare system, hospitals generate massive amounts of data, only
a small minority of which is utilized to inform current and future medical
practice. Similarly, corporations too expend large amounts of money to collect,
secure and transmit data from one centralized source to another. In all three
scenarios, data moves under the traditional paradigm of centralization, in
which data is hosted and curated by individuals and organizations and of
benefit to only a small subset of people.
",1,0,0,0,0,0
308,Seismic fragility curves for structures using non-parametric representations,"  Fragility curves are commonly used in civil engineering to assess the
vulnerability of structures to earthquakes. The probability of failure
associated with a prescribed criterion (e.g. the maximal inter-storey drift of
a building exceeding a certain threshold) is represented as a function of the
intensity of the earthquake ground motion (e.g. peak ground acceleration or
spectral acceleration). The classical approach relies on assuming a lognormal
shape of the fragility curves; it is thus parametric. In this paper, we
introduce two non-parametric approaches to establish the fragility curves
without employing the above assumption, namely binned Monte Carlo simulation
and kernel density estimation. As an illustration, we compute the fragility
curves for a three-storey steel frame using a large number of synthetic ground
motions. The curves obtained with the non-parametric approaches are compared
with respective curves based on the lognormal assumption. A similar comparison
is presented for a case when a limited number of recorded ground motions is
available. It is found that the accuracy of the lognormal curves depends on the
ground motion intensity measure, the failure criterion and most importantly, on
the employed method for estimating the parameters of the lognormal shape.
",0,0,0,1,0,0
309,Metastable Markov chains: from the convergence of the trace to the convergence of the finite-dimensional distributions,"  We consider continuous-time Markov chains which display a family of wells at
the same depth. We provide sufficient conditions which entail the convergence
of the finite-dimensional distributions of the order parameter to the ones of a
finite state Markov chain. We also show that the state of the process can be
represented as a time-dependent convex combination of metastable states, each
of which is supported on one well.
",0,0,1,0,0,0
310,Construction of embedded periodic surfaces in $\mathbb{R}^n$,"  We construct embedded minimal surfaces which are $n$-periodic in
$\mathbb{R}^n$. They are new for codimension $n-2\ge 2$. We start with a Jordan
curve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk
which Schwarz reflection extends to a complete minimal surface. Studying the
group of Schwarz reflections, we can characterize those Jordan curves for which
the complete surface is embedded. For example, for $n=4$ exactly five such
Jordan curves generate embedded surfaces. Our results apply to surface classes
other than minimal as well, for instance polygonal surfaces.
",0,0,1,0,0,0
311,A System of Three Super Earths Transiting the Late K-Dwarf GJ 9827 at Thirty Parsecs,"  We report the discovery of three small transiting planets orbiting GJ 9827, a
bright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\pm0.11$
$R_{\rm \oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$
$R_{\rm \oplus}$ super Earth on a 3.6 day period, and a $2.07\pm0.14$ $R_{\rm
\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting
GJ 9827 span the transition between predominantly rocky and gaseous planets,
and GJ 9827 b and c fall in or close to the known gap in the radius
distribution of small planets between these populations. At a distance of 30
parsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making
these planets well-suited for atmospheric studies with the upcoming James Webb
Space Telescope. The GJ 9827 system provides a valuable opportunity to
characterize interior structure and atmospheric properties of coeval planets
spanning the rocky to gaseous transition.
",0,1,0,0,0,0
312,State Sum Invariants of Three Manifolds from Spherical Multi-fusion Categories,"  We define a family of quantum invariants of closed oriented $3$-manifolds
using spherical multi-fusion categories. The state sum nature of this invariant
leads directly to $(2+1)$-dimensional topological quantum field theories
($\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury
($\text{TVBW}$) $\text{TQFT}$s from spherical fusion categories. The invariant
is given as a state sum over labeled triangulations, which is mostly parallel
to, but richer than the $\text{TVBW}$ approach in that here the labels live not
only on $1$-simplices but also on $0$-simplices. It is shown that a
multi-fusion category in general cannot be a spherical fusion category in the
usual sense. Thus we introduce the concept of a spherical multi-fusion category
by imposing a weakened version of sphericity. Besides containing the
$\text{TVBW}$ theory, our construction also includes the recent higher gauge
theory $(2+1)$-$\text{TQFT}$s given by Kapustin and Thorngren, which was not
known to have a categorical origin before.
",0,1,1,0,0,0
313,Sparse Neural Networks Topologies,"  We propose Sparse Neural Network architectures that are based on random or
structured bipartite graph topologies. Sparse architectures provide compression
of the models learned and speed-ups of computations, they can also surpass
their unstructured or fully connected counterparts. As we show, even more
compact topologies of the so-called SNN (Sparse Neural Network) can be achieved
with the use of structured graphs of connections between consecutive layers of
neurons. In this paper, we investigate how the accuracy and training speed of
the models depend on the topology and sparsity of the neural network. Previous
approaches using sparcity are all based on fully connected neural network
models and create sparcity during training phase, instead we explicitly define
a sparse architectures of connections before the training. Building compact
neural network models is coherent with empirical observations showing that
there is much redundancy in learned neural network models. We show
experimentally that the accuracy of the models learned with neural networks
depends on expander-like properties of the underlying topologies such as the
spectral gap and algebraic connectivity rather than the density of the graphs
of connections.
",1,0,0,1,0,0
314,Human perception in computer vision,"  Computer vision has made remarkable progress in recent years. Deep neural
network (DNN) models optimized to identify objects in images exhibit
unprecedented task-trained accuracy and, remarkably, some generalization
ability: new visual problems can now be solved more easily based on previous
learning. Biological vision (learned in life and through evolution) is also
accurate and general-purpose. Is it possible that these different learning
regimes converge to similar problem-dependent optimal computations? We
therefore asked whether the human system-level computation of visual perception
has DNN correlates and considered several anecdotal test cases. We found that
perceptual sensitivity to image changes has DNN mid-computation correlates,
while sensitivity to segmentation, crowding and shape has DNN end-computation
correlates. Our results quantify the applicability of using DNN computation to
estimate perceptual loss, and are consistent with the fascinating theoretical
view that properties of human perception are a consequence of
architecture-independent visual learning.
",1,0,0,0,0,0
315,Analyses and estimation of certain design parameters of micro-grooved heat pipes,"  A numerical analysis of heat conduction through the cover plate of a heat
pipe is carried out to determine the temperature of the working substance,
average temperature of heating and cooling surfaces, heat spread in the
transmitter, and the heat bypass through the cover plate. Analysis has been
extended for the estimation of heat transfer requirements at the outer surface
of the con- denser under different heat load conditions using Genetic
Algorithm. This paper also presents the estimation of an average heat transfer
coefficient for the boiling and condensation of the working substance inside
the microgrooves corresponding to a known temperature of the heat source. The
equation of motion of the working fluid in the meniscus of an equilateral
triangular groove has been presented from which a new term called the minimum
surface tension required for avoiding the dry out condition is defined.
Quantitative results showing the effect of thickness of cover plate, heat load,
angle of inclination and viscosity of the working fluid on the different
aspects of the heat transfer, minimum surface tension required to avoid dry
out, velocity distribution of the liquid, and radius of liquid meniscus inside
the micro-grooves have been presented and discussed.
",0,1,0,0,0,0
316,"Merge decompositions, two-sided Krohn-Rhodes, and aperiodic pointlikes","  This paper provides short proofs of two fundamental theorems of finite
semigroup theory whose previous proofs were significantly longer, namely the
two-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike
theorem, using a new algebraic technique that we call the merge decomposition.
A prototypical application of this technique decomposes a semigroup $T$ into a
two-sided semidirect product whose components are built from two subsemigroups
$T_1,T_2$, which together generate $T$, and the subsemigroup generated by their
setwise product $T_1T_2$. In this sense we decompose $T$ by merging the
subsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup
homomorphisms from free semigroups.
",1,0,1,0,0,0
317,Deep Multimodal Image-Repurposing Detection,"  Nefarious actors on social media and other platforms often spread rumors and
falsehoods through images whose metadata (e.g., captions) have been modified to
provide visual substantiation of the rumor/falsehood. This type of modification
is referred to as image repurposing, in which often an unmanipulated image is
published along with incorrect or manipulated metadata to serve the actor's
ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
dataset, a substantially challenging dataset over that which has been
previously available to support research into image repurposing detection. The
new dataset includes location, person, and organization manipulations on
real-world data sourced from Flickr. We also present a novel, end-to-end, deep
multimodal learning model for assessing the integrity of an image by combining
information extracted from the image with related information from a knowledge
base. The proposed method is compared against state-of-the-art techniques on
existing datasets as well as MEIR, where it outperforms existing methods across
the board, with AUC improvement up to 0.23.
",1,0,0,0,0,0
318,DeepFense: Online Accelerated Defense Against Adversarial Deep Learning,"  Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
",1,0,0,1,0,0
319,Retrospective Higher-Order Markov Processes for User Trails,"  Users form information trails as they browse the web, checkin with a
geolocation, rate items, or consume media. A common problem is to predict what
a user might do next for the purposes of guidance, recommendation, or
prefetching. First-order and higher-order Markov chains have been widely used
methods to study such sequences of data. First-order Markov chains are easy to
estimate, but lack accuracy when history matters. Higher-order Markov chains,
in contrast, have too many parameters and suffer from overfitting the training
data. Fitting these parameters with regularization and smoothing only offers
mild improvements. In this paper we propose the retrospective higher-order
Markov process (RHOMP) as a low-parameter model for such sequences. This model
is a special case of a higher-order Markov chain where the transitions depend
retrospectively on a single history state instead of an arbitrary combination
of history states. There are two immediate computational advantages: the number
of parameters is linear in the order of the Markov chain and the model can be
fit to large state spaces. Furthermore, by providing a specific structure to
the higher-order chain, RHOMPs improve the model accuracy by efficiently
utilizing history states without risks of overfitting the data. We demonstrate
how to estimate a RHOMP from data and we demonstrate the effectiveness of our
method on various real application datasets spanning geolocation data, review
sequences, and business locations. The RHOMP model uniformly outperforms
higher-order Markov chains, Kneser-Ney regularization, and tensor
factorizations in terms of prediction accuracy.
",1,0,0,1,0,0
320,Frank-Wolfe with Subsampling Oracle,"  We analyze two novel randomized variants of the Frank-Wolfe (FW) or
conditional gradient algorithm. While classical FW algorithms require solving a
linear minimization problem over the domain at each iteration, the proposed
method only requires to solve a linear minimization problem over a small
\emph{subset} of the original domain. The first algorithm that we propose is a
randomized variant of the original FW algorithm and achieves a
$\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic
counterpart. The second algorithm is a randomized variant of the Away-step FW
algorithm, and again as its deterministic counterpart, reaches linear (i.e.,
exponential) convergence rate making it the first provably convergent
randomized variant of Away-step FW. In both cases, while subsampling reduces
the convergence rate by a constant factor, the linear minimization step can be
a fraction of the cost of that of the deterministic versions, especially when
the data is streamed. We illustrate computational gains of the algorithms on
regression problems, involving both $\ell_1$ and latent group lasso penalties.
",0,0,0,1,0,0
321,A mean value formula and a Liouville theorem for the complex Monge-Ampère equation,"  In this paper, we prove a mean value formula for bounded subharmonic
Hermitian matrix valued function on a complete Riemannian manifold with
nonnegative Ricci curvature. As its application, we obtain a Liouville type
theorem for the complex Monge-Ampère equation on product manifolds.
",0,0,1,0,0,0
322,Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI Super-Resolution,"  In this work, we investigate the value of uncertainty modeling in 3D
super-resolution with convolutional neural networks (CNNs). Deep learning has
shown success in a plethora of medical image transformation problems, such as
super-resolution (SR) and image synthesis. However, the highly ill-posed nature
of such problems results in inevitable ambiguity in the learning of networks.
We propose to account for intrinsic uncertainty through a per-patch
heteroscedastic noise model and for parameter uncertainty through approximate
Bayesian inference in the form of variational dropout. We show that the
combined benefits of both lead to the state-of-the-art performance SR of
diffusion MR brain images in terms of errors compared to ground truth. We
further show that the reduced error scores produce tangible benefits in
downstream tractography. In addition, the probabilistic nature of the methods
naturally confers a mechanism to quantify uncertainty over the super-resolved
output. We demonstrate through experiments on both healthy and pathological
brains the potential utility of such an uncertainty measure in the risk
assessment of the super-resolved images for subsequent clinical use.
",1,0,0,0,0,0
323,Yu-Shiba-Rusinov bands in superconductors in contact with a magnetic insulator,"  Superconductor-Ferromagnet (SF) heterostructures are of interest due to
numerous phenomena related to the spin-dependent interaction of Cooper pairs
with the magnetization. Here we address the effects of a magnetic insulator on
the density of states of a superconductor based on a recently developed
boundary condition for strongly spin-dependent interfaces. We show that the
boundary to a magnetic insulator has a similar effect like the presence of
magnetic impurities. In particular we find that the impurity effects of
strongly scattering localized spins leading to the formation of Shiba bands can
be mapped onto the boundary problem.
",0,1,0,0,0,0
324,Bayesian Optimization for Probabilistic Programs,"  We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.
",1,0,0,1,0,0
325,Long-Term Load Forecasting Considering Volatility Using Multiplicative Error Model,"  Long-term load forecasting plays a vital role for utilities and planners in
terms of grid development and expansion planning. An overestimate of long-term
electricity load will result in substantial wasted investment in the
construction of excess power facilities, while an underestimate of future load
will result in insufficient generation and unmet demand. This paper presents
first-of-its-kind approach to use multiplicative error model (MEM) in
forecasting load for long-term horizon. MEM originates from the structure of
autoregressive conditional heteroscedasticity (ARCH) model where conditional
variance is dynamically parameterized and it multiplicatively interacts with an
innovation term of time-series. Historical load data, accessed from a U.S.
regional transmission operator, and recession data for years 1993-2016 is used
in this study. The superiority of considering volatility is proven by
out-of-sample forecast results as well as directional accuracy during the great
economic recession of 2008. To incorporate future volatility, backtesting of
MEM model is performed. Two performance indicators used to assess the proposed
model are mean absolute percentage error (for both in-sample model fit and
out-of-sample forecasts) and directional accuracy.
",0,0,0,1,0,0
326,Geometrically stopped Markovian random growth processes and Pareto tails,"  Many empirical studies document power law behavior in size distributions of
economic interest such as cities, firms, income, and wealth. One mechanism for
generating such behavior combines independent and identically distributed
Gaussian additive shocks to log-size with a geometric age distribution. We
generalize this mechanism by allowing the shocks to be non-Gaussian (but
light-tailed) and dependent upon a Markov state variable. Our main results
provide sharp bounds on tail probabilities and simple formulas for Pareto
exponents. We present two applications: (i) we show that the tails of the
wealth distribution in a heterogeneous-agent dynamic general equilibrium model
with idiosyncratic endowment risk decay exponentially, unlike models with
investment risk where the tails may be Paretian, and (ii) we show that a random
growth model for the population dynamics of Japanese prefectures is consistent
with the observed Pareto exponent but only after allowing for Markovian
dynamics.
",0,0,1,0,0,0
327,Mathematics of Topological Quantum Computing,"  In topological quantum computing, information is encoded in ""knotted"" quantum
states of topological phases of matter, thus being locked into topology to
prevent decay. Topological precision has been confirmed in quantum Hall liquids
by experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum
memory. In this survey, we discuss the conceptual development of this
interdisciplinary field at the juncture of mathematics, physics and computer
science. Our focus is on computing and physical motivations, basic mathematical
notions and results, open problems and future directions related to and/or
inspired by topological quantum computing.
",0,1,1,0,0,0
328,Graph Clustering using Effective Resistance,"  $ \def\vecc#1{\boldsymbol{#1}} $We design a polynomial time algorithm that
for any weighted undirected graph $G = (V, E,\vecc w)$ and sufficiently large
$\delta > 1$, partitions $V$ into subsets $V_1, \ldots, V_h$ for some $h\geq
1$, such that
$\bullet$ at most $\delta^{-1}$ fraction of the weights are between clusters,
i.e. \[ w(E - \cup_{i = 1}^h E(V_i)) \lesssim \frac{w(E)}{\delta};\]
$\bullet$ the effective resistance diameter of each of the induced subgraphs
$G[V_i]$ is at most $\delta^3$ times the average weighted degree, i.e. \[
\max_{u, v \in V_i} \mathsf{Reff}_{G[V_i]}(u, v) \lesssim \delta^3 \cdot
\frac{|V|}{w(E)} \quad \text{ for all } i=1, \ldots, h.\]
In particular, it is possible to remove one percent of weight of edges of any
given graph such that each of the resulting connected components has effective
resistance diameter at most the inverse of the average weighted degree.
Our proof is based on a new connection between effective resistance and low
conductance sets. We show that if the effective resistance between two vertices
$u$ and $v$ is large, then there must be a low conductance cut separating $u$
from $v$. This implies that very mildly expanding graphs have constant
effective resistance diameter. We believe that this connection could be of
independent interest in algorithm design.
",1,0,0,0,0,0
329,Self-supervised learning: When is fusion of the primary and secondary sensor cue useful?,"  Self-supervised learning (SSL) is a reliable learning mechanism in which a
robot enhances its perceptual capabilities. Typically, in SSL a trusted,
primary sensor cue provides supervised training data to a secondary sensor cue.
In this article, a theoretical analysis is performed on the fusion of the
primary and secondary cue in a minimal model of SSL. A proof is provided that
determines the specific conditions under which it is favorable to perform
fusion. In short, it is favorable when (i) the prior on the target value is
strong or (ii) the secondary cue is sufficiently accurate. The theoretical
findings are validated with computational experiments. Subsequently, a
real-world case study is performed to investigate if fusion in SSL is also
beneficial when assumptions of the minimal model are not met. In particular, a
flying robot learns to map pressure measurements to sonar height measurements
and then fuses the two, resulting in better height estimation. Fusion is also
beneficial in the opposite case, when pressure is the primary cue. The analysis
and results are encouraging to study SSL fusion also for other robots and
sensors.
",1,0,0,0,0,0
330,Playing Atari with Six Neurons,"  Deep reinforcement learning on Atari games maps pixel directly to actions;
internally, the deep neural network bears the responsibility of both extracting
useful information and making decisions based on it. Aiming at devoting entire
deep networks to decision making alone, we propose a new method for learning
policies and compact state representations separately but simultaneously for
policy approximation in reinforcement learning. State representations are
generated by a novel algorithm based on Vector Quantization and Sparse Coding,
trained online along with the network, and capable of growing its dictionary
size over time. We also introduce new techniques allowing both the neural
network and the evolution strategy to cope with varying dimensions. This
enables networks of only 6 to 18 neurons to learn to play a selection of Atari
games with performance comparable---and occasionally superior---to
state-of-the-art techniques using evolution strategies on deep networks two
orders of magnitude larger.
",0,0,0,1,0,0
331,Contraction and uniform convergence of isotonic regression,"  We consider the problem of isotonic regression, where the underlying signal
$x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the
cone $\{ x\in\mathbb{R}^n : x_1 \leq \dots \leq x_n\}$. We study the isotonic
projection operator (projection to this cone), and find a necessary and
sufficient condition characterizing all norms with respect to which this
projection is contractive. This enables a simple and non-asymptotic analysis of
the convergence properties of isotonic regression, yielding uniform confidence
bands that adapt to the local Lipschitz properties of the signal.
",0,0,1,1,0,0
332,A Systematic Approach for Exploring Tradeoffs in Predictive HVAC Control Systems for Buildings,"  Heating, Ventilation, and Cooling (HVAC) systems are often the most
significant contributor to the energy usage, and the operational cost, of large
office buildings. Therefore, to understand the various factors affecting the
energy usage, and to optimize the operational efficiency of building HVAC
systems, energy analysts and architects often create simulations (e.g.,
EnergyPlus or DOE-2), of buildings prior to construction or renovation to
determine energy savings and quantify the Return-on-Investment (ROI). While
useful, these simulations usually use static HVAC control strategies such as
lowering room temperature at night, or reactive control based on simulated room
occupancy. Recently, advances have been made in HVAC control algorithms that
predict room occupancy. However, these algorithms depend on costly sensor
installations and the tradeoffs between predictive accuracy, energy savings,
comfort and expenses are not well understood. Current simulation frameworks do
not support easy analysis of these tradeoffs. Our contribution is a simulation
framework that can be used to explore this design space by generating objective
estimates of the energy savings and occupant comfort for different levels of
HVAC prediction and control performance. We validate our framework on a
real-world occupancy dataset spanning 6 months for 235 rooms in a large
university office building. Using the gold standard of energy use modeling and
simulation (Revit and Energy Plus), we compare the energy consumption and
occupant comfort in 29 independent simulations that explore our parameter
space. Our results highlight a number of potentially useful tradeoffs with
respect to energy savings, comfort, and algorithmic performance among
predictive, reactive, and static schedules, for a stakeholder of our building.
",1,0,0,0,0,0
333,On types of degenerate critical points of real polynomial functions,"  In this paper, we consider the problem of identifying the type (local
minimizer, maximizer or saddle point) of a given isolated real critical point
$c$, which is degenerate, of a multivariate polynomial function $f$. To this
end, we introduce the definition of faithful radius of $c$ by means of the
curve of tangency of $f$. We show that the type of $c$ can be determined by the
global extrema of $f$ over the Euclidean ball centered at $c$ with a faithful
radius.We propose algorithms to compute a faithful radius of $c$ and determine
its type.
",0,0,1,0,0,0
334,Contribution of Data Categories to Readmission Prediction Accuracy,"  Identification of patients at high risk for readmission could help reduce
morbidity and mortality as well as healthcare costs. Most of the existing
studies on readmission prediction did not compare the contribution of data
categories. In this study we analyzed relative contribution of 90,101 variables
across 398,884 admission records corresponding to 163,468 patients, including
patient demographics, historical hospitalization information, discharge
disposition, diagnoses, procedures, medications and laboratory test results. We
established an interpretable readmission prediction model based on Logistic
Regression in scikit-learn, and added the available variables to the model one
by one in order to analyze the influences of individual data categories on
readmission prediction accuracy. Diagnosis related groups (c-statistic
increment of 0.0933) and discharge disposition (c-statistic increment of
0.0269) were the strongest contributors to model accuracy. Additionally, we
also identified the top ten contributing variables in every data category.
",0,0,0,0,1,0
335,Tropical recurrent sequences,"  Tropical recurrent sequences are introduced satisfying a given vector (being
a tropical counterpart of classical linear recurrent sequences). We consider
the case when Newton polygon of the vector has a single (bounded) edge. In this
case there are periodic tropical recurrent sequences which are similar to
classical linear recurrent sequences. A question is studied when there exists a
non-periodic tropical recurrent sequence satisfying a given vector, and partial
answers are provided to this question. Also an algorithm is designed which
tests existence of non-periodic tropical recurrent sequences satisfying a given
vector with integer coordinates. Finally, we introduce a tropical entropy of a
vector and provide some bounds on it.
",1,0,0,0,0,0
336,Invariance of Weight Distributions in Rectified MLPs,"  An interesting approach to analyzing neural networks that has received
renewed attention is to examine the equivalent kernel of the neural network.
This is based on the fact that a fully connected feedforward network with one
hidden layer, a certain weight distribution, an activation function, and an
infinite number of neurons can be viewed as a mapping into a Hilbert space. We
derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for
all rotationally-invariant weight distributions, generalizing a previous result
that required Gaussian weight distributions. Additionally, the Central Limit
Theorem is used to show that for certain activation functions, kernels
corresponding to layers with weight distributions having $0$ mean and finite
absolute third moment are asymptotically universal, and are well approximated
by the kernel corresponding to layers with spherical Gaussian weights. In deep
networks, as depth increases the equivalent kernel approaches a pathological
fixed point, which can be used to argue why training randomly initialized
networks can be difficult. Our results also have implications for weight
initialization.
",1,0,0,1,0,0
337,Learning to Adapt by Minimizing Discrepancy,"  We explore whether useful temporal neural generative models can be learned
from sequential data without back-propagation through time. We investigate the
viability of a more neurocognitively-grounded approach in the context of
unsupervised generative modeling of sequences. Specifically, we build on the
concept of predictive coding, which has gained influence in cognitive science,
in a neural framework. To do so we develop a novel architecture, the Temporal
Neural Coding Network, and its learning algorithm, Discrepancy Reduction. The
underlying directed generative model is fully recurrent, meaning that it
employs structural feedback connections and temporal feedback connections,
yielding information propagation cycles that create local learning signals.
This facilitates a unified bottom-up and top-down approach for information
transfer inside the architecture. Our proposed algorithm shows promise on the
bouncing balls generative modeling problem. Further experiments could be
conducted to explore the strengths and weaknesses of our approach.
",1,0,0,1,0,0
338,Incarnation of Majorana Fermions in Kitaev Quantum Spin Lattice,"  Kitaev quantum spin liquid is a topological magnetic quantum state
characterized by Majorana fermions of fractionalized spin excitations, which
are identical to their own antiparticles. Here, we demonstrate emergence of
Majorana fermions thermally fractionalized in the Kitaev honeycomb spin lattice
{\alpha}-RuCl3. The specific heat data unveil the characteristic two-stage
release of magnetic entropy involving localized and itinerant Majorana
fermions. The inelastic neutron scattering results further corroborate these
two distinct fermions by exhibiting quasielastic excitations at low energies
around the Brillouin zone center and Y-shaped magnetic continuum at high
energies, which are evident for the ferromagnetic Kitaev model. Our results
provide an opportunity to build a unified conceptual framework of
fractionalized excitations, applicable also for the quantum Hall states,
superconductors, and frustrated magnets.
",0,1,0,0,0,0
339,Haro 11: Where is the Lyman continuum source?,"  Identifying the mechanism by which high energy Lyman continuum (LyC) photons
escaped from early galaxies is one of the most pressing questions in cosmic
evolution. Haro 11 is the best known local LyC leaking galaxy, providing an
important opportunity to test our understanding of LyC escape. The observed LyC
emission in this galaxy presumably originates from one of the three bright,
photoionizing knots known as A, B, and C. It is known that Knot C has strong
Ly$\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray
source, which may be a low-luminosity AGN. To clarify the LyC source, we carry
out ionization-parameter mapping (IPM) by obtaining narrow-band imaging from
the Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved
ratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization
structure of the interstellar medium and allows us to identify optically thin
regions. To optimize the continuum subtraction, we introduce a new method for
determining the best continuum scale factor derived from the mode of the
continuum-subtracted, image flux distribution. We find no conclusive evidence
of LyC escape from Knots B or C, but instead, we identify a high-ionization
region extending over at least 1 kpc from Knot A. Knot A shows evidence of an
extremely young age ($\lesssim 1$ Myr), perhaps containing very massive stars
($>100$ M$_\odot$). It is weak in Ly$\alpha$, so if it is confirmed as the LyC
source, our results imply that LyC emission may be independent of Ly$\alpha$
emission.
",0,1,0,0,0,0
340,Typed Closure Conversion for the Calculus of Constructions,"  Dependently typed languages such as Coq are used to specify and verify the
full functional correctness of source programs. Type-preserving compilation can
be used to preserve these specifications and proofs of correctness through
compilation into the generated target-language programs. Unfortunately,
type-preserving compilation of dependent types is hard. In essence, the problem
is that dependent type systems are designed around high-level compositional
abstractions to decide type checking, but compilation interferes with the
type-system rules for reasoning about run-time terms.
We develop a type-preserving closure-conversion translation from the Calculus
of Constructions (CC) with strong dependent pairs ($\Sigma$ types)---a subset
of the core language of Coq---to a type-safe, dependently typed compiler
intermediate language named CC-CC. The central challenge in this work is how to
translate the source type-system rules for reasoning about functions into
target type-system rules for reasoning about closures. To justify these rules,
we prove soundness of CC-CC by giving a model in CC. In addition to type
preservation, we prove correctness of separate compilation.
",1,0,0,0,0,0
341,Untangling Planar Curves,"  Any generic closed curve in the plane can be transformed into a simple closed
curve by a finite sequence of local transformations called homotopy moves. We
prove that simplifying a planar closed curve with $n$ self-crossings requires
$\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the
best previous upper bound $O(n^2)$, which is already implicit in the classical
work of Steinitz; the matching lower bound follows from the construction of
closed curves with large defect, a topological invariant of generic closed
curves introduced by Aicardi and Arnold. Our lower bound also implies that
$\Omega(n^{3/2})$ facial electrical transformations are required to reduce any
plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex, matching
known upper bounds for rectangular and cylindrical grid graphs. More generally,
we prove that transforming one immersion of $k$ circles with at most $n$
self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy
moves in the worst case. Finally, we prove that transforming one
noncontractible closed curve to another on any orientable surface requires
$\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if
the curve is homotopic to a simple closed curve.
",1,0,1,0,0,0
342,Greedy-Merge Degrading has Optimal Power-Law,"  Consider a channel with a given input distribution. Our aim is to degrade it
to a channel with at most L output letters. One such degradation method is the
so called ""greedy-merge"" algorithm. We derive an upper bound on the reduction
in mutual information between input and output. For fixed input alphabet size
and variable L, the upper bound is within a constant factor of an
algorithm-independent lower bound. Thus, we establish that greedy-merge is
optimal in the power-law sense.
",1,0,1,0,0,0
343,Radially distributed values and normal families,"  Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let
${\mathcal F}$ be the family of all functions holomorphic in the unit disk
${\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on
$L_1$. It is shown that ${\mathcal F}$ is normal in ${\mathbb
D}\backslash\{0\}$. The case where $L_0$ is the positive real axis and $L_1$ is
the negative real axis is studied in more detail.
",0,0,1,0,0,0
344,Characterizing Exoplanet Habitability,"  A habitable exoplanet is a world that can maintain stable liquid water on its
surface. Techniques and approaches to characterizing such worlds are essential,
as performing a census of Earth-like planets that may or may not have life will
inform our understanding of how frequently life originates and is sustained on
worlds other than our own. Observational techniques like high contrast imaging
and transit spectroscopy can reveal key indicators of habitability for
exoplanets. Both polarization measurements and specular reflectance from oceans
(also known as ""glint"") can provide direct evidence for surface liquid water,
while constraining surface pressure and temperature (from moderate resolution
spectra) can indicate liquid water stability. Indirect evidence for
habitability can come from a variety of sources, including observations of
variability due to weather, surface mapping studies, and/or measurements of
water vapor or cloud profiles that indicate condensation near a surface.
Approaches to making the types of measurements that indicate habitability are
diverse, and have different considerations for the required wavelength range,
spectral resolution, maximum noise levels, stellar host temperature, and
observing geometry.
",0,1,0,0,0,0
345,Deep Learning for Classification Tasks on Geospatial Vector Polygons,"  In this paper, we evaluate the accuracy of deep learning approaches on
geospatial vector geometry classification tasks. The purpose of this evaluation
is to investigate the ability of deep learning models to learn from geometry
coordinates directly. Previous machine learning research applied to geospatial
polygon data did not use geometries directly, but derived properties thereof.
These are produced by way of extracting geometry properties such as Fourier
descriptors. Instead, our introduced deep neural net architectures are able to
learn on sequences of coordinates mapped directly from polygons. In three
classification tasks we show that the deep learning architectures are
competitive with common learning algorithms that require extracted features.
",0,0,0,1,0,0
346,The Distance Standard Deviation,"  The distance standard deviation, which arises in distance correlation
analysis of multivariate data, is studied as a measure of spread. New
representations for the distance standard deviation are obtained in terms of
Gini's mean difference and in terms of the moments of spacings of order
statistics. Inequalities for the distance variance are derived, proving that
the distance standard deviation is bounded above by the classical standard
deviation and by Gini's mean difference. Further, it is shown that the distance
standard deviation satisfies the axiomatic properties of a measure of spread.
Explicit closed-form expressions for the distance variance are obtained for a
broad class of parametric distributions. The asymptotic distribution of the
sample distance variance is derived.
",0,0,1,1,0,0
347,Combining learned and analytical models for predicting action effects,"  One of the most basic skills a robot should possess is predicting the effect
of physical interactions with objects in the environment. This enables optimal
action selection to reach a certain goal state. Traditionally, dynamics are
approximated by physics-based analytical models. These models rely on specific
state representations that may be hard to obtain from raw sensory data,
especially if no knowledge of the object shape is assumed. More recently, we
have seen learning approaches that can predict the effect of complex physical
interactions directly from sensory input. It is however an open question how
far these models generalize beyond their training data. In this work, we
investigate the advantages and limitations of neural network based learning
approaches for predicting the effects of actions based on sensory input and
show how analytical and learned models can be combined to leverage the best of
both worlds. As physical interaction task, we use planar pushing, for which
there exists a well-known analytical model and a large real-world dataset. We
propose to use a convolutional neural network to convert raw depth images or
organized point clouds into a suitable representation for the analytical model
and compare this approach to using neural networks for both, perception and
prediction. A systematic evaluation of the proposed approach on a very large
real-world dataset shows two main advantages of the hybrid architecture.
Compared to a pure neural network, it significantly (i) reduces required
training data and (ii) improves generalization to novel physical interaction.
",1,0,0,0,0,0
348,Leavitt path algebras: Graded direct-finiteness and graded $Σ$-injective simple modules,"  In this paper, we give a complete characterization of Leavitt path algebras
which are graded $\Sigma $-$V$ rings, that is, rings over which a direct sum of
arbitrary copies of any graded simple module is graded injective. Specifically,
we show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded
$\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of
arbitrary size but with finitely many non-zero entries over $K$ or
$K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical
characterization of such a graded $\Sigma $-$V$ ring $L$% . When the graph $E$
is finite, we show that $L$ is a graded $\Sigma $-$V$ ring $\Longleftrightarrow
L$ is graded directly-finite $\Longleftrightarrow L $ has bounded index of
nilpotence $\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that
the equivalence of these properties in the preceding statement no longer holds
when the graph $E$ is infinite. Following this, we also characterize Leavitt
path algebras $L$ which are non-graded $\Sigma $-$V$ rings. Graded rings which
are graded directly-finite are explored and it is shown that if a Leavitt path
algebra $L$ is a graded $\Sigma$-$V$ ring, then $L$ is always graded
directly-finite. Examples show the subtle differences between graded and
non-graded directly-finite rings. Leavitt path algebras which are graded
directly-finite are shown to be directed unions of graded semisimple rings.
Using this, we give an alternative proof of a theorem of Vaš \cite{V} on
directly-finite Leavitt path algebras.
",0,0,1,0,0,0
349,Ensemble Estimation of Mutual Information,"  We derive the mean squared error convergence rates of kernel density-based
plug-in estimators of mutual information measures between two multidimensional
random variables $\mathbf{X}$ and $\mathbf{Y}$ for two cases: 1) $\mathbf{X}$
and $\mathbf{Y}$ are both continuous; 2) $\mathbf{X}$ is continuous and
$\mathbf{Y}$ is discrete. Using the derived rates, we propose an ensemble
estimator of these information measures for the second case by taking a
weighted sum of the plug-in estimators with varied bandwidths. The resulting
ensemble estimator achieves the $1/N$ parametric convergence rate when the
conditional densities of the continuous variables are sufficiently smooth. To
the best of our knowledge, this is the first nonparametric mutual information
estimator known to achieve the parametric convergence rate for this case, which
frequently arises in applications (e.g. variable selection in classification).
The estimator is simple to implement as it uses the solution to an offline
convex optimization problem and simple plug-in estimators. A central limit
theorem is also derived for the ensemble estimator. Ensemble estimators that
achieve the parametric rate are also derived for the first case ($\mathbf{X}$
and $\mathbf{Y}$ are both continuous) and another case 3) $\mathbf{X}$ and
$\mathbf{Y}$ may have any mixture of discrete and continuous components.
",1,0,1,1,0,0
350,Social media mining for identification and exploration of health-related information from pregnant women,"  Widespread use of social media has led to the generation of substantial
amounts of information about individuals, including health-related information.
Social media provides the opportunity to study health-related information about
selected population groups who may be of interest for a particular study. In
this paper, we explore the possibility of utilizing social media to perform
targeted data collection and analysis from a particular population group --
pregnant women. We hypothesize that we can use social media to identify cohorts
of pregnant women and follow them over time to analyze crucial health-related
information. To identify potentially pregnant women, we employ simple
rule-based searches that attempt to detect pregnancy announcements with
moderate precision. To further filter out false positives and noise, we employ
a supervised classifier using a small number of hand-annotated data. We then
collect their posts over time to create longitudinal health timelines and
attempt to divide the timelines into different pregnancy trimesters. Finally,
we assess the usefulness of the timelines by performing a preliminary analysis
to estimate drug intake patterns of our cohort at different trimesters. Our
rule-based cohort identification technique collected 53,820 users over thirty
months from Twitter. Our pregnancy announcement classification technique
achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user
timelines. Analysis of the timelines revealed that pertinent health-related
information, such as drug-intake and adverse reactions can be mined from the
data. Our approach to using user timelines in this fashion has produced very
encouraging results and can be employed for other important tasks where
cohorts, for which health-related information may not be available from other
sources, are required to be followed over time to derive population-based
estimates.
",1,0,0,0,0,0
351,The vortex method for 2D ideal flows in the exterior of a disk,"  The vortex method is a common numerical and theoretical approach used to
implement the motion of an ideal flow, in which the vorticity is approximated
by a sum of point vortices, so that the Euler equations read as a system of
ordinary differential equations. Such a method is well justified in the full
plane, thanks to the explicit representation formulas of Biot and Savart. In an
exterior domain, we also replace the impermeable boundary by a collection of
point vortices generating the circulation around the obstacle. The density of
these point vortices is chosen in order that the flow remains tangent at
midpoints between adjacent vortices. In this work, we provide a rigorous
justification for this method in exterior domains. One of the main mathematical
difficulties being that the Biot-Savart kernel defines a singular integral
operator when restricted to a curve. For simplicity and clarity, we only treat
the case of the unit disk in the plane approximated by a uniformly distributed
mesh of point vortices. The complete and general version of our work is
available in [arXiv:1707.01458].
",0,0,1,0,0,0
352,Neeman's characterization of K(R-Proj) via Bousfield localization,"  Let $R$ be an associative ring with unit and denote by $K({\rm R
\mbox{-}Proj})$ the homotopy category of complexes of projective left
$R$-modules. Neeman proved the theorem that $K({\rm R \mbox{-}Proj})$ is
$\aleph_1$-compactly generated, with the category $K^+ ({\rm R \mbox{-}proj})$
of left bounded complexes of finitely generated projective $R$-modules
providing an essentially small class of such generators. Another proof of
Neeman's theorem is explained, using recent ideas of Christensen and Holm, and
Emmanouil. The strategy of the proof is to show that every complex in $K({\rm R
\mbox{-}Proj})$ vanishes in the Bousfield localization $K({\rm R
\mbox{-}Flat})/\langle K^+ ({\rm R \mbox{-}proj}) \rangle.$
",0,0,1,0,0,0
353,Qualification Conditions in Semi-algebraic Programming,"  For an arbitrary finite family of semi-algebraic/definable functions, we
consider the corresponding inequality constraint set and we study qualification
conditions for perturbations of this set. In particular we prove that all
positive diagonal perturbations, save perhaps a finite number of them, ensure
that any point within the feasible set satisfies Mangasarian-Fromovitz
constraint qualification. Using the Milnor-Thom theorem, we provide a bound for
the number of singular perturbations when the constraints are polynomial
functions. Examples show that the order of magnitude of our exponential bound
is relevant. Our perturbation approach provides a simple protocol to build
sequences of ""regular"" problems approximating an arbitrary
semi-algebraic/definable problem. Applications to sequential quadratic
programming methods and sum of squares relaxation are provided.
",0,0,1,0,0,0
354,Curvature in Hamiltonian Mechanics And The Einstein-Maxwell-Dilaton Action,"  Riemannian geometry is a particular case of Hamiltonian mechanics: the orbits
of the hamiltonian $H=\frac{1}{2}g^{ij}p_{i}p_{j}$ are the geodesics. Given a
symplectic manifold (\Gamma,\omega), a hamiltonian $H:\Gamma\to\mathbb{R}$ and
a Lagrangian sub-manifold $M\subset\Gamma$ we find a generalization of the
notion of curvature. The particular case
$H=\frac{1}{2}g^{ij}\left[p_{i}-A_{i}\right]\left[p_{j}-A_{j}\right]+\phi $ of
a particle moving in a gravitational, electromagnetic and scalar fields is
studied in more detail. The integral of the generalized Ricci tensor w.r.t. the
Boltzmann weight reduces to the action principle
$\int\left[R+\frac{1}{4}F_{ik}F_{jl}g^{kl}g^{ij}-g^{ij}\partial_{i}\phi\partial_{j}\phi\right]e^{-\phi}\sqrt{g}d^{n}q$
for the scalar, vector and tensor fields.
",0,0,1,0,0,0
355,End-to-End Navigation in Unknown Environments using Neural Networks,"  We investigate how a neural network can learn perception actions loops for
navigation in unknown environments. Specifically, we consider how to learn to
navigate in environments populated with cul-de-sacs that represent convex local
minima that the robot could fall into instead of finding a set of feasible
actions that take it to the goal. Traditional methods rely on maintaining a
global map to solve the problem of over coming a long cul-de-sac. However, due
to errors induced from local and global drift, it is highly challenging to
maintain such a map for long periods of time. One way to mitigate this problem
is by using learning techniques that do not rely on hand engineered map
representations and instead output appropriate control policies directly from
their sensory input. We first demonstrate that such a problem cannot be solved
directly by deep reinforcement learning due to the sparse reward structure of
the environment. Further, we demonstrate that deep supervised learning also
cannot be used directly to solve this problem. We then investigate network
models that offer a combination of reinforcement learning and supervised
learning and highlight the significance of adding fully differentiable memory
units to such networks. We evaluate our networks on their ability to generalize
to new environments and show that adding memory to such networks offers huge
jumps in performance
",1,0,0,0,0,0
356,A Combinatorial Approach to the Opposite Bi-Free Partial $S$-Transform,"  In this paper, we present a combinatorial approach to the opposite 2-variable
bi-free partial $S$-transforms where the opposite multiplication is used on the
right. In addition, extensions of this partial $S$-transforms to the
conditional bi-free and operator-valued bi-free settings are discussed.
",0,0,1,0,0,0
357,Roche-lobe overflow in eccentric planet-star systems,"  Many giant exoplanets are found near their Roche limit and in mildly
eccentric orbits. In this study we examine the fate of such planets through
Roche-lobe overflow as a function of the physical properties of the binary
components, including the eccentricity and the asynchronicity of the rotating
planet. We use a direct three-body integrator to compute the trajectories of
the lost mass in the ballistic limit and investigate the possible outcomes. We
find three different outcomes for the mass transferred through the Lagrangian
point $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the
stellar surface, (iii) disk formation around the star. We explore the parameter
space of the three different regimes and find that at low eccentricities,
$e\lesssim 0.2$, mass overflow leads to disk formation for most systems, while
for higher eccentricities or retrograde orbits self-accretion is the only
possible outcome. We conclude that the assumption often made in previous work
that when a planet overflows its Roche lobe it is quickly disrupted and
accreted by the star is not always valid.
",0,1,0,0,0,0
358,A short-orbit spectrometer for low-energy pion detection in electroproduction experiments at MAMI,"  A new Short-Orbit Spectrometer (SOS) has been constructed and installed
within the experimental facility of the A1 collaboration at Mainz Microtron
(MAMI), with the goal to detect low-energy pions. It is equipped with a
Browne-Buechner magnet and a detector system consisting of two helium-ethane
based drift chambers and a scintillator telescope made of five layers. The
detector system allows detection of pions in the momentum range of 50 - 147
MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can
be placed at a distance range of 54 - 66 cm from the target center. Two
collimators are available for the measurements, one having 1.8 msr aperture and
the other having 7 msr aperture. The Short-Orbit Spectrometer has been
successfully calibrated and used in coincidence measurements together with the
standard magnetic spectrometers of the A1 collaboration.
",0,1,0,0,0,0
359,A one-dimensional model for water desalination by flow-through electrode capacitive deionization,"  Capacitive deionization (CDI) is a fast-emerging water desalination
technology in which a small cell voltage of ~1 V across porous carbon
electrodes removes salt from feedwaters via electrosorption. In flow-through
electrode (FTE) CDI cell architecture, feedwater is pumped through macropores
or laser perforated channels in porous electrodes, enabling highly compact
cells with parallel flow and electric field, as well as rapid salt removal. We
here present a one-dimensional model describing water desalination by FTE CDI,
and a comparison to data from a custom-built experimental cell. The model
employs simple cell boundary conditions derived via scaling arguments. We show
good model-to-data fits with reasonable values for fitting parameters such as
the Stern layer capacitance, micropore volume, and attraction energy. Thus, we
demonstrate that from an engineering modeling perspective, an FTE CDI cell may
be described with simpler one-dimensional models, unlike more typical
flow-between electrodes architecture where 2D models are required.
",1,1,0,0,0,0
360,Deep Learning Scooping Motion using Bilateral Teleoperations,"  We present bilateral teleoperation system for task learning and robot motion
generation. Our system includes a bilateral teleoperation platform and a deep
learning software. The deep learning software refers to human demonstration
using the bilateral teleoperation platform to collect visual images and robotic
encoder values. It leverages the datasets of images and robotic encoder
information to learn about the inter-modal correspondence between visual images
and robot motion. In detail, the deep learning software uses a combination of
Deep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent
Neural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor
angles, to learn motion taught be human teleoperation. The learnt models are
used to predict new motion trajectories for similar tasks. Experimental results
show that our system has the adaptivity to generate motion for similar scooping
tasks. Detailed analysis is performed based on failure cases of the
experimental results. Some insights about the cans and cannots of the system
are summarized.
",1,0,0,0,0,0
361,XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification,"  We propose two multimodal deep learning architectures that allow for
cross-modal dataflow (XFlow) between the feature extractors, thereby extracting
more interpretable features and obtaining a better representation than through
unimodal learning, for the same amount of training data. These models can
usefully exploit correlations between audio and visual data, which have a
different dimensionality and are therefore nontrivially exchangeable. Our work
improves on existing multimodal deep learning metholodogies in two essential
ways: (1) it presents a novel method for performing cross-modality (before
features are learned from individual modalities) and (2) extends the previously
proposed cross-connections, which only transfer information between streams
that process compatible data. Both cross-modal architectures outperformed their
baselines (by up to 7.5%) when evaluated on the AVletters dataset.
",1,0,0,1,0,0
362,Making Sense of Physics through Stories: High School Students Narratives about Electric Charges and Interactions,"  Educational research has shown that narratives are useful tools that can help
young students make sense of scientific phenomena. Based on previous research,
I argue that narratives can also become tools for high school students to make
sense of concepts such as the electric field. In this paper I examine high
school students visual and oral narratives in which they describe the
interaction among electric charges as if they were characters of a cartoon
series. The study investigates: given the prompt to produce narratives for
electrostatic phenomena during a classroom activity prior to receiving formal
instruction, (1) what ideas of electrostatics do students attend to in their
narratives?; (2) what role do students narratives play in their understanding
of electrostatics? The participants were a group of high school students
engaged in an open-ended classroom activity prior to receiving formal
instruction about electrostatics. During the activity, the group was asked to
draw comic strips for electric charges. In addition to individual work,
students shared their work within small groups as well as with the whole group.
Post activity, six students from a small group were interviewed individually
about their work. In this paper I present two cases in which students produced
narratives to express their ideas about electrostatics in different ways. In
each case, I present student work for the comic strip activity (visual
narratives), their oral descriptions of their work (oral narratives) during the
interview and/or to their peers during class, and the their ideas of the
electric interactions expressed through their narratives.
",0,1,0,0,0,0
363,Preventing Hospital Acquired Infections Through a Workflow-Based Cyber-Physical System,"  Hospital acquired infections (HAI) are infections acquired within the
hospital from healthcare workers, patients or from the environment, but which
have no connection to the initial reason for the patient's hospital admission.
HAI are a serious world-wide problem, leading to an increase in mortality
rates, duration of hospitalisation as well as significant economic burden on
hospitals. Although clear preventive guidelines exist, studies show that
compliance to them is frequently poor. This paper details the software
perspective for an innovative, business process software based cyber-physical
system that will be implemented as part of a European Union-funded research
project. The system is composed of a network of sensors mounted in different
sites around the hospital, a series of wearables used by the healthcare workers
and a server side workflow engine. For better understanding, we describe the
system through the lens of a single, simple clinical workflow that is
responsible for a significant portion of all hospital infections. The goal is
that when completed, the system will be configurable in the sense of
facilitating the creation and automated monitoring of those clinical workflows
that when combined, account for over 90\% of hospital infections.
",1,0,0,0,0,0
364,OpenML Benchmarking Suites and the OpenML100,"  We advocate the use of curated, comprehensive benchmark suites of machine
learning datasets, backed by standardized OpenML-based interfaces and
complementary software toolkits written in Python, Java and R. Major
distinguishing features of OpenML benchmark suites are (a) ease of use through
standardized data formats, APIs, and existing client libraries; (b)
machine-readable meta-information regarding the contents of the suite; and (c)
online sharing of results, enabling large scale comparisons. As a first such
suite, we propose the OpenML100, a machine learning benchmark suite of
100~classification datasets carefully curated from the thousands of datasets
available on OpenML.org.
",1,0,0,1,0,0
365,On orbifold constructions associated with the Leech lattice vertex operator algebra,"  In this article, we study orbifold constructions associated with the Leech
lattice vertex operator algebra. As an application, we prove that the structure
of a strongly regular holomorphic vertex operator algebra of central charge
$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra
has the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,
$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse
orbifold construction. Our result also provides alternative constructions of
these vertex operator algebras (except for the case $A_{6,7}$) from the Leech
lattice vertex operator algebra.
",0,0,1,0,0,0
366,From mindless mathematics to thinking meat?,"  Deconstruction of the theme of the 2017 FQXi essay contest is already an
interesting exercise in its own right: Teleology is rarely useful in physics
--- the only known mainstream physics example (black hole event horizons) has a
very mixed score-card --- so the ""goals"" and ""aims and intentions"" alluded to
in the theme of the 2017 FQXi essay contest are already somewhat pushing the
limits. Furthermore, ""aims and intentions"" certainly carries the implication of
consciousness, and opens up a whole can of worms related to the mind-body
problem. As for ""mindless mathematical laws"", that allusion is certainly in
tension with at least some versions of the ""mathematical universe hypothesis"".
Finally ""wandering towards a goal"" again carries the implication of
consciousness, with all its attendant problems.
In this essay I will argue, simply because we do not yet have any really good
mathematical or physical theory of consciousness, that the theme of this essay
contest is premature, and unlikely to lead to any resolution that would be
widely accepted in the mathematics or physics communities.
",0,1,0,0,0,0
367,Phase correction for ALMA - Investigating water vapour radiometer scaling:The long-baseline science verification data case study,"  The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water
vapour radiometers (WVR), which monitor the atmospheric water vapour line at
183 GHz along the line of sight above each antenna to correct for phase delays
introduced by the wet component of the troposphere. The application of WVR
derived phase corrections improve the image quality and facilitate successful
observations in weather conditions that were classically marginal or poor. We
present work to indicate that a scaling factor applied to the WVR solutions can
act to further improve the phase stability and image quality of ALMA data. We
find reduced phase noise statistics for 62 out of 75 datasets from the
long-baseline science verification campaign after a WVR scaling factor is
applied. The improvement of phase noise translates to an expected coherence
improvement in 39 datasets. When imaging the bandpass source, we find 33 of the
39 datasets show an improvement in the signal-to-noise ratio (S/N) between a
few to ~30 percent. There are 23 datasets where the S/N of the science image is
improved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies
studied (band 6 and band 7) are those most improved, specifically datasets with
low precipitable water vapour (PWV), <1mm, where the dominance of the wet
component is reduced. Although these improvements are not profound, phase
stability improvements via the WVR scaling factor come into play for the higher
frequency (>450 GHz) and long-baseline (>5km) observations. These inherently
have poorer phase stability and are taken in low PWV (<1mm) conditions for
which we find the scaling to be most effective. A promising explanation for the
scaling factor is the mixing of dry and wet air components, although other
origins are discussed. We have produced a python code to allow ALMA users to
undertake WVR scaling tests and make improvements to their data.
",0,1,0,0,0,0
368,On the $μ$-ordinary locus of a Shimura variety,"  In this paper, we study the $\mu$-ordinary locus of a Shimura variety with
parahoric level structure. Under the axioms in \cite{HR}, we show that
$\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport
strata introduced in \cite{HR} and we give criteria on the density of the
$\mu$-ordinary locus.
",0,0,1,0,0,0
369,Scatteract: Automated extraction of data from scatter plots,"  Charts are an excellent way to convey patterns and trends in data, but they
do not facilitate further modeling of the data or close inspection of
individual data points. We present a fully automated system for extracting the
numerical values of data points from images of scatter plots. We use deep
learning techniques to identify the key components of the chart, and optical
character recognition together with robust regression to map from pixels to the
coordinate system of the chart. We focus on scatter plots with linear scales,
which already have several interesting challenges. Previous work has done fully
automatic extraction for other types of charts, but to our knowledge this is
the first approach that is fully automatic for scatter plots. Our method
performs well, achieving successful data extraction on 89% of the plots in our
test set.
",1,0,0,1,0,0
370,When the Universe Expands Too Fast: Relentless Dark Matter,"  We consider a modification to the standard cosmological history consisting of
introducing a new species $\phi$ whose energy density red-shifts with the scale
factor $a$ like $\rho_\phi \propto a^{-(4+n)}$. For $n>0$, such a red-shift is
faster than radiation, hence the new species dominates the energy budget of the
universe at early times while it is completely negligible at late times. If
equality with the radiation energy density is achieved at low enough
temperatures, dark matter can be produced as a thermal relic during the new
cosmological phase. Dark matter freeze-out then occurs at higher temperatures
compared to the standard case, implying that reproducing the observed abundance
requires significantly larger annihilation rates. Here, we point out a
completely new phenomenon, which we refer to as $\textit{relentless}$ dark
matter: for large enough $n$, unlike the standard case where annihilation ends
shortly after the departure from thermal equilibrium, dark matter particles
keep annihilating long after leaving chemical equilibrium, with a significant
depletion of the final relic abundance. Relentless annihilation occurs for $n
\geq 2$ and $n \geq 4$ for s-wave and p-wave annihilation, respectively, and it
thus occurs in well motivated scenarios such as a quintessence with a kination
phase. We discuss a few microscopic realizations for the new cosmological
component and highlight the phenomenological consequences of our calculations
for dark matter searches.
",0,1,0,0,0,0
371,Polygons with prescribed edge slopes: configuration space and extremal points of perimeter,"  We describe the configuration space $\mathbf{S}$ of polygons with prescribed
edge slopes, and study the perimeter $\mathcal{P}$ as a Morse function on
$\mathbf{S}$. We characterize critical points of $\mathcal{P}$ (these are
\textit{tangential} polygons) and compute their Morse indices. This setup is
motivated by a number of results about critical points and Morse indices of the
oriented area function defined on the configuration space of polygons with
prescribed edge lengths (flexible polygons). As a by-product, we present an
independent computation of the Morse index of the area function (obtained
earlier by G. Panina and A. Zhukova).
",0,0,1,0,0,0
372,An Asymptotically Efficient Metropolis-Hastings Sampler for Bayesian Inference in Large-Scale Educational Measuremen,"  This paper discusses a Metropolis-Hastings algorithm developed by
\citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is
proven that the algorithm becomes more efficient with more data and meets the
growing demands of large scale educational measurement.
",0,0,0,1,0,0
373,When Streams of Optofluidics Meet the Sea of Life,"  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National
University of Singapore. In this contribution he describes the power of
optofluidics as a research tool and reviews new insights within the areas of
single cell analysis, microphysiological analysis, and integrated systems.
",0,0,0,0,1,0
374,"L lines, C points and Chern numbers: understanding band structure topology using polarization fields","  Topology has appeared in different physical contexts. The most prominent
application is topologically protected edge transport in condensed matter
physics. The Chern number, the topological invariant of gapped Bloch
Hamiltonians, is an important quantity in this field. Another example of
topology, in polarization physics, are polarization singularities, called L
lines and C points. By establishing a connection between these two theories, we
develop a novel technique to visualize and potentially measure the Chern
number: it can be expressed either as the winding of the polarization azimuth
along L lines in reciprocal space, or in terms of the handedness and the index
of the C points. For mechanical systems, this is directly connected to the
visible motion patterns.
",0,1,0,0,0,0
375,Willis Theory via Graphs,"  We study the scale and tidy subgroups of an endomorphism of a totally
disconnected locally compact group using a geometric framework. This leads to
new interpretations of tidy subgroups and the scale function. Foremost, we
obtain a geometric tidying procedure which applies to endomorphisms as well as
a geometric proof of the fact that tidiness is equivalent to being minimizing
for a given endomorphism. Our framework also yields an endomorphism version of
the Baumgartner-Willis tree representation theorem. We conclude with a
construction of new endomorphisms of totally disconnected locally compact
groups from old via HNN-extensions.
",0,0,1,0,0,0
376,The Effect of Site-Specific Spectral Densities on the High-Dimensional Exciton-Vibrational Dynamics in the FMO Complex,"  The coupled exciton-vibrational dynamics of a three-site model of the FMO
complex is investigated using the Multi-layer Multi-configuration
Time-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of
the spectral density on the exciton state populations as well as on the
vibrational and vibronic non-equilibrium excitations. Models which use either a
single or site-specific spectral densities are contrasted to a spectral density
adapted from experiment. For the transfer efficiency, the total integrated
Huang-Rhys factor is found to be more important than details of the spectral
distributions. However, the latter are relevant for the obtained
non-equilibrium vibrational and vibronic distributions and thus influence the
actual pattern of population relaxation.
",0,1,0,0,0,0
377,Space dependent adhesion forces mediated by transient elastic linkages : new convergence and global existence results,"  In the first part of this work we show the convergence with respect to an
asymptotic parameter {\epsilon} of a delayed heat equation. It represents a
mathematical extension of works considered previously by the authors [Milisic
et al. 2011, Milisic et al. 2016]. Namely, this is the first result involving
delay operators approximating protein linkages coupled with a spatial elliptic
second order operator. For the sake of simplicity we choose the Laplace
operator, although more general results could be derived. The main arguments
are (i) new energy estimates and (ii) a stability result extended from the
previous work to this more involved context. They allow to prove convergence of
the delay operator to a friction term together with the Laplace operator in the
same asymptotic regime considered without the space dependence in [Milisic et
al, 2011]. In a second part we extend fixed-point results for the fully
non-linear model introduced in [Milisic et al, 2016] and prove global existence
in time. This shows that the blow-up scenario observed previously does not
occur. Since the latter result was interpreted as a rupture of adhesion forces,
we discuss the possibility of bond breaking both from the analytic and
numerical point of view.
",0,0,1,0,0,0
378,On the nonparametric maximum likelihood estimator for Gaussian location mixture densities with application to Gaussian denoising,"  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for
estimating Gaussian location mixture densities in $d$-dimensions from
independent observations. Unlike usual likelihood-based methods for fitting
mixtures, NPMLEs are based on convex optimization. We prove finite sample
results on the Hellinger accuracy of every NPMLE. Our results imply, in
particular, that every NPMLE achieves near parametric risk (up to logarithmic
multiplicative factors) when the true density is a discrete Gaussian mixture
without any prior information on the number of mixture components. NPMLEs can
naturally be used to yield empirical Bayes estimates of the Oracle Bayes
estimator in the Gaussian denoising problem. We prove bounds for the accuracy
of the empirical Bayes estimate as an approximation to the Oracle Bayes
estimator. Here our results imply that the empirical Bayes estimator performs
at nearly the optimal level (up to logarithmic multiplicative factors) for
denoising in clustering situations without any prior knowledge of the number of
clusters.
",0,0,1,1,0,0
379,Gravitational wave production from preheating: parameter dependence,"  Parametric resonance is among the most efficient phenomena generating
gravitational waves (GWs) in the early Universe. The dynamics of parametric
resonance, and hence of the GWs, depend exclusively on the resonance parameter
$q$. The latter is determined by the properties of each scenario: the initial
amplitude and potential curvature of the oscillating field, and its coupling to
other species. Previous works have only studied the GW production for fixed
value(s) of $q$. We present an analytical derivation of the GW amplitude
dependence on $q$, valid for any scenario, which we confront against numerical
results. By running lattice simulations in an expanding grid, we study for a
wide range of $q$ values, the production of GWs in post-inflationary preheating
scenarios driven by parametric resonance. We present simple fits for the final
amplitude and position of the local maxima in the GW spectrum. Our
parametrization allows to predict the location and amplitude of the GW
background today, for an arbitrary $q$. The GW signal can be rather large, as
$h^2\Omega_{\rm GW}(f_p) \lesssim 10^{-11}$, but it is always peaked at high
frequencies $f_p \gtrsim 10^{7}$ Hz. We also discuss the case of
spectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,
or the Standard Model Higgs.
",0,1,0,0,0,0
380,IP determination and 1+1 REMPI spectrum of SiO at 210-220 nm with implications for SiO$^{+}$ ion trap loading,"  The 1+1 REMPI spectrum of SiO in the 210-220 nm range is recorded. Observed
bands are assigned to the $A-X$ vibrational bands $(v``=0-3, v`=5-10)$ and a
tentative assignment is given to the 2-photon transition from $X$ to the
n=12-13 $[X^{2}{\Sigma}^{+},v^{+}=1]$ Rydberg states at 216-217 nm. We estimate
the IP of SiO to be 11.59(1) eV. The SiO$^{+}$ cation has previously been
identified as a molecular candidate amenable to laser control. Our work allows
us to identify an efficient method for loading cold SiO$^{+}$ from an ablated
sample of SiO into an ion trap via the $(5,0)$ $A-X$ band at 213.977 nm.
",0,1,0,0,0,0
381,Adaptive Model Predictive Control for High-Accuracy Trajectory Tracking in Changing Conditions,"  Robots and automated systems are increasingly being introduced to unknown and
dynamic environments where they are required to handle disturbances, unmodeled
dynamics, and parametric uncertainties. Robust and adaptive control strategies
are required to achieve high performance in these dynamic environments. In this
paper, we propose a novel adaptive model predictive controller that combines
model predictive control (MPC) with an underlying $\mathcal{L}_1$ adaptive
controller to improve trajectory tracking of a system subject to unknown and
changing disturbances. The $\mathcal{L}_1$ adaptive controller forces the
system to behave in a predefined way, as specified by a reference model. A
higher-level model predictive controller then uses this reference model to
calculate the optimal reference input based on a cost function, while taking
into account input and state constraints. We focus on the experimental
validation of the proposed approach and demonstrate its effectiveness in
experiments on a quadrotor. We show that the proposed approach has a lower
trajectory tracking error compared to non-predictive, adaptive approaches and a
predictive, non-adaptive approach, even when external wind disturbances are
applied.
",1,0,0,0,0,0
382,An enhanced method to compute the similarity between concepts of ontology,"  With the use of ontologies in several domains such as semantic web,
information retrieval, artificial intelligence, the concept of similarity
measuring has become a very important domain of research. Therefore, in the
current paper, we propose our method of similarity measuring which uses the
Dijkstra algorithm to define and compute the shortest path. Then, we use this
one to compute the semantic distance between two concepts defined in the same
hierarchy of ontology. Afterward, we base on this result to compute the
semantic similarity. Finally, we present an experimental comparison between our
method and other methods of similarity measuring.
",1,0,0,0,0,0
383,Eigenvalues of symmetric tridiagonal interval matrices revisited,"  In this short note, we present a novel method for computing exact lower and
upper bounds of eigenvalues of a symmetric tridiagonal interval matrix.
Compared to the known methods, our approach is fast, simple to present and to
implement, and avoids any assumptions. Our construction explicitly yields those
matrices for which particular lower and upper bounds are attained.
",1,0,0,0,0,0
384,PRE-render Content Using Tiles (PRECUT). 1. Large-Scale Compound-Target Relationship Analyses,"  Visualizing a complex network is computationally intensive process and
depends heavily on the number of components in the network. One way to solve
this problem is not to render the network in real time. PRE-render Content
Using Tiles (PRECUT) is a process to convert any complex network into a
pre-rendered network. Tiles are generated from pre-rendered images at different
zoom levels, and navigating the network simply becomes delivering relevant
tiles. PRECUT is exemplified by performing large-scale compound-target
relationship analyses. Matched molecular pair (MMP) networks were created using
compounds and the target class description found in the ChEMBL database. To
visualize MMP networks, the MMP network viewer has been implemented in COMBINE
and as a web application, hosted at this http URL.
",1,0,0,0,0,0
385,The list chromatic number of graphs with small clique number,"  We prove that every triangle-free graph with maximum degree $\Delta$ has list
chromatic number at most $(1+o(1))\frac{\Delta}{\ln \Delta}$. This matches the
best-known bound for graphs of girth at least 5. We also provide a new proof
that for any $r\geq 4$ every $K_r$-free graph has list-chromatic number at most
$200r\frac{\Delta\ln\ln\Delta}{\ln\Delta}$.
",0,0,1,0,0,0
386,Topology of Large-Scale Structures of Galaxies in Two Dimensions - Systematic Effects,"  We study the two-dimensional topology of the galactic distribution when
projected onto two-dimensional spherical shells. Using the latest Horizon Run 4
simulation data, we construct the genus of the two-dimensional field and
consider how this statistic is affected by late-time nonlinear effects --
principally gravitational collapse and redshift space distortion (RSD). We also
consider systematic and numerical artifacts such as shot noise, galaxy bias,
and finite pixel effects. We model the systematics using a Hermite polynomial
expansion and perform a comprehensive analysis of known effects on the
two-dimensional genus, with a view toward using the statistic for cosmological
parameter estimation. We find that the finite pixel effect is dominated by an
amplitude drop and can be made less than $1\%$ by adopting pixels smaller than
$1/3$ of the angular smoothing length. Nonlinear gravitational evolution
introduces time-dependent coefficients of the zeroth, first, and second Hermite
polynomials, but the genus amplitude changes by less than $1\%$ between $z=1$
and $z=0$ for smoothing scales $R_{\rm G} > 9 {\rm Mpc/h}$. Non-zero terms are
measured up to third order in the Hermite polynomial expansion when studying
RSD. Differences in shapes of the genus curves in real and redshift space are
small when we adopt thick redshift shells, but the amplitude change remains a
significant $\sim {\cal O}(10\%)$ effect. The combined effects of galaxy
biasing and shot noise produce systematic effects up to the second Hermite
polynomial. It is shown that, when sampling, the use of galaxy mass cuts
significantly reduces the effect of shot noise relative to random sampling.
",0,1,0,0,0,0
387,Wiki-index of authors popularity,"  The new index of the author's popularity estimation is represented in the
paper. The index is calculated on the basis of Wikipedia encyclopedia analysis
(Wikipedia Index - WI). Unlike the conventional existed citation indices, the
suggested mark allows to evaluate not only the popularity of the author, as it
can be done by means of calculating the general citation number or by the
Hirsch index, which is often used to measure the author's research rate. The
index gives an opportunity to estimate the author's popularity, his/her
influence within the sought-after area ""knowledge area"" in the Internet - in
the Wikipedia. The suggested index is supposed to be calculated in frames of
the subject domain, and it, on the one hand, avoids the mistaken computation of
the homonyms, and on the other hand - provides the entirety of the subject
area. There are proposed algorithms and the technique of the Wikipedia Index
calculation through the network encyclopedia sounding, the exemplified
calculations of the index for the prominent researchers, and also the methods
of the information networks formation - models of the subject domains by the
automatic monitoring and networks information reference resources analysis. The
considered in the paper notion network corresponds the terms-heads of the
Wikipedia articles.
",1,0,0,0,0,0
388,Belyi map for the sporadic group J1,"  We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree
266 and describe the applied method. This yields explicit polynomials having J1
as a Galois group over K(t), [K:Q] = 7.
",0,0,1,0,0,0
389,"Convolution Semigroups of Probability Measures on Gelfand Pairs, Revisited","  Our goal is to find classes of convolution semigroups on Lie groups $G$ that
give rise to interesting processes in symmetric spaces $G/K$. The
$K$-bi-invariant convolution semigroups are a well-studied example. An
appealing direction for the next step is to generalise to right $K$-invariant
convolution semigroups, but recent work of Liao has shown that these are in
one-to-one correspondence with $K$-bi-invariant convolution semigroups. We
investigate a weaker notion of right $K$-invariance, but show that this is, in
fact, the same as the usual notion. Another possible approach is to use
generalised notions of negative definite functions, but this also leads to
nothing new. We finally find an interesting class of convolution semigroups
that are obtained by making use of the Cartan decomposition of a semisimple Lie
group, and the solution of certain stochastic differential equations. Examples
suggest that these are well-suited for generating random motion along geodesics
in symmetric spaces.
",0,0,1,0,0,0
390,Toward Optimal Coupon Allocation in Social Networks: An Approximate Submodular Optimization Approach,"  CMO Council reports that 71\% of internet users in the U.S. were influenced
by coupons and discounts when making their purchase decisions. It has also been
shown that offering coupons to a small fraction of users (called seed users)
may affect the purchase decisions of many other users in a social network. This
motivates us to study the optimal coupon allocation problem, and our objective
is to allocate coupons to a set of users so as to maximize the expected
cascade. Different from existing studies on influence maximizaton (IM), our
framework allows a general utility function and a more complex set of
constraints. In particular, we formulate our problem as an approximate
submodular maximization problem subject to matroid and knapsack constraints.
Existing techniques relying on the submodularity of the utility function, such
as greedy algorithm, can not work directly on a non-submodular function. We use
$\epsilon$ to measure the difference between our function and its closest
submodular function and propose a novel approximate algorithm with
approximation ratio $\beta(\epsilon)$ with $\lim_{\epsilon\rightarrow
0}\beta(\epsilon)=1-1/e$. This is the best approximation guarantee for
approximate submodular maximization subject to a partition matroid and knapsack
constraints, our results apply to a broad range of optimization problems that
can be formulated as an approximate submodular maximization problem.
",1,0,0,0,0,0
391,Lefschetz duality for intersection (co)homology,"  We prove the Lefschetz duality for intersection (co)homology in the framework
of $\partial$-pesudomanifolds. We work with general perversities and without
restriction on the coefficient ring.
",0,0,1,0,0,0
392,Empirical determination of the optimum attack for fragmentation of modular networks,"  All possible removals of $n=5$ nodes from networks of size $N=100$ are
performed in order to find the optimal set of nodes which fragments the
original network into the smallest largest connected component. The resulting
attacks are ordered according to the size of the largest connected component
and compared with the state of the art methods of network attacks. We chose
attacks of size $5$ on relative small networks of size $100$ because the number
of all possible attacks, ${100}\choose{5}$ $\approx 10^8$, is at the verge of
the possible to compute with the available standard computers. Besides, we
applied the procedure in a series of networks with controlled and varied
modularity, comparing the resulting statistics with the effect of removing the
same amount of vertices, according to the known most efficient disruption
strategies, i.e., High Betweenness Adaptive attack (HBA), Collective Index
attack (CI), and Modular Based Attack (MBA). Results show that modularity has
an inverse relation with robustness, with $Q_c \approx 0.7$ being the critical
value. For modularities lower than $Q_c$, all heuristic method gives mostly the
same results than with random attacks, while for bigger $Q$, networks are less
robust and highly vulnerable to malicious attacks.
",1,0,0,0,0,0
393,Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence,"  In this paper, we study stochastic non-convex optimization with non-convex
random functions. Recent studies on non-convex optimization revolve around
establishing second-order convergence, i.e., converging to a nearly
second-order optimal stationary points. However, existing results on stochastic
non-convex optimization are limited, especially with a high probability
second-order convergence. We propose a novel updating step (named NCG-S) by
leveraging a stochastic gradient and a noisy negative curvature of a stochastic
Hessian, where the stochastic gradient and Hessian are based on a proper
mini-batch of random functions. Building on this step, we develop two
algorithms and establish their high probability second-order convergence. To
the best of our knowledge, the proposed stochastic algorithms are the first
with a second-order convergence in {\it high probability} and a time complexity
that is {\it almost linear} in the problem's dimensionality.
",1,0,0,1,0,0
394,On the optimality and sharpness of Laguerre's lower bound on the smallest eigenvalue of a symmetric positive definite matrix,"  Lower bounds on the smallest eigenvalue of a symmetric positive definite
matrices $A\in\mathbb{R}^{m\times m}$ play an important role in condition
number estimation and in iterative methods for singular value computation. In
particular, the bounds based on ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$
attract attention recently because they can be computed in $O(m)$ work when $A$
is tridiagonal. In this paper, we focus on these bounds and investigate their
properties in detail. First, we consider the problem of finding the optimal
bound that can be computed solely from ${\rm Tr}(A^{-1})$ and ${\rm
Tr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one
in terms of sharpness. Next, we study the gap between the Laguerre bound and
the smallest eigenvalue. We characterize the situation in which the gap becomes
largest in terms of the eigenvalue distribution of $A$ and show that the gap
becomes smallest when ${\rm Tr}(A^{-2})/\{{\rm Tr}(A^{-1})\}^2$ approaches 1 or
$\frac{1}{m}$. These results will be useful, for example, in designing
efficient shift strategies for singular value computation algorithms.
",1,0,1,0,0,0
395,Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques,"  This paper describes the development of a magnetic attitude control subsystem
for a 2U cubesat. Due to the presence of gravity gradient torques, the
satellite dynamics are open-loop unstable near the desired pointing
configuration. Nevertheless the linearized time-varying system is completely
controllable, under easily verifiable conditions, and the system's disturbance
rejection capabilities can be enhanced by adding air drag panels exemplifying a
beneficial interplay between hardware design and control. In the paper,
conditions for the complete controllability for the case of a magnetically
controlled satellite with passive air drag panels are developed, and simulation
case studies with the LQR and MPC control designs applied in combination with a
nonlinear time-varying input transformation are presented to demonstrate the
ability of the closed-loop system to satisfy mission objectives despite
disturbance torques.
",0,0,1,0,0,0
396,"Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem","  One advantage of decision tree based methods like random forests is their
ability to natively handle categorical predictors without having to first
transform them (e.g., by using feature engineering techniques). However, in
this paper, we show how this capability can lead to an inherent ""absent levels""
problem for decision tree based methods that has never been thoroughly
discussed, and whose consequences have never been carefully explored. This
problem occurs whenever there is an indeterminacy over how to handle an
observation that has reached a categorical split which was determined when the
observation in question's level was absent during training. Although these
incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's
random forests FORTRAN code and the randomForest R package (Liaw and Wiener,
2002) as motivating case studies, we examine how overlooking the absent levels
problem can systematically bias a model. Furthermore, by using three real data
examples, we illustrate how absent levels can dramatically alter a model's
performance in practice, and we empirically demonstrate how some simple
heuristics can be used to help mitigate the effects of the absent levels
problem until a more robust theoretical solution is found.
",1,0,0,1,0,0
397,Temporal correlation detection using computational phase-change memory,"  For decades, conventional computers based on the von Neumann architecture
have performed computation by repeatedly transferring data between their
processing and their memory units, which are physically separated. As
computation becomes increasingly data-centric and as the scalability limits in
terms of performance and power are being reached, alternative computing
paradigms are searched for in which computation and storage are collocated. A
fascinating new approach is that of computational memory where the physics of
nanoscale memory devices are used to perform certain computational tasks within
the memory unit in a non-von Neumann manner. Here we present a large-scale
experimental demonstration using one million phase-change memory devices
organized to perform a high-level computational primitive by exploiting the
crystallization dynamics. Also presented is an application of such a
computational memory to process real-world data-sets. The results show that
this co-existence of computation and storage at the nanometer scale could be
the enabler for new, ultra-dense, low power, and massively parallel computing
systems.
",1,0,0,0,0,0
398,Complexity and capacity bounds for quantum channels,"  We generalise some well-known graph parameters to operator systems by
considering their underlying quantum channels. In particular, we introduce the
quantum complexity as the dimension of the smallest co-domain Hilbert space a
quantum channel requires to realise a given operator system as its
non-commutative confusability graph. We describe quantum complexity as a
generalised minimum semidefinite rank and, in the case of a graph operator
system, as a quantum intersection number. The quantum complexity and a closely
related quantum version of orthogonal rank turn out to be upper bounds for the
Shannon zero-error capacity of a quantum channel, and we construct examples for
which these bounds beat the best previously known general upper bound for the
capacity of quantum channels, given by the quantum Lovász theta number.
",0,0,1,0,0,0
399,Quantum Interference of Glory Rescattering in Strong-Field Atomic Ionization,"  During the ionization of atoms irradiated by linearly polarized intense laser
fields, we find for the first time that the transverse momentum distribution of
photoelectrons can be well fitted by a squared zeroth-order Bessel function
because of the quantum interference effect of Glory rescattering. The
characteristic of the Bessel function is determined by the common angular
momentum of a bunch of semiclassical paths termed as Glory trajectories, which
are launched with different nonzero initial transverse momenta distributed on a
specific circle in the momentum plane and finally deflected to the same
asymptotic momentum, which is along the polarization direction, through
post-tunneling rescattering. Glory rescattering theory (GRT) based on the
semiclassical path-integral formalism is developed to address this effect
quantitatively. Our theory can resolve the long-standing discrepancies between
existing theories and experiments on the fringe location, predict the sudden
transition of the fringe structure in holographic patterns, and shed light on
the quantum interference aspects of low-energy structures in strong-field
atomic ionization.
",0,1,0,0,0,0
400,On vector measures and extensions of transfunctions,"  We are interested in extending operators defined on positive measures, called
here transfunctions, to signed measures and vector measures. Our methods use a
somewhat nonstandard approach to measures and vector measures. The necessary
background, including proofs of some auxiliary results, is included.
",0,0,1,0,0,0
401,Deep Within-Class Covariance Analysis for Robust Audio Representation Learning,"  Convolutional Neural Networks (CNNs) can learn effective features, though
have been shown to suffer from a performance drop when the distribution of the
data changes from training to test data. In this paper we analyze the internal
representations of CNNs and observe that the representations of unseen data in
each class, spread more (with higher variance) in the embedding space of the
CNN compared to representations of the training data. More importantly, this
difference is more extreme if the unseen data comes from a shifted
distribution. Based on this observation, we objectively evaluate the degree of
representation's variance in each class via eigenvalue decomposition on the
within-class covariance of the internal representations of CNNs and observe the
same behaviour. This can be problematic as larger variances might lead to
mis-classification if the sample crosses the decision boundary of its class. We
apply nearest neighbor classification on the representations and empirically
show that the embeddings with the high variance actually have significantly
worse KNN classification performances, although this could not be foreseen from
their end-to-end classification results. To tackle this problem, we propose
Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that
significantly reduces the within-class covariance of a DNN's representation,
improving performance on unseen test data from a shifted distribution. We
empirically evaluate DWCCA on two datasets for Acoustic Scene Classification
(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA
significantly improve the network's internal representation, it also increases
the end-to-end classification accuracy, especially when the test set exhibits a
distribution shift. By adding DWCCA to a VGG network, we achieve around 6
percentage points improvement in the case of a distribution mismatch.
",1,0,0,0,0,0
402,Efficient Online Bandit Multiclass Learning with $\tilde{O}(\sqrt{T})$ Regret,"  We present an efficient second-order algorithm with
$\tilde{O}(\frac{1}{\eta}\sqrt{T})$ regret for the bandit online multiclass
problem. The regret bound holds simultaneously with respect to a family of loss
functions parameterized by $\eta$, for a range of $\eta$ restricted by the norm
of the competitor. The family of loss functions ranges from hinge loss
($\eta=0$) to squared hinge loss ($\eta=1$). This provides a solution to the
open problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for
$\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our
algorithm experimentally, showing that it also performs favorably against
earlier algorithms.
",0,0,0,1,0,0
403,Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning,"  Swarm systems constitute a challenging problem for reinforcement learning
(RL) as the algorithm needs to learn decentralized control policies that can
cope with limited local sensing and communication abilities of the agents.
While it is often difficult to directly define the behavior of the agents,
simple communication protocols can be defined more easily using prior knowledge
about the given task. In this paper, we propose a number of simple
communication protocols that can be exploited by deep reinforcement learning to
find decentralized control policies in a multi-robot swarm environment. The
protocols are based on histograms that encode the local neighborhood relations
of the agents and can also transmit task-specific information, such as the
shortest distance and direction to a desired target. In our framework, we use
an adaptation of Trust Region Policy Optimization to learn complex
collaborative tasks, such as formation building and building a communication
link. We evaluate our findings in a simulated 2D-physics environment, and
compare the implications of different communication protocols.
",1,0,0,1,0,0
404,Towards exascale real-time RFI mitigation,"  We describe the design and implementation of an extremely scalable real-time
RFI mitigation method, based on the offline AOFlagger. All algorithms scale
linearly in the number of samples. We describe how we implemented the flagger
in the LOFAR real-time pipeline, on both CPUs and GPUs. Additionally, we
introduce a novel simple history-based flagger that helps reduce the impact of
our small window on the data.
By examining an observation of a known pulsar, we demonstrate that our
flagger can achieve much higher quality than a simple thresholder, even when
running in real time, on a distributed system. The flagger works on visibility
data, but also on raw voltages, and beam formed data. The algorithms are
scale-invariant, and work on microsecond to second time scales. We are
currently implementing a prototype for the time domain pipeline of the SKA
central signal processor.
",0,1,0,0,0,0
405,Learning body-affordances to simplify action spaces,"  Controlling embodied agents with many actuated degrees of freedom is a
challenging task. We propose a method that can discover and interpolate between
context dependent high-level actions or body-affordances. These provide an
abstract, low-dimensional interface indexing high-dimensional and time-
extended action policies. Our method is related to recent ap- proaches in the
machine learning literature but is conceptually simpler and easier to
implement. More specifically our method requires the choice of a n-dimensional
target sensor space that is endowed with a distance metric. The method then
learns an also n-dimensional embedding of possibly reactive body-affordances
that spread as far as possible throughout the target sensor space.
",1,0,0,0,0,0
406,Cayley properties of the line graphs induced by of consecutive layers of the hypercube,"  Let $n >3$ and $ 0< k < \frac{n}{2} $ be integers. In this paper, we
investigate some algebraic properties of the line graph of the graph $
{Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$
which is induced by the set of vertices of weights $k$ and $k+1$. In the first
step, we determine the automorphism groups of these graphs for all values of
$k$. In the second step, we study Cayley properties of the line graph of these
graphs. In particular, we show that for $ k>2, $ if $ 2k+1 \neq n$, then the
line graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley
graph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a
Cayley graph if and only if $ n$ is a power of a prime $p$.
",0,0,1,0,0,0
407,Beyond the technical challenges for deploying Machine Learning solutions in a software company,"  Recently software development companies started to embrace Machine Learning
(ML) techniques for introducing a series of advanced functionality in their
products such as personalisation of the user experience, improved search,
content recommendation and automation. The technical challenges for tackling
these problems are heavily researched in literature. A less studied area is a
pragmatic approach to the role of humans in a complex modern industrial
environment where ML based systems are developed. Key stakeholders affect the
system from inception and up to operation and maintenance. Product managers
want to embed ""smart"" experiences for their users and drive the decisions on
what should be built next; software engineers are challenged to build or
utilise ML software tools that require skills that are well outside of their
comfort zone; legal and risk departments may influence design choices and data
access; operations teams are requested to maintain ML systems which are
non-stationary in their nature and change behaviour over time; and finally ML
practitioners should communicate with all these stakeholders to successfully
build a reliable system. This paper discusses some of the challenges we faced
in Atlassian as we started investing more in the ML space.
",1,0,0,1,0,0
408,Class-Splitting Generative Adversarial Networks,"  Generative Adversarial Networks (GANs) produce systematically better quality
samples when class label information is provided., i.e. in the conditional GAN
setup. This is still observed for the recently proposed Wasserstein GAN
formulation which stabilized adversarial training and allows considering high
capacity network architectures such as ResNet. In this work we show how to
boost conditional GAN by augmenting available class labels. The new classes
come from clustering in the representation space learned by the same GAN model.
The proposed strategy is also feasible when no class information is available,
i.e. in the unsupervised setup. Our generated samples reach state-of-the-art
Inception scores for CIFAR-10 and STL-10 datasets in both supervised and
unsupervised setup.
",0,0,0,1,0,0
409,Dynamical system analysis of dark energy models in scalar coupled metric-torsion theories,"  We study the phase space dynamics of cosmological models in the theoretical
formulations of non-minimal metric-torsion couplings with a scalar field, and
investigate in particular the critical points which yield stable solutions
exhibiting cosmic acceleration driven by the {\em dark energy}. The latter is
defined in a way that it effectively has no direct interaction with the
cosmological fluid, although in an equivalent scalar-tensor cosmological setup
the scalar field interacts with the fluid (which we consider to be the
pressureless dust). Determining the conditions for the existence of the stable
critical points we check their physical viability, in both Einstein and Jordan
frames. We also verify that in either of these frames, the evolution of the
universe at the corresponding stable points matches with that given by the
respective exact solutions we have found in an earlier work (arXiv: 1611.00654
[gr-qc]). We not only examine the regions of physical relevance for the
trajectories in the phase space when the coupling parameter is varied, but also
demonstrate the evolution profiles of the cosmological parameters of interest
along fiducial trajectories in the effectively non-interacting scenarios, in
both Einstein and Jordan frames.
",0,1,0,0,0,0
410,J-MOD$^{2}$: Joint Monocular Obstacle Detection and Depth Estimation,"  In this work, we propose an end-to-end deep architecture that jointly learns
to detect obstacles and estimate their depth for MAV flight applications. Most
of the existing approaches either rely on Visual SLAM systems or on depth
estimation models to build 3D maps and detect obstacles. However, for the task
of avoiding obstacles this level of complexity is not required. Recent works
have proposed multi task architectures to both perform scene understanding and
depth estimation. We follow their track and propose a specific architecture to
jointly estimate depth and obstacles, without the need to compute a global map,
but maintaining compatibility with a global SLAM system if needed. The network
architecture is devised to exploit the joint information of the obstacle
detection task, that produces more reliable bounding boxes, with the depth
estimation one, increasing the robustness of both to scenario changes. We call
this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
experiments on sequences with different appearance and focal lengths and
compare it to SotA multi task methods that jointly perform semantic
segmentation and depth estimation. In addition, we show the integration in a
full system using a set of simulated navigation experiments where a MAV
explores an unknown scenario and plans safe trajectories by using our detection
model.
",1,0,0,0,0,0
411,The Calabi flow with rough initial data,"  In this paper, we prove that there exists a dimensional constant $\delta > 0$
such that given any background Kähler metric $\omega$, the Calabi flow with
initial data $u_0$ satisfying \begin{equation*} \partial \bar \partial u_0 \in
L^\infty (M) \text{ and } (1- \delta )\omega < \omega_{u_0} < (1+\delta
)\omega, \end{equation*} admits a unique short time solution and it becomes
smooth immediately, where $\omega_{u_0} : = \omega +\sqrt{-1}\partial
\bar\partial u_0$. The existence time depends on initial data $u_0$ and the
metric $\omega$. As a corollary, we get that Calabi flow has short time
existence for any initial data satisfying \begin{equation*} \partial \bar
\partial u_0 \in C^0(M) \text{ and } \omega_{u_0} > 0, \end{equation*} which
should be interpreted as a ""continuous Kähler metric"". A main technical
ingredient is Schauder-type estimates for biharmonic heat equation on
Riemannian manifolds with time weighted Hölder norms.
",0,0,1,0,0,0
412,Star Formation Activity in the molecular cloud G35.20$-$0.74: onset of cloud-cloud collision,"  To probe the star-formation (SF) processes, we present results of an analysis
of the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency
observations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.
An almost horseshoe-like structure embedded within the MCG35.2 is evident in
the infrared and millimeter images and harbors the previously known sites,
ultra-compact/hyper-compact G35.20$-$0.74N H\,{\sc ii} region, Ap2-1, and
Mercer 14 at its base. The site, Ap2-1 is found to be excited by a radio
spectral type of B0.5V star where the distribution of 20 cm and H$\alpha$
emission is surrounded by the extended molecular hydrogen emission. Using the
{\it Herschel} 160-500 $\mu$m and photometric 1-24 $\mu$m data analysis,
several embedded clumps and clusters of young stellar objects (YSOs) are
investigated within the MCG35.2, revealing the SF activities. Majority of the
YSOs clusters and massive clumps (500-4250 M$_{\odot}$) are seen toward the
horseshoe-like structure. The position-velocity analysis of $^{13}$CO emission
shows a blue-shifted peak (at 33 km s$^{-1}$) and a red-shifted peak (at 37 km
s$^{-1}$) interconnected by lower intensity intermediated velocity emission,
tracing a broad bridge feature. The presence of such broad bridge feature
suggests the onset of a collision between molecular components in the MCG35.2.
A noticeable change in the H-band starlight mean polarization angles has also
been observed in the MCG35.2, probably tracing the interaction between
molecular components. Taken together, it seems that the cloud-cloud collision
process has influenced the birth of massive stars and YSOs clusters in the
MCG35.2.
",0,1,0,0,0,0
413,Oblivious Routing via Random Walks,"  We present novel oblivious routing algorithms for both splittable and
unsplittable multicommodity flow. Our algorithm for minimizing congestion for
\emph{unsplittable} multicommodity flow is the first oblivious routing
algorithm for this setting. As an intermediate step towards this algorithm, we
present a novel generalization of Valiant's classical load balancing scheme for
packet-switched networks to arbitrary graphs, which is of independent interest.
Our algorithm for minimizing congestion for \emph{splittable} multicommodity
flow improves upon the state-of-the-art, in terms of both running time and
performance, for graphs that exhibit good expansion guarantees. Our algorithms
rely on diffusing traffic via iterative applications of the random walk
operator. Consequently, the performance guarantees of our algorithms are
derived from the convergence of the random walk operator to the stationary
distribution and are expressed in terms of the spectral gap of the graph (which
dominates the mixing time).
",1,0,0,0,0,0
414,On Functional Graphs of Quadratic Polynomials,"  We study functional graphs generated by quadratic polynomials over prime
fields. We introduce efficient algorithms for methodical computations and
provide the values of various direct and cumulative statistical parameters of
interest. These include: the number of connected functional graphs, the number
of graphs having a maximal cycle, the number of cycles of fixed size, the
number of components of fixed size, as well as the shape of trees extracted
from functional graphs. We particularly focus on connected functional graphs,
that is, the graphs which contain only one component (and thus only one cycle).
Based on the results of our computations, we formulate several conjectures
highlighting the similarities and differences between these functional graphs
and random mappings.
",0,0,1,0,0,0
415,Helmholtz decomposition theorem and Blumenthal's extension by regularization,"  Helmholtz decomposition theorem for vector fields is usually presented with
too strong restrictions on the fields and only for time independent fields.
Blumenthal showed in 1905 that decomposition is possible for any asymptotically
weakly decreasing vector field. He used a regularization method in his proof
which can be extended to prove the theorem even for vector fields
asymptotically increasing sublinearly. Blumenthal's result is then applied to
the time-dependent fields of the dipole radiation and an artificial sublinearly
increasing field.
",0,1,0,0,0,0
416,A homotopy decomposition of the fibre of the squaring map on $Ω^3S^{17}$,"  We use Richter's $2$-primary proof of Gray's conjecture to give a homotopy
decomposition of the fibre $\Omega^3S^{17}\{2\}$ of the $H$-space squaring map
on the triple loop space of the $17$-sphere. This induces a splitting of the
mod-$2$ homotopy groups $\pi_\ast(S^{17}; \mathbb{Z}/2\mathbb{Z})$ in terms of
the integral homotopy groups of the fibre of the double suspension
$E^2:S^{2n-1} \to \Omega^2S^{2n+1}$ and refines a result of Cohen and Selick,
who gave similar decompositions for $S^5$ and $S^9$. We relate these
decompositions to various Whitehead products in the homotopy groups of mod-$2$
Moore spaces and Stiefel manifolds to show that the Whitehead square $[i_{2n},
i_{2n}]$ of the inclusion of the bottom cell of the Moore space $P^{2n+1}(2)$
is divisible by $2$ if and only if $2n=2, 4, 8$ or $16$.
",0,0,1,0,0,0
417,Spaces of orders of some one-relator groups,"  We show that certain orderable groups admit no isolated left orders. The
groups we consider are cyclic amalgamations of a free group with a general
orderable group, the HNN extensions of free groups over cyclic subgroups, and a
particular class of one-relator groups. In order to prove the results about
orders, we develop perturbation techniques for actions of these groups on the
line.
",0,0,1,0,0,0
418,Adversarial Attacks on Neural Network Policies,"  Machine learning classifiers are known to be vulnerable to inputs maliciously
constructed by adversaries to force misclassification. Such adversarial
examples have been extensively studied in the context of computer vision
applications. In this work, we show adversarial attacks are also effective when
targeting neural network policies in reinforcement learning. Specifically, we
show existing adversarial example crafting techniques can be used to
significantly degrade test-time performance of trained policies. Our threat
model considers adversaries capable of introducing small perturbations to the
raw input of the policy. We characterize the degree of vulnerability across
tasks and training algorithms, for a subclass of adversarial-example attacks in
white-box and black-box settings. Regardless of the learned task or training
algorithm, we observe a significant drop in performance, even with small
adversarial perturbations that do not interfere with human perception. Videos
are available at this http URL.
",1,0,0,1,0,0
419,Stellar streams as gravitational experiments I. The case of Sagittarius,"  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy
offer a unique way to constrain the shape of galactic gravitational potentials.
Such streams can be used as leaning tower gravitational experiments on galactic
scales. The most well motivated modification of gravity proposed as an
alternative to dark matter on galactic scales is Milgromian dynamics (MOND),
and we present here the first ever N-body simulations of the dynamical
evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a
realistic baryonic mass model for the Milky Way, we attempt to reproduce the
present-day spatial and kinematic structure of the Sagittarius dwarf and its
immense tidal stream that wraps around the Milky Way. With very little freedom
on the original structure of the progenitor, constrained by the total
luminosity of the Sagittarius structure and by the observed stellar mass-size
relation for isolated dwarf galaxies, we find reasonable agreement between our
simulations and observations of this system. The observed stellar velocities in
the leading arm can be reproduced if we include a massive hot gas corona around
the Milky Way that is flattened in the direction of the principal plane of its
satellites. This is the first time that tidal dissolution in MOND has been
tested rigorously at these mass and acceleration scales.
",0,1,0,0,0,0
420,Tuning quantum non-local effects in graphene plasmonics,"  The response of an electron system to electromagnetic fields with sharp
spatial variations is strongly dependent on quantum electronic properties, even
in ambient conditions, but difficult to access experimentally. We use
propagating graphene plasmons, together with an engineered dielectric-metallic
environment, to probe the graphene electron liquid and unveil its detailed
electronic response at short wavelengths.The near-field imaging experiments
reveal a parameter-free match with the full theoretical quantum description of
the massless Dirac electron gas, in which we identify three types of quantum
effects as keys to understanding the experimental response of graphene to
short-ranged terahertz electric fields. The first type is of single-particle
nature and is related to shape deformations of the Fermi surface during a
plasmon oscillations. The second and third types are a many-body effect
controlled by the inertia and compressibility of the interacting electron
liquid in graphene. We demonstrate how, in principle, our experimental approach
can determine the full spatiotemporal response of an electron system.
",0,1,0,0,0,0
421,Flows along arch filaments observed in the GRIS 'very fast spectroscopic mode',"  A new generation of solar instruments provides improved spectral, spatial,
and temporal resolution, thus facilitating a better understanding of dynamic
processes on the Sun. High-resolution observations often reveal
multiple-component spectral line profiles, e.g., in the near-infrared He I
10830 \AA\ triplet, which provides information about the chromospheric velocity
and magnetic fine structure. We observed an emerging flux region, including two
small pores and an arch filament system, on 2015 April 17 with the 'very fast
spectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the
1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We
discuss this method of obtaining fast (one per minute) spectral scans of the
solar surface and its potential to follow dynamic processes on the Sun. We
demonstrate the performance of the 'very fast spectroscopic mode' by tracking
chromospheric high-velocity features in the arch filament system.
",0,1,0,0,0,0
422,Rethinking Information Sharing for Actionable Threat Intelligence,"  In the past decade, the information security and threat landscape has grown
significantly making it difficult for a single defender to defend against all
attacks at the same time. This called for introduc- ing information sharing, a
paradigm in which threat indicators are shared in a community of trust to
facilitate defenses. Standards for representation, exchange, and consumption of
indicators are pro- posed in the literature, although various issues are
undermined. In this paper, we rethink information sharing for actionable
intelli- gence, by highlighting various issues that deserve further explo-
ration. We argue that information sharing can benefit from well- defined use
models, threat models, well-understood risk by mea- surement and robust
scoring, well-understood and preserved pri- vacy and quality of indicators and
robust mechanism to avoid free riding behavior of selfish agent. We call for
using the differential nature of data and community structures for optimizing
sharing.
",1,0,0,0,0,0
423,More new classes of permutation trinomials over $\mathbb{F}_{2^n}$,"  Permutation polynomials over finite fields have wide applications in many
areas of science and engineering. In this paper, we present six new classes of
permutation trinomials over $\mathbb{F}_{2^n}$ which have explicit forms by
determining the solutions of some equations.
",0,0,1,0,0,0
424,Distributive Aronszajn trees,"  Ben-David and Shelah proved that if $\lambda$ is a singular strong-limit
cardinal and $2^\lambda=\lambda^+$, then $\square^*_\lambda$ entails the
existence of a normal $\lambda$-distributive $\lambda^+$-Aronszajn tree. Here,
it is proved that the same conclusion remains valid after replacing the
hypothesis $\square^*_\lambda$ by $\square(\lambda^+,{<}\lambda)$.
As $\square(\lambda^+,{<}\lambda)$ does not impose a bound on the order-type
of the witnessing clubs, our construction is necessarily different from that of
Ben-David and Shelah, and instead uses walks on ordinals augmented with club
guessing.
A major component of this work is the study of postprocessing functions and
their effect on square sequences. A byproduct of this study is the finding that
for $\kappa$ regular uncountable, $\square(\kappa)$ entails the existence of a
partition of $\kappa$ into $\kappa$ many fat sets. When contrasted with a
classic model of Magidor, this shows that it is equiconsistent with the
existence of a weakly compact cardinal that $\omega_2$ cannot be split into two
fat sets.
",0,0,1,0,0,0
425,Analytical solutions for the radial Scarf II potential,"  The real Scarf II potential is discussed as a radial problem. This potential
has been studied extensively as a one-dimensional problem, and now these
results are used to construct its bound and resonance solutions for $l=0$ by
setting the origin at some arbitrary value of the coordinate. The solutions
with appropriate boundary conditions are composed as the linear combination of
the two independent solutions of the Schrödinger equation. The asymptotic
expression of these solutions is used to construct the $S_0(k)$ s-wave
$S$-matrix, the poles of which supply the $k$ values corresponding to the
bound, resonance and anti-bound solutions. The location of the discrete energy
eigenvalues is analyzed, and the relation of the solutions of the radial and
one-dimensional Scarf II potentials is discussed. It is shown that the
generalized Woods--Saxon potential can be generated from the Rosen--Morse II
potential in the same way as the radial Scarf II potential is obtained from its
one-dimensional correspondent. Based on this analogy, possible applications are
also pointed out.
",0,1,0,0,0,0
426,Gated Multimodal Units for Information Fusion,"  This paper presents a novel model for multimodal learning based on gated
neural networks. The Gated Multimodal Unit (GMU) model is intended to be used
as an internal unit in a neural network architecture whose purpose is to find
an intermediate representation based on a combination of data from different
modalities. The GMU learns to decide how modalities influence the activation of
the unit using multiplicative gates. It was evaluated on a multilabel scenario
for genre classification of movies using the plot and the poster. The GMU
improved the macro f-score performance of single-modality approaches and
outperformed other fusion strategies, including mixture of experts models.
Along with this work, the MM-IMDb dataset is released which, to the best of our
knowledge, is the largest publicly available multimodal dataset for genre
prediction on movies.
",0,0,0,1,0,0
427,Why Condorcet Consistency is Essential,"  In a single winner election with several candidates and ranked choice or
rating scale ballots, a Condorcet winner is one who wins all their two way
races by majority rule or MR. A voting system has Condorcet consistency or CC
if it names any Condorcet winner the winner. Many voting systems lack CC, but a
three step line of reasoning is used here to show why it is necessary. In step
1 we show that we can dismiss all the electoral criteria which conflict with
CC. In step 2 we point out that CC follows almost automatically if we can agree
that MR is the only acceptable system for elections with two candidates. In
step 3 we make that argument for MR. This argument itself has three parts.
First, in races with two candidates, the only well known alternatives to MR can
sometimes name as winner a candidate who is preferred over their opponent by
only one voter, with all others preferring the opponent. That is unacceptable.
Second, those same systems are also extremely susceptible to strategic
insincere voting. Third, in simulation studies using spatial models with two
candidates, the best known alternative to MR picks the best or most centrist
candidate significantly less often than MR does.
",0,0,0,1,0,0
428,Birefringence induced by pp-wave modes in an electromagnetically active dynamic aether,"  In the framework of the Einstein-Maxwell-aether theory we study the
birefringence effect, which can occur in the pp-wave symmetric dynamic aether.
The dynamic aether is considered to be latently birefringent quasi-medium,
which displays this hidden property if and only if the aether motion is
non-uniform, i.e., when the aether flow is characterized by the non-vanishing
expansion, shear, vorticity or acceleration. In accordance with the
dynamo-optical scheme of description of the interaction between electromagnetic
waves and the dynamic aether, we shall model the susceptibility tensors by the
terms linear in the covariant derivative of the aether velocity four-vector.
When the pp-wave modes appear in the dynamic aether, we deal with a
gravitationally induced degeneracy removal with respect to hidden
susceptibility parameters. As a consequence, the phase velocities of
electromagnetic waves possessing orthogonal polarizations do not coincide, thus
displaying the birefringence effect. Two electromagnetic field configurations
are studied in detail: longitudinal and transversal with respect to the aether
pp-wave front. For both cases the solutions are found, which reveal anomalies
in the electromagnetic response on the action of the pp-wave aether mode.
",0,1,0,0,0,0
429,On generalizations of $p$-sets and their applications,"  The $p$-set, which is in a simple analytic form, is well distributed in unit
cubes. The well-known Weil's exponential sum theorem presents an upper bound of
the exponential sum over the $p$-set. Based on the result, one shows that the
$p$-set performs well in numerical integration, in compressed sensing as well
as in UQ. However, $p$-set is somewhat rigid since the cardinality of the
$p$-set is a prime $p$ and the set only depends on the prime number $p$. The
purpose of this paper is to present generalizations of $p$-sets, say
$\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$, which is more flexible.
Particularly, when a prime number $p$ is given, we have many different choices
of the new $p$-sets. Under the assumption that Goldbach conjecture holds, for
any even number $m$, we present a point set, say ${\mathcal L}_{p,q}$, with
cardinality $m-1$ by combining two different new $p$-sets, which overcomes a
major bottleneck of the $p$-set. We also present the upper bounds of the
exponential sums over $\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$ and ${\mathcal
L}_{p,q}$, which imply these sets have many potential applications.
",1,0,1,0,0,0
430,Robot human interface for housekepeer with wireless capabilities,"  This paper presents the design and implementation of a Human Interface for a
housekeeper robot. It bases on the idea of making the robot understand the
human needs without making the human go through the details of robots work, for
example, the way that the robot implements the work or the method that the
robot uses to plan the path in order to reach the work area. The interface
commands based on idioms of the natural human language and designed in a manner
that the user gives the robot several commands with their execution date/time.
",1,0,0,0,0,0
431,Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers,"  This work proposes a new algorithm for training a re-weighted L2 Support
Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès
et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In
particular, the margin required for each training vector is set independently,
defining a new weighted SVM model. These weights are selected to be binary, and
they are automatically adapted during the training of the model, resulting in a
variation of the Frank-Wolfe optimization algorithm with essentially the same
computational complexity as the original algorithm. As shown experimentally,
this algorithm is computationally cheaper to apply since it requires less
iterations to converge, and it produces models with a sparser representation in
terms of support vectors and which are more stable with respect to the
selection of the regularization hyper-parameter.
",1,0,0,1,0,0
432,Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation,"  Learning-based approaches to robotic manipulation are limited by the
scalability of data collection and accessibility of labels. In this paper, we
present a multi-task domain adaptation framework for instance grasping in
cluttered scenes by utilizing simulated robot experiments. Our neural network
takes monocular RGB images and the instance segmentation mask of a specified
target object as inputs, and predicts the probability of successfully grasping
the specified object for each candidate motor command. The proposed transfer
learning framework trains a model for instance grasping in simulation and uses
a domain-adversarial loss to transfer the trained model to real robots using
indiscriminate grasping data, which is available both in simulation and the
real world. We evaluate our model in real-world robot experiments, comparing it
with alternative model architectures as well as an indiscriminate grasping
baseline.
",1,0,0,0,0,0
433,Bounded Depth Ascending HNN Extensions and $π_1$-Semistability at $\infty$,"  A 1-ended finitely presented group has semistable fundamental group at
$\infty$ if it acts geometrically on some (equivalently any) simply connected
and locally finite complex $X$ with the property that any two proper rays in
$X$ are properly homotopic. If $G$ has semistable fundamental group at $\infty$
then one can unambiguously define the fundamental group at $\infty$ for $G$.
The problem, asking if all finitely presented groups have semistable
fundamental group at $\infty$ has been studied for over 40 years. If $G$ is an
ascending HNN extension of a finitely presented group then indeed, $G$ has
semistable fundamental group at $\infty$, but since the early 1980's it has
been suggested that the finitely presented groups that are ascending HNN
extensions of {\it finitely generated} groups may include a group with
non-semistable fundamental group at $\infty$. Ascending HNN extensions
naturally break into two classes, those with bounded depth and those with
unbounded depth. Our main theorem shows that bounded depth finitely presented
ascending HNN extensions of finitely generated groups have semistable
fundamental group at $\infty$. Semistability is equivalent to two weaker
asymptotic conditions on the group holding simultaneously. We show one of these
conditions holds for all ascending HNN extensions, regardless of depth. We give
a technique for constructing ascending HNN extensions with unbounded depth.
This work focuses attention on a class of groups that may contain a group with
non-semistable fundamental group at $\infty$.
",0,0,1,0,0,0
434,AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles,"  Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.
",1,0,0,0,0,0
435,Hausdorff dimensions in $p$-adic analytic groups,"  Let $G$ be a finitely generated pro-$p$ group, equipped with the $p$-power
series. The associated metric and Hausdorff dimension function give rise to the
Hausdorff spectrum, which consists of the Hausdorff dimensions of closed
subgroups of $G$. In the case where $G$ is $p$-adic analytic, the Hausdorff
dimension function is well understood; in particular, the Hausdorff spectrum
consists of finitely many rational numbers closely linked to the analytic
dimensions of subgroups of $G$.
Conversely, it is a long-standing open question whether the finiteness of the
Hausdorff spectrum implies that $G$ is $p$-adic analytic. We prove that the
answer is yes, in a strong sense, under the extra condition that $G$ is
soluble.
Furthermore, we explore the problem and related questions also for other
filtration series, such as the lower $p$-series, the Frattini series, the
modular dimension subgroup series and quite general filtration series. For
instance, we prove, for odd primes $p$, that every countably based pro-$p$
group $G$ with an open subgroup mapping onto 2 copies of the $p$-adic integers
admits a filtration series such that the corresponding Hausdorff spectrum
contains an infinite real interval.
",0,0,1,0,0,0
436,Real-time brain machine interaction via social robot gesture control,"  Brain-Machine Interaction (BMI) system motivates interesting and promising
results in forward/feedback control consistent with human intention. It holds
great promise for advancements in patient care and applications to
neurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic
platform using a personalized social robot in order to assist patients having
cognitive deficits through bilateral rehabilitation and mental training. For
initial testing of the platform, electroencephalography (EEG) brainwaves of a
human user were collected in real time during tasks of imaginary movements.
First, the brainwaves associated with imagined body kinematics parameters were
decoded to control a cursor on a computer screen in training protocol. Then,
the experienced subject was able to interact with a social robot via our
real-time BMI robotic platform. Corresponding to subject's imagery performance,
he/she received specific gesture movements and eye color changes as
neural-based feedback from the robot. This hands-free neurofeedback interaction
not only can be used for mind control of a social robot's movements, but also
sets the stage for application to enhancing and recovering mental abilities
such as attention via training in humans by providing real-time neurofeedback
from a social robot.
",1,0,0,0,0,0
437,City-Scale Road Audit System using Deep Learning,"  Road networks in cities are massive and is a critical component of mobility.
Fast response to defects, that can occur not only due to regular wear and tear
but also because of extreme events like storms, is essential. Hence there is a
need for an automated system that is quick, scalable and cost-effective for
gathering information about defects. We propose a system for city-scale road
audit, using some of the most recent developments in deep learning and semantic
segmentation. For building and benchmarking the system, we curated a dataset
which has annotations required for road defects. However, many of the labels
required for road audit have high ambiguity which we overcome by proposing a
label hierarchy. We also propose a multi-step deep learning model that segments
the road, subdivide the road further into defects, tags the frame for each
defect and finally localizes the defects on a map gathered using GPS. We
analyze and evaluate the models on image tagging as well as segmentation at
different levels of the label hierarchy.
",1,0,0,0,0,0
438,Mass and moment of inertia govern the transition in the dynamics and wakes of freely rising and falling cylinders,"  In this Letter, we study the motion and wake-patterns of freely rising and
falling cylinders in quiescent fluid. We show that the amplitude of oscillation
and the overall system-dynamics are intricately linked to two parameters: the
particle's mass-density relative to the fluid $m^* \equiv \rho_p/\rho_f$ and
its relative moment-of-inertia $I^* \equiv {I}_p/{I}_f$. This supersedes the
current understanding that a critical mass density ($m^*\approx$ 0.54) alone
triggers the sudden onset of vigorous vibrations. Using over 144 combinations
of ${m}^*$ and $I^*$, we comprehensively map out the parameter space covering
very heavy ($m^* > 10$) to very buoyant ($m^* < 0.1$) particles. The entire
data collapses into two scaling regimes demarcated by a transitional Strouhal
number, $St_t \approx 0.17$. $St_t$ separates a mass-dominated regime from a
regime dominated by the particle's moment of inertia. A shift from one regime
to the other also marks a gradual transition in the wake-shedding pattern: from
the classical $2S$~(2-Single) vortex mode to a $2P$~(2-Pairs) vortex mode.
Thus, auto-rotation can have a significant influence on the trajectories and
wakes of freely rising isotropic bodies.
",0,1,0,0,0,0
439,It Takes Two to Tango: Towards Theory of AI's Mind,"  Theory of Mind is the ability to attribute mental states (beliefs, intents,
knowledge, perspectives, etc.) to others and recognize that these mental states
may differ from one's own. Theory of Mind is critical to effective
communication and to teams demonstrating higher collective performance. To
effectively leverage the progress in Artificial Intelligence (AI) to make our
lives more productive, it is important for humans and AI to work well together
in a team. Traditionally, there has been much emphasis on research to make AI
more accurate, and (to a lesser extent) on having it better understand human
intentions, tendencies, beliefs, and contexts. The latter involves making AI
more human-like and having it develop a theory of our minds. In this work, we
argue that for human-AI teams to be effective, humans must also develop a
theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,
and quirks. We instantiate these ideas within the domain of Visual Question
Answering (VQA). We find that using just a few examples (50), lay people can be
trained to better predict responses and oncoming failures of a complex VQA
model. We further evaluate the role existing explanation (or interpretability)
modalities play in helping humans build ToAIM. Explainable AI has received
considerable scientific and popular attention in recent times. Surprisingly, we
find that having access to the model's internal states - its confidence in its
top-k predictions, explicit or implicit attention maps which highlight regions
in the image (and words in the question) the model is looking at (and listening
to) while answering a question about an image - do not help people better
predict its behavior.
",1,0,0,0,0,0
440,"On variation of dynamical canonical heights, and Intersection numbers","  We study families of varieties endowed with polarized canonical eigensystems
of several maps, inducing canonical heights on the dominating variety as well
as on the ""good"" fibers of the family. We show explicitely the dependence on
the parameter for global and local canonical heights defined by Kawaguchi when
the fibers change, extending previous works of J. Silverman and others.
Finally, fixing an absolute value $v \in K$ and a variety $V/K$, we descript
the Kawaguchi`s canonical local height $\hat{\lambda}_{V,E,\mathcal{Q},}(.,v)$
as an intersection number, provided that the polarized system $(V,\mathcal{Q})$
has a certain weak Néron model over Spec$(\mathcal{O}_v)$ to be defined and
under some conditions depending on the special fiber. With this we extend
Néron's work strengthening Silverman's results, which were for systems
having only one map.
",0,0,1,0,0,0
441,Enhancing the Spectral Hardening of Cosmic TeV Photons by Mixing with Axionlike Particles in the Magnetized Cosmic Web,"  Large-scale extragalactic magnetic fields may induce conversions between
very-high-energy photons and axionlike particles (ALPs), thereby shielding the
photons from absorption on the extragalactic background light. However, in
simplified ""cell"" models, used so far to represent extragalactic magnetic
fields, this mechanism would be strongly suppressed by current astrophysical
bounds. Here we consider a recent model of extragalactic magnetic fields
obtained from large-scale cosmological simulations. Such simulated magnetic
fields would have large enhancement in the filaments of matter. As a result,
photon-ALP conversions would produce a significant spectral hardening for
cosmic TeV photons. This effect would be probed with the upcoming Cherenkov
Telescope Array detector. This possible detection would give a unique chance to
perform a tomography of the magnetized cosmic web with ALPs.
",0,1,0,0,0,0
442,Forecasting in the light of Big Data,"  Predicting the future state of a system has always been a natural motivation
for science and practical applications. Such a topic, beyond its obvious
technical and societal relevance, is also interesting from a conceptual point
of view. This owes to the fact that forecasting lends itself to two equally
radical, yet opposite methodologies. A reductionist one, based on the first
principles, and the naive inductivist one, based only on data. This latter view
has recently gained some attention in response to the availability of
unprecedented amounts of data and increasingly sophisticated algorithmic
analytic techniques. The purpose of this note is to assess critically the role
of big data in reshaping the key aspects of forecasting and in particular the
claim that bigger data leads to better predictions. Drawing on the
representative example of weather forecasts we argue that this is not generally
the case. We conclude by suggesting that a clever and context-dependent
compromise between modelling and quantitative analysis stands out as the best
forecasting strategy, as anticipated nearly a century ago by Richardson and von
Neumann.
",0,1,0,0,0,0
443,Adelic point groups of elliptic curves,"  We show that for an elliptic curve E defined over a number field K, the group
E(A) of points of E over the adele ring A of K is a topological group that can
be analyzed in terms of the Galois representation associated to the torsion
points of E. An explicit description of E(A) is given, and we prove that for K
of degree n, almost all elliptic curves over K have an adelic point group
topologically isomorphic to a universal group depending on n. We also show that
there exist infinitely many elliptic curves over K having a different adelic
point group.
",0,0,1,0,0,0
444,Position Aided Beam Alignment for Millimeter Wave Backhaul Systems with Large Phased Arrays,"  Wireless backhaul communication has been recently realized with large
antennas operating in the millimeter wave (mmWave) frequency band and
implementing highly directional beamforming. In this paper, we focus on the
alignment problem of narrow beams between fixed position network nodes in
mmWave backhaul systems that are subject to small displacements due to wind
flow or ground vibration. We consider nodes equipped with antenna arrays that
are capable of performing only analog processing and communicate through
wireless channels including a line-of-sight component. Aiming at minimizing the
time needed to achieve beam alignment, we present an efficient method that
capitalizes on the exchange of position information between the nodes to design
their beamforming and combining vectors. Some numerical results on the outage
probability with the proposed beam alignment method offer useful preliminary
insights on the impact of some system and operation parameters.
",1,0,1,0,0,0
445,Deep & Cross Network for Ad Click Predictions,"  Feature engineering has been the key to the success of many prediction
models. However, the process is non-trivial and often requires manual feature
engineering or exhaustive searching. DNNs are able to automatically learn
feature interactions; however, they generate all the interactions implicitly,
and are not necessarily efficient in learning all types of cross features. In
this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits
of a DNN model, and beyond that, it introduces a novel cross network that is
more efficient in learning certain bounded-degree feature interactions. In
particular, DCN explicitly applies feature crossing at each layer, requires no
manual feature engineering, and adds negligible extra complexity to the DNN
model. Our experimental results have demonstrated its superiority over the
state-of-art algorithms on the CTR prediction dataset and dense classification
dataset, in terms of both model accuracy and memory usage.
",1,0,0,1,0,0
446,Fan-type spin structure in uni-axial chiral magnets,"  We investigate the spin structure of a uni-axial chiral magnet near the
transition temperatures in low fields perpendicular to the helical axis. We
find a fan-type modulation structure where the clockwise and counterclockwise
windings appear alternatively along the propagation direction of the modulation
structure. This structure is often realized in a Yoshimori-type (non-chiral)
helimagnet but it is rarely realized in a chiral helimagnet. To discuss
underlying physics of this structure, we reconsider the phase diagram (phase
boundary and crossover lines) through the free energy and asymptotic behaviors
of isolated solitons. The fan structure appears slightly below the phase
boundary of the continuous transition of instability-type. In this region,
there are no solutions containing any types of isolated solitons to the mean
field equations.
",0,1,0,0,0,0
447,Rotation of a synchronous viscoelastic shell,"  Several natural satellites of the giant planets have shown evidence of a
global internal ocean, coated by a thin, icy crust. This crust is probably
viscoelastic, which would alter its rotational response. This response would
translate into several rotational quantities, i.e. the obliquity, and the
librations at different frequencies, for which the crustal elasticity reacts
differently. This study aims at modelling the global response of the
viscoelastic crust. For that, I derive the time-dependency of the tensor of
inertia, which I combine with the time evolution of the rotational quantities,
thanks to an iterative algorithm. This algorithm combines numerical simulations
of the rotation with a digital filtering of the resulting tensor of inertia.
The algorithm works very well in the elastic case, provided the problem is not
resonant. However, considering tidal dissipation adds different phase lags to
the oscillating contributions, which challenge the convergence of the
algorithm.
",0,1,0,0,0,0
448,Direct estimation of density functionals using a polynomial basis,"  A number of fundamental quantities in statistical signal processing and
information theory can be expressed as integral functions of two probability
density functions. Such quantities are called density functionals as they map
density functions onto the real line. For example, information divergence
functions measure the dissimilarity between two probability density functions
and are useful in a number of applications. Typically, estimating these
quantities requires complete knowledge of the underlying distribution followed
by multi-dimensional integration. Existing methods make parametric assumptions
about the data distribution or use non-parametric density estimation followed
by high-dimensional integration. In this paper, we propose a new alternative.
We introduce the concept of ""data-driven basis functions"" - functions of
distributions whose value we can estimate given only samples from the
underlying distributions without requiring distribution fitting or direct
integration. We derive a new data-driven complete basis that is similar to the
deterministic Bernstein polynomial basis and develop two methods for performing
basis expansions of functionals of two distributions. We also show that the new
basis set allows us to approximate functions of distributions as closely as
desired. Finally, we evaluate the methodology by developing data driven
estimators for the Kullback-Leibler divergences and the Hellinger distance and
by constructing empirical estimates of tight bounds on the Bayes error rate.
",1,0,0,1,0,0
449,Experimental Evidence on a Refined Conjecture of the BSD type,"  Let $E/\mathbb{Q}$ be an elliptic curve of level $N$ and rank equal to $1$.
Let $p$ be a prime of ordinary reduction. We experimentally study conjecture
$4$ of B. Mazur and J. Tate in his article ""Refined Conjectures of the Birch
and Swinnerton-Dyer Type"". We report the computational evidence.
",0,0,1,0,0,0
450,The Tu--Deng Conjecture holds almost surely,"  The Tu--Deng Conjecture is concerned with the sum of digits $w(n)$ of $n$ in
base~$2$ (the Hamming weight of the binary expansion of $n$) and states the
following: assume that $k$ is a positive integer and $1\leq t<2^k-1$. Then
\[\Bigl \lvert\Bigl\{(a,b)\in\bigl\{0,\ldots,2^k-2\bigr\}^2:a+b\equiv t\bmod
2^k-1, w(a)+w(b)<k\Bigr\}\Bigr \rvert\leq 2^{k-1}.\]
We prove that the Tu--Deng Conjecture holds almost surely in the following
sense: the proportion of $t\in[1,2^k-2]$ such that the above inequality holds
approaches $1$ as $k\rightarrow\infty$.
Moreover, we prove that the Tu--Deng Conjecture implies a conjecture due to
T.~W.~Cusick concerning the sum of digits of $n$ and $n+t$.
",1,0,1,0,0,0
451,Convergence Analysis of the Dynamics of a Special Kind of Two-Layered Neural Networks with $\ell_1$ and $\ell_2$ Regularization,"  In this paper, we made an extension to the convergence analysis of the
dynamics of two-layered bias-free networks with one $ReLU$ output. We took into
consideration two popular regularization terms: the $\ell_1$ and $\ell_2$ norm
of the parameter vector $w$, and added it to the square loss function with
coefficient $\lambda/2$. We proved that when $\lambda$ is small, the weight
vector $w$ converges to the optimal solution $\hat{w}$ (with respect to the new
loss function) with probability $\geq (1-\varepsilon)(1-A_d)/2$ under random
initiations in a sphere centered at the origin, where $\varepsilon$ is a small
value and $A_d$ is a constant. Numerical experiments including phase diagrams
and repeated simulations verified our theory.
",1,0,0,1,0,0
452,"From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter","  We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One
conclusion from the present work is that, except in the extremely low--density
regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with
respect to the unitary limit. Starting from the functional, we introduce
density--dependent scales and show that scales associated to the bare
interaction are strongly renormalized by medium effects. As a consequence, some
of the scales at play around saturation are dominated by the unitary gas
properties and not directly to low-energy constants. For instance, we show that
the scale in the s-wave channel around saturation is proportional to the
so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also
point out that these scales are of the same order of magnitude than those
empirically obtained in the Skyrme energy density functional. We finally
propose a slight modification of the functional such that it becomes accurate
up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$.
",0,1,0,0,0,0
453,EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras (Extended Abstract),"  Marker-based and marker-less optical skeletal motion-capture methods use an
outside-in arrangement of cameras placed around a scene, with viewpoints
converging on the center. They often create discomfort by possibly needed
marker suits, and their recording volume is severely restricted and often
constrained to indoor scenes with controlled backgrounds. We therefore propose
a new method for real-time, marker-less and egocentric motion capture which
estimates the full-body skeleton pose from a lightweight stereo pair of fisheye
cameras that are attached to a helmet or virtual-reality headset. It combines
the strength of a new generative pose estimation framework for fisheye views
with a ConvNet-based body-part detector trained on a new automatically
annotated and augmented dataset. Our inside-in method captures full-body motion
in general indoor and outdoor scenes, and also crowded scenes.
",1,0,0,0,0,0
454,Diffusion Maps meet Nyström,"  Diffusion maps are an emerging data-driven technique for non-linear
dimensionality reduction, which are especially useful for the analysis of
coherent structures and nonlinear embeddings of dynamical systems. However, the
computational complexity of the diffusion maps algorithm scales with the number
of observations. Thus, long time-series data presents a significant challenge
for fast and efficient embedding. We propose integrating the Nyström method
with diffusion maps in order to ease the computational demand. We achieve a
speedup of roughly two to four times when approximating the dominant diffusion
map components.
",0,0,0,1,0,0
455,Multiphase Flows of N Immiscible Incompressible Fluids: An Outflow/Open Boundary Condition and Algorithm,"  We present a set of effective outflow/open boundary conditions and an
associated algorithm for simulating the dynamics of multiphase flows consisting
of $N$ ($N\geqslant 2$) immiscible incompressible fluids in domains involving
outflows or open boundaries. These boundary conditions are devised based on the
properties of energy stability and reduction consistency. The energy stability
property ensures that the contributions of these boundary conditions to the
energy balance will not cause the total energy of the N-phase system to
increase over time. Therefore, these open/outflow boundary conditions are very
effective in overcoming the backflow instability in multiphase systems. The
reduction consistency property ensures that if some fluid components are absent
from the N-phase system then these N-phase boundary conditions will reduce to
those corresponding boundary conditions for the equivalent smaller system. Our
numerical algorithm for the proposed boundary conditions together with the
N-phase governing equations involves only the solution of a set of de-coupled
individual Helmholtz-type equations within each time step, and the resultant
linear algebraic systems after discretization involve only constant and
time-independent coefficient matrices which can be pre-computed. Therefore, the
algorithm is computationally very efficient and attractive. We present
extensive numerical experiments for flow problems involving multiple fluid
components and inflow/outflow boundaries to test the proposed method. In
particular, we compare in detail the simulation results of a three-phase
capillary wave problem with Prosperetti's exact physical solution and
demonstrate that the method developed herein produces physically accurate
results.
",0,1,0,0,0,0
456,Deadly dark matter cusps vs faint and extended star clusters: Eridanus II and Andromeda XXV,"  The recent detection of two faint and extended star clusters in the central
regions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV,
raises the question of whether clusters with such low densities can survive the
tidal field of cold dark matter haloes with central density cusps. Using both
analytic arguments and a suite of collisionless N-body simulations, I show that
these clusters are extremely fragile and quickly disrupted in the presence of
central cusps $\rho\sim r^{-\alpha}$ with $\alpha\gtrsim 0.2$. Furthermore, the
scenario in which the clusters where originally more massive and sank to the
center of the halo requires extreme fine tuning and does not naturally
reproduce the observed systems. In turn, these clusters are long lived in cored
haloes, whose central regions are safe shelters for $\alpha\lesssim 0.2$. The
only viable scenario for hosts that have preserved their primoridal cusp to the
present time is that the clusters formed at rest at the bottom of the
potential, which is easily tested by measurement of the clusters proper
velocity within the host. This offers means to readily probe the central
density profile of two dwarf galaxies as faint as $L_V\sim5\times 10^5 L_\odot$
and $L_V\sim6\times10^4 L_\odot$, in which stellar feedback is unlikely to be
effective.
",0,1,0,0,0,0
457,"Mutual Information, Relative Entropy and Estimation Error in Semi-martingale Channels","  Fundamental relations between information and estimation have been
established in the literature for the continuous-time Gaussian and Poisson
channels, in a long line of work starting from the classical representation
theorems by Duncan and Kabanov respectively. In this work, we demonstrate that
such relations hold for a much larger family of continuous-time channels. We
introduce the family of semi-martingale channels where the channel output is a
semi-martingale stochastic process, and the channel input modulates the
characteristics of the semi-martingale. For these channels, which includes as a
special case the continuous time Gaussian and Poisson models, we establish new
representations relating the mutual information between the channel input and
output to an optimal causal filtering loss, thereby unifying and considerably
extending results from the Gaussian and Poisson settings. Extensions to the
setting of mismatched estimation are also presented where the relative entropy
between the laws governing the output of the channel under two different input
distributions is equal to the cumulative difference between the estimation loss
incurred by using the mismatched and optimal causal filters respectively. The
main tool underlying these results is the Doob--Meyer decomposition of a class
of likelihood ratio sub-martingales. The results in this work can be viewed as
the continuous-time analogues of recent generalizations for relations between
information and estimation for discrete-time Lévy channels.
",1,0,0,0,0,0
458,Testing redMaPPer centring probabilities using galaxy clustering and galaxy-galaxy lensing,"  Galaxy cluster centring is a key issue for precision cosmology studies using
galaxy surveys. Mis-identification of central galaxies causes systematics in
various studies such as cluster lensing, satellite kinematics, and galaxy
clustering. The red-sequence Matched-filter Probabilistic Percolation
(redMaPPer) estimates the probability that each member galaxy is central from
photometric information rather than specifying one central galaxy. The
redMaPPer estimates can be used for calibrating the off-centring effect,
however, the centring algorithm has not previously been well-tested. We test
the centring probabilities of redMaPPer cluster catalog using the projected
cross correlation between redMaPPer clusters with photometric red galaxies and
galaxy-galaxy lensing. We focus on the subsample of redMaPPer clusters in which
the redMaPPer central galaxies (RMCGs) are not the brightest member galaxies
(BMEM) and both of them have spectroscopic redshift. This subsample represents
nearly 10% of the whole cluster sample. We find a clear difference in the
cross-correlation measurements between RMCGs and BMEMs, and the estimated
centring probability is 74$\pm$10% for RMCGs and 13$\pm$4% for BMEMs in the
Gaussian offset model and 78$\pm$9% for RMCGs and 5$\pm$5% for BMEMs in the NFW
offset model. These values are in agreement with the centring probability
values reported by redMaPPer (75% for RMCG and 10% for BMEMs) within 1$\sigma$.
Our analysis provides a strong consistency test of the redMaPPer centring
probabilities. Our results suggest that redMaPPer centring probabilities are
reliably estimated. We confirm that the brightest galaxy in the cluster is not
always the central galaxy as has been shown in previous works.
",0,1,0,0,0,0
459,Criterion of positivity for semilinear problems with applications in biology,"  The goal of this article is to provide an useful criterion of positivity and
well-posedness for a wide range of infinite dimensional semilinear abstract
Cauchy problems. This criterion is based on some weak assumptions on the
non-linear part of the semilinear problem and on the existence of a strongly
continuous semigroup generated by the differential operator. To illustrate a
large variety of applications, we exhibit the feasibility of this criterion
through three examples in mathematical biology: epidemiology, predator-prey
interactions and oncology.
",0,0,1,0,0,0
460,Axiomatic quantum mechanics: Necessity and benefits for the physics studies,"  The ongoing progress in quantum theory emphasizes the crucial role of the
very basic principles of quantum theory. However, this is not properly followed
in teaching quantum mechanics on the graduate and undergraduate levels of
physics studies. The existing textbooks typically avoid the axiomatic
presentation of the theory. We emphasize usefulness of the systematic,
axiomatic approach to the basics of quantum theory as well as its importance in
the light of the modern scientific-research context.
",0,1,0,0,0,0
461,Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs,"  Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.
",0,0,0,0,1,0
462,Shortening binary complexes and commutativity of $K$-theory with infinite products,"  We show that in Grayson's model of higher algebraic $K$-theory using binary
acyclic complexes, the complexes of length two suffice to generate the whole
group. Moreover, we prove that the comparison map from Nenashev's model for
$K_1$ to Grayson's model for $K_1$ is an isomorphism. It follows that algebraic
$K$-theory of exact categories commutes with infinite products.
",0,0,1,0,0,0
463,Cost-Effective Seed Selection in Online Social Networks,"  We study the min-cost seed selection problem in online social networks, where
the goal is to select a set of seed nodes with the minimum total cost such that
the expected number of influenced nodes in the network exceeds a predefined
threshold. We propose several algorithms that outperform the previous studies
both on the theoretical approximation ratios and on the experimental
performance. Under the case where the nodes have heterogeneous costs, our
algorithms are the first bi- criteria approximation algorithms with polynomial
running time and provable logarithmic performance bounds using a general
contagion model. Under the case where the users have uniform costs, our
algorithms achieve logarithmic approximation ratio and provable time complexity
which is smaller than that of existing algorithms in orders of magnitude. We
conduct extensive experiments using real social networks. The experimental
results show that, our algorithms significantly outperform the existing
algorithms both on the total cost and on the running time, and also scale well
to billion-scale networks.
",1,0,0,0,0,0
464,Fast Meta-Learning for Adaptive Hierarchical Classifier Design,"  We propose a new splitting criterion for a meta-learning approach to
multiclass classifier design that adaptively merges the classes into a
tree-structured hierarchy of increasingly difficult binary classification
problems. The classification tree is constructed from empirical estimates of
the Henze-Penrose bounds on the pairwise Bayes misclassification rates that
rank the binary subproblems in terms of difficulty of classification. The
proposed empirical estimates of the Bayes error rate are computed from the
minimal spanning tree (MST) of the samples from each pair of classes. Moreover,
a meta-learning technique is presented for quantifying the one-vs-rest Bayes
error rate for each individual class from a single MST on the entire dataset.
Extensive simulations on benchmark datasets show that the proposed hierarchical
method can often be learned much faster than competing methods, while achieving
competitive accuracy.
",1,0,0,1,0,0
465,Vibrational Density Matrix Renormalization Group,"  Variational approaches for the calculation of vibrational wave functions and
energies are a natural route to obtain highly accurate results with
controllable errors. However, the unfavorable scaling and the resulting high
computational cost of standard variational approaches limit their application
to small molecules with only few vibrational modes. Here, we demonstrate how
the density matrix renormalization group (DMRG) can be exploited to optimize
vibrational wave functions (vDMRG) expressed as matrix product states. We study
the convergence of these calculations with respect to the size of the local
basis of each mode, the number of renormalized block states, and the number of
DMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for
small molecules that were intensively studied in the literature. We then
proceed to show that the complete fingerprint region of the sarcosyn-glycin
dipeptide can be calculated with vDMRG.
",0,1,0,0,0,0
466,Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering,"  In this work, we present a methodology that enables an agent to make
efficient use of its exploratory actions by autonomously identifying possible
objectives in its environment and learning them in parallel. The identification
of objectives is achieved using an online and unsupervised adaptive clustering
algorithm. The identified objectives are learned (at least partially) in
parallel using Q-learning. Using a simulated agent and environment, it is shown
that the converged or partially converged value function weights resulting from
off-policy learning can be used to accumulate knowledge about multiple
objectives without any additional exploration. We claim that the proposed
approach could be useful in scenarios where the objectives are initially
unknown or in real world scenarios where exploration is typically a time and
energy intensive process. The implications and possible extensions of this work
are also briefly discussed.
",1,0,0,0,0,0
467,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,"  Recent studies on diffusion-based sampling methods have shown that Langevin
Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and
rigorous theoretical guarantees have been proven for both asymptotic and
finite-time regimes. Algorithmically, LMC-based algorithms resemble the
well-known gradient descent (GD) algorithm, where the GD recursion is perturbed
by an additive Gaussian noise whose variance has a particular form. Fractional
Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the
Gaussian noise is replaced by a heavy-tailed {\alpha}-stable noise. As opposed
to its Gaussian counterpart, these heavy-tailed perturbations can incur large
jumps and it has been empirically demonstrated that the choice of
{\alpha}-stable noise can provide several advantages in modern machine learning
problems, both in optimization and sampling contexts. However, as opposed to
LMC, only asymptotic convergence properties of FLMC have been yet established.
In this study, we analyze the non-asymptotic behavior of FLMC for non-convex
optimization and prove finite-time bounds for its expected suboptimality. Our
results show that the weak-error of FLMC increases faster than LMC, which
suggests using smaller step-sizes in FLMC. We finally extend our results to the
case where the exact gradients are replaced by stochastic gradients and show
that similar results hold in this setting as well.
",1,0,0,1,0,0
468,Spoken English Intelligibility Remediation with PocketSphinx Alignment and Feature Extraction Improves Substantially over the State of the Art,"  We use automatic speech recognition to assess spoken English learner
pronunciation based on the authentic intelligibility of the learners' spoken
responses determined from support vector machine (SVM) classifier or deep
learning neural network model predictions of transcription correctness. Using
numeric features produced by PocketSphinx alignment mode and many recognition
passes searching for the substitution and deletion of each expected phoneme and
insertion of unexpected phonemes in sequence, the SVM models achieve 82 percent
agreement with the accuracy of Amazon Mechanical Turk crowdworker
transcriptions, up from 75 percent reported by multiple independent
researchers. Using such features with SVM classifier probability prediction
models can help computer-aided pronunciation teaching (CAPT) systems provide
intelligibility remediation.
",1,0,0,1,0,0
469,Second-Order Analysis and Numerical Approximation for Bang-Bang Bilinear Control Problems,"  We consider bilinear optimal control problems, whose objective functionals do
not depend on the controls. Hence, bang-bang solutions will appear. We
investigate sufficient second-order conditions for bang-bang controls, which
guarantee local quadratic growth of the objective functional in $L^1$. In
addition, we prove that for controls that are not bang-bang, no such growth can
be expected. Finally, we study the finite-element discretization, and prove
error estimates of bang-bang controls in $L^1$-norms.
",0,0,1,0,0,0
470,On the letter frequencies and entropy of written Marathi,"  We carry out a comprehensive analysis of letter frequencies in contemporary
written Marathi. We determine sets of letters which statistically predominate
any large generic Marathi text, and use these sets to estimate the entropy of
Marathi.
",1,0,0,0,0,0
471,Robust Orchestration of Concurrent Application Workflows in Mobile Device Clouds,"  A hybrid mobile/fixed device cloud that harnesses sensing, computing,
communication, and storage capabilities of mobile and fixed devices in the
field as well as those of computing and storage servers in remote datacenters
is envisioned. Mobile device clouds can be harnessed to enable innovative
pervasive applications that rely on real-time, in-situ processing of sensor
data collected in the field. To support concurrent mobile applications on the
device cloud, a robust and secure distributed computing framework, called
Maestro, is proposed. The key components of Maestro are (i) a task scheduling
mechanism that employs controlled task replication in addition to task
reallocation for robustness and (ii) Dedup for task deduplication among
concurrent pervasive workflows. An architecture-based solution that relies on
task categorization and authorized access to the categories of tasks is
proposed for different levels of protection. Experimental evaluation through
prototype testbed of Android- and Linux-based mobile devices as well as
simulations is performed to demonstrate Maestro's capabilities.
",1,0,0,0,0,0
472,Anisotropy and multiband superconductivity in Sr2RuO4,"  Despite numerous studies the exact nature of the order parameter in
superconducting Sr2RuO4 remains unresolved. We have extended previous
small-angle neutron scattering studies of the vortex lattice in this material
to a wider field range, higher temperatures, and with the field applied close
to both the <100> and <110> basal plane directions. Measurements at high field
were made possible by the use of both spin polarization and analysis to improve
the signal-to-noise ratio. Rotating the field towards the basal plane causes a
distortion of the square vortex lattice observed for H // <001>, and also a
symmetry change to a distorted triangular symmetry for fields close to <100>.
The vortex lattice distortion allows us to determine the intrinsic
superconducting anisotropy between the c-axis and the Ru-O basal plane,
yielding a value of ~60 at low temperature and low to intermediate fields. This
greatly exceeds the upper critical field anisotropy of ~20 at low temperature,
reminiscent of Pauli limiting. Indirect evidence for Pauli paramagnetic effects
on the unpaired quasiparticles in the vortex cores are observed, but a direct
detection lies below the measurement sensitivity. The superconducting
anisotropy is found to be independent of temperature but increases for fields >
1 T, indicating multiband superconductvity in Sr2RuO4. Finally, the temperature
dependence of the scattered intensity provides further support for gap nodes or
deep minima in the superconducting gap.
",0,1,0,0,0,0
473,"Time-Reversal Breaking in QCD$_4$, Walls, and Dualities in 2+1 Dimensions","  We study $SU(N)$ Quantum Chromodynamics (QCD) in 3+1 dimensions with $N_f$
degenerate fundamental quarks with mass $m$ and a $\theta$-parameter. For
generic $m$ and $\theta$ the theory has a single gapped vacuum. However, as
$\theta$ is varied through $\theta=\pi$ for large $m$ there is a first order
transition. For $N_f=1$ the first order transition line ends at a point with a
massless $\eta'$ particle (for all $N$) and for $N_f>1$ the first order
transition ends at $m=0$, where, depending on the value of $N_f$, the IR theory
has free Nambu-Goldstone bosons, an interacting conformal field theory, or a
free gauge theory. Even when the $4d$ bulk is smooth, domain walls and
interfaces can have interesting phase transitions separating different $3d$
phases. These turn out to be the phases of the recently studied $3d$
Chern-Simons matter theories, thus relating the dynamics of QCD$_4$ and
QCD$_3$, and, in particular, making contact with the recently discussed
dualities in 2+1 dimensions. For example, when the massless $4d$ theory has an
$SU(N_f)$ sigma model, the domain wall theory at low (nonzero) mass supports a
$3d$ massless $CP^{N_f-1}$ nonlinear $\sigma$-model with a Wess-Zumino term, in
agreement with the conjectured dynamics in 2+1 dimensions.
",0,1,0,0,0,0
474,Comparative Investigation of the High Pressure Autoignition of the Butanol Isomers,"  Investigation of the autoignition delay of the butanol isomers has been
performed at elevated pressures of 15 bar and 30 bar and low to intermediate
temperatures of 680-860 K. The reactivity of the stoichiometric isomers of
butanol, in terms of inverse ignition delay, was ranked as n-butanol >
sec-butanol ~ iso-butanol > tert-butanol at a compressed pressure of 15 bar but
changed to n-butanol > tert-butanol > sec-butanol > iso-butanol at 30 bar. For
the temperature and pressure conditions in this study, no NTC or two-stage
ignition behavior were observed. However, for both of the compressed pressures
studied in this work, tert-butanol exhibited unique pre-ignition heat release
characteristics. As such, tert-butanol was further studied at two additional
equivalence ratios ($\phi$ = 0.5 and 2.0) to help determine the cause of the
heat release.
",0,1,0,0,0,0
475,Selecting optimal minimum spanning trees that share a topological correspondence with phylogenetic trees,"  Choi et. al (2011) introduced a minimum spanning tree (MST)-based method
called CLGrouping, for constructing tree-structured probabilistic graphical
models, a statistical framework that is commonly used for inferring
phylogenetic trees. While CLGrouping works correctly if there is a unique MST,
we observe an indeterminacy in the method in the case that there are multiple
MSTs. In this work we remove this indeterminacy by introducing so-called
vertex-ranked MSTs. We note that the effectiveness of CLGrouping is inversely
related to the number of leaves in the MST. This motivates the problem of
finding a vertex-ranked MST with the minimum number of leaves (MLVRMST). We
provide a polynomial time algorithm for the MLVRMST problem, and prove its
correctness for graphs whose edges are weighted with tree-additive distances.
",1,0,1,0,0,0
476,Noisy Natural Gradient as Variational Inference,"  Variational Bayesian neural nets combine the flexibility of deep learning
with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff
between cheap but simple variational families (e.g.~fully factorized) or
expensive and complicated inference procedures. We show that natural gradient
ascent with adaptive weight noise implicitly fits a variational posterior to
maximize the evidence lower bound (ELBO). This insight allows us to train
full-covariance, fully factorized, or matrix-variate Gaussian variational
posteriors using noisy versions of natural gradient, Adam, and K-FAC,
respectively, making it possible to scale up to modern-size ConvNets. On
standard regression benchmarks, our noisy K-FAC algorithm makes better
predictions and matches Hamiltonian Monte Carlo's predictive variances better
than existing methods. Its improved uncertainty estimates lead to more
efficient exploration in active learning, and intrinsic motivation for
reinforcement learning.
",1,0,0,1,0,0
477,A Game of Life on Penrose tilings,"  We define rules for cellular automata played on quasiperiodic tilings of the
plane arising from the multigrid method in such a way that these cellular
automata are isomorphic to Conway's Game of Life. Although these tilings are
nonperiodic, determining the next state of each tile is a local computation,
requiring only knowledge of the local structure of the tiling and the states of
finitely many nearby tiles. As an example, we show a version of a ""glider""
moving through a region of a Penrose tiling. This constitutes a potential
theoretical framework for a method of executing computations in
non-periodically structured substrates such as quasicrystals.
",0,1,1,0,0,0
478,"Single and Multiple Vortex Rings in Three-Dimensional Bose-Einstein Condensates: Existence, Stability and Dynamics","  In the present work, we explore the existence, stability and dynamics of
single and multiple vortex ring states that can arise in Bose-Einstein
condensates. Earlier works have illustrated the bifurcation of such states, in
the vicinity of the linear limit, for isotropic or anisotropic
three-dimensional harmonic traps. Here, we extend these states to the regime of
large chemical potentials, the so-called Thomas-Fermi limit, and explore their
properties such as equilibrium radii and inter-ring distance, for multi-ring
states, as well as their vibrational spectra and possible instabilities. In
this limit, both the existence and stability characteristics can be partially
traced to a particle picture that considers the rings as individual particles
oscillating within the trap and interacting pairwise with one another. Finally,
we examine some representative instability scenarios of the multi-ring dynamics
including breakup and reconnections, as well as the transient formation of
vortex lines.
",0,1,0,0,0,0
479,Dimension-free Wasserstein contraction of nonlinear filters,"  For a class of partially observed diffusions, sufficient conditions are given
for the map from initial condition of the signal to filtering distribution to
be contractive with respect to Wasserstein distances, with rate which has no
dependence on the dimension of the state-space and is stable under tensor
products of the model. The main assumptions are that the signal has affine
drift and constant diffusion coefficient, and that the likelihood functions are
log-concave. Contraction estimates are obtained from an $h$-process
representation of the transition probabilities of the signal reweighted so as
to condition on the observations.
",0,0,1,1,0,0
480,Vortex Nucleation Limited Mobility of Free Electron Bubbles in the Gross-Pitaevskii Model of a Superfluid,"  We study the motion of an electron bubble in the zero temperature limit where
neither phonons nor rotons provide a significant contribution to the drag
exerted on an ion moving within the superfluid. By using the Gross-Clark model,
in which a Gross-Pitaevskii equation for the superfluid wavefunction is coupled
to a Schrödinger equation for the electron wavefunction, we study how
vortex nucleation affects the measured drift velocity of the ion. We use
parameters that give realistic values of the ratio of the radius of the bubble
with respect to the healing length in superfluid $^4$He at a pressure of one
bar. By performing fully 3D spatio-temporal simulations of the superfluid
coupled to an electron, that is modelled within an adiabatic approximation and
moving under the influence of an applied electric field, we are able to recover
the key dynamics of the ion-vortex interactions that arise and the subsequent
ion-vortex complexes that can form. Using the numerically computed drift
velocity of the ion as a function of the applied electric field, we determine
the vortex-nucleation limited mobility of the ion to recover values in
reasonable agreement with measured data.
",0,1,0,0,0,0
481,Radio variability and non-thermal components in stars evolving toward planetary nebulae,"  We present new JVLA multi-frequency measurements of a set of stars in
transition from the post-AGB to the Planetary Nebula phase monitored in the
radio range over several years. Clear variability is found for five sources.
Their light curves show increasing and decreasing patterns. New radio
observations at high angular resolution are also presented for two sources.
Among these is IRAS 18062+2410, whose radio structure is compared to
near-infrared images available in the literature. With these new maps, we can
estimate inner and outer radii of 0.03$""$ and 0.08$""$ for the ionised shell, an
ionised mass of $3.2\times10^{-4}$ M$_\odot$, and a density at the inner radius
of $7.7\times 10^{-5}$ cm$^{-3}$, obtained by modelling the radio shell with
the new morphological constraints. The combination of multi-frequency data and,
where available, spectral-index maps leads to the detection of spectral indices
not due to thermal emission, contrary to what one would expect in planetary
nebulae. Our results allow us to hypothesise the existence of a link between
radio variability and non-thermal emission mechanisms in the nebulae. This link
seems to hold for IRAS 22568+6141 and may generally hold for those nebulae
where the radio flux decreases over time.
",0,1,0,0,0,0
482,Sequential testing for structural stability in approximate factor models,"  We develop an on-line monitoring procedure to detect a change in a large
approximate factor model. Our statistics are based on a well-known property of
the $% \left( r+1\right) $-th eigenvalue of the sample covariance matrix of the
data (having defined $r$ as the number of common factors): whilst under the
null the $\left( r+1\right) $-th eigenvalue is bounded, under the alternative
of a change (either in the loadings, or in the number of factors itself) it
becomes spiked. Given that the sample eigenvalue cannot be estimated
consistently under the null, we regularise the problem by randomising the test
statistic in conjunction with sample conditioning, obtaining a sequence of
\textit{i.i.d.}, asymptotically chi-square statistics which are then employed
to build the monitoring scheme. Numerical evidence shows that our procedure
works very well in finite samples, with a very small probability of false
detections and tight detection times in presence of a genuine change-point.
",0,0,0,1,0,0
483,Susceptibility Propagation by Using Diagonal Consistency,"  A susceptibility propagation that is constructed by combining a belief
propagation and a linear response method is used for approximate computation
for Markov random fields. Herein, we formulate a new, improved susceptibility
propagation by using the concept of a diagonal matching method that is based on
mean-field approaches to inverse Ising problems. The proposed susceptibility
propagation is robust for various network structures, and it is reduced to the
ordinary susceptibility propagation and to the adaptive
Thouless-Anderson-Palmer equation in special cases.
",0,0,1,1,0,0
484,Performance Analysis of Ultra-Dense Networks with Elevated Base Stations,"  This paper analyzes the downlink performance of ultra-dense networks with
elevated base stations (BSs). We consider a general dual-slope pathloss model
with distance-dependent probability of line-of-sight (LOS) transmission between
BSs and receivers. Specifically, we consider the scenario where each link may
be obstructed by randomly placed buildings. Using tools from stochastic
geometry, we show that both coverage probability and area spectral efficiency
decay to zero as the BS density grows large. Interestingly, we show that the BS
height alone has a detrimental effect on the system performance even when the
standard single-slope pathloss model is adopted.
",1,0,0,0,0,0
485,Learning to Drive in a Day,"  We demonstrate the first application of deep reinforcement learning to
autonomous driving. From randomly initialised parameters, our model is able to
learn a policy for lane following in a handful of training episodes using a
single monocular image as input. We provide a general and easy to obtain
reward: the distance travelled by the vehicle without the safety driver taking
control. We use a continuous, model-free deep reinforcement learning algorithm,
with all exploration and optimisation performed on-vehicle. This demonstrates a
new framework for autonomous driving which moves away from reliance on defined
logical rules, mapping, and direct supervision. We discuss the challenges and
opportunities to scale this approach to a broader range of autonomous driving
tasks.
",1,0,0,1,0,0
486,Strong-coupling of WSe2 in ultra-compact plasmonic nanocavities at room temperature,"  Strong-coupling of monolayer metal dichalcogenide semiconductors with light
offers encouraging prospects for realistic exciton devices at room temperature.
However, the nature of this coupling depends extremely sensitively on the
optical confinement and the orientation of electronic dipoles and fields. Here,
we show how plasmon strong coupling can be achieved in compact robust
easily-assembled gold nano-gap resonators at room temperature. We prove that
strong coupling is impossible with monolayers due to the large exciton
coherence size, but resolve clear anti-crossings for 8 layer devices with Rabi
splittings exceeding 135 meV. We show that such structures improve on prospects
for nonlinear exciton functionalities by at least 10^4, while retaining quantum
efficiencies above 50%.
",0,1,0,0,0,0
487,Stigmergy-based modeling to discover urban activity patterns from positioning data,"  Positioning data offer a remarkable source of information to analyze crowds
urban dynamics. However, discovering urban activity patterns from the emergent
behavior of crowds involves complex system modeling. An alternative approach is
to adopt computational techniques belonging to the emergent paradigm, which
enables self-organization of data and allows adaptive analysis. Specifically,
our approach is based on stigmergy. By using stigmergy each sample position is
associated with a digital pheromone deposit, which progressively evaporates and
aggregates with other deposits according to their spatiotemporal proximity.
Based on this principle, we exploit positioning data to identify high density
areas (hotspots) and characterize their activity over time. This
characterization allows the comparison of dynamics occurring in different days,
providing a similarity measure exploitable by clustering techniques. Thus, we
cluster days according to their activity behavior, discovering unexpected urban
activity patterns. As a case study, we analyze taxi traces in New York City
during 2015.
",1,1,0,0,0,0
488,BiHom-Lie colour algebras structures,"  BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with
two commuting multiplicative linear maps. The main purpose of this paper is to
define representations and a cohomology of BiHom-Lie colour algebras and to
study some key constructions and properties.
Moreover, we discuss $\alpha^{k}\beta^l$-generalized derivations,
$\alpha^{k}\beta^l$-quasi-derivations and $\alpha^{k}\beta^l$-quasi-centroid.
We provide some properties and their relationships with BiHom-Jordan colour
algebra.
",0,0,1,0,0,0
489,Clustering of Gamma-Ray bursts through kernel principal component analysis,"  We consider the problem related to clustering of gamma-ray bursts (from
""BATSE"" catalogue) through kernel principal component analysis in which our
proposed kernel outperforms results of other competent kernels in terms of
clustering accuracy and we obtain three physically interpretable groups of
gamma-ray bursts. The effectivity of the suggested kernel in combination with
kernel principal component analysis in revealing natural clusters in noisy and
nonlinear data while reducing the dimension of the data is also explored in two
simulated data sets.
",0,1,0,1,0,0
490,Bounded gaps between primes in short intervals,"  Baker, Harman, and Pintz showed that a weak form of the Prime Number Theorem
holds in intervals of the form $[x-x^{0.525},x]$ for large $x$. In this paper,
we extend a result of Maynard and Tao concerning small gaps between primes to
intervals of this length. More precisely, we prove that for any $\delta\in
[0.525,1]$ there exist positive integers $k,d$ such that for sufficiently large
$x$, the interval $[x-x^\delta,x]$ contains $\gg_{k} \frac{x^\delta}{(\log
x)^k}$ pairs of consecutive primes differing by at most $d$. This confirms a
speculation of Maynard that results on small gaps between primes can be refined
to the setting of short intervals of this length.
",0,0,1,0,0,0
491,Handover analysis of the Improved Phantom Cells,"  Improved Phantom cell is a new scenario which has been introduced recently to
enhance the capacity of Heterogeneous Networks (HetNets). The main trait of
this scenario is that, besides maximizing the total network capacity in both
indoor and outdoor environments, it claims to reduce the handover number
compared to the conventional scenarios. In this paper, by a comprehensive
review of the Improved Phantom cells structure, an appropriate algorithm will
be introduced for the handover procedure of this scenario. To reduce the number
of handover in the proposed algorithm, various parameters such as the received
Signal to Interference plus Noise Ratio (SINR) at the user equipment (UE),
users access conditions to the phantom cells, and users staying time in the
target cell based on its velocity, has been considered. Theoretical analyses
and simulation results show that applying the suggested algorithm the improved
phantom cell structure has a much better performance than conventional HetNets
in terms of the number of handover.
",1,0,0,0,0,0
492,Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation,"  We address the problem of localisation of objects as bounding boxes in images
with weak labels. This weakly supervised object localisation problem has been
tackled in the past using discriminative models where each object class is
localised independently from other classes. We propose a novel framework based
on Bayesian joint topic modelling. Our framework has three distinctive
advantages over previous works: (1) All object classes and image backgrounds
are modelled jointly together in a single generative model so that ""explaining
away"" inference can resolve ambiguity and lead to better learning and
localisation. (2) The Bayesian formulation of the model enables easy
integration of prior knowledge about object appearance to compensate for
limited supervision. (3) Our model can be learned with a mixture of weakly
labelled and unlabelled data, allowing the large volume of unlabelled images on
the Internet to be exploited for learning. Extensive experiments on the
challenging VOC dataset demonstrate that our approach outperforms the
state-of-the-art competitors.
",1,0,0,0,0,0
493,Psychological model of the investor and manager behavior in risk,"  All people have to make risky decisions in everyday life. And we do not know
how true they are. But is it possible to mathematically assess the correctness
of our choice? This article discusses the model of decision making under risk
on the example of project management. This is a game with two players, one of
which is Investor, and the other is the Project Manager. Each player makes a
risky decision for himself, based on his past experience. With the help of a
mathematical model, the players form a level of confidence, depending on who
the player accepts the strategy or does not accept. The project manager
assesses the costs and compares them with the level of confidence. An investor
evaluates past results. Also visit the case where the strategy of the player
accepts the part.
",0,0,0,0,0,1
494,Constraints on Super-Earths Interiors from Stellar Abundances,"  Modeling the interior of exoplanets is essential to go further than the
conclusions provided by mean density measurements. In addition to the still
limited precision on the planets' fundamental parameters, models are limited by
the existence of degeneracies on their compositions. Here we present a model of
internal structure dedicated to the study of solid planets up to ~10 Earth
masses, i.e. Super-Earths. When the measurement is available, the assumption
that the bulk Fe/Si ratio of a planet is similar to that of its host star
allows us to significantly reduce the existing degeneracy and more precisely
constrain the planet's composition. Based on our model, we provide an update of
the mass-radius relationships used to provide a first estimate of a planet's
composition from density measurements. Our model is also applied to the cases
of two well-known exoplanets, CoRoT-7b and Kepler-10b, using their recently
updated parameters. The core mass fractions of CoRoT-7b and Kepler-10b are
found to lie within the 10-37% and 10-33% ranges, respectively, allowing both
planets to be compatible with an Earth-like composition. We also extend the
recent study of Proxima Centauri b, and show that its radius may reach 1.94
Earth radii in the case of a 5 Earth masses planet, as there is a 96.7%
probability that the real mass of Proxima Centauri b is below this value.
",0,1,0,0,0,0
495,Software correlator for Radioastron mission,"  In this paper we discuss the characteristics and operation of Astro Space
Center (ASC) software FX correlator that is an important component of
space-ground interferometer for Radioastron project. This project performs
joint observations of compact radio sources using 10 meter space radio
telescope (SRT) together with ground radio telescopes at 92, 18, 6 and 1.3 cm
wavelengths. In this paper we describe the main features of space-ground VLBI
data processing of Radioastron project using ASC correlator. Quality of
implemented fringe search procedure provides positive results without
significant losses in correlated amplitude. ASC Correlator has a computational
power close to real time operation. The correlator has a number of processing
modes: ""Continuum"", ""Spectral Line"", ""Pulsars"", ""Giant Pulses"",""Coherent"".
Special attention is paid to peculiarities of Radioastron space-ground VLBI
data processing. The algorithms of time delay and delay rate calculation are
also discussed, which is a matter of principle for data correlation of
space-ground interferometers. During 5 years of Radioastron space radio
telescope (SRT) successful operation, ASC correlator showed high potential of
satisfying steady growing needs of current and future ground and space VLBI
science. Results of ASC software correlator operation are demonstrated.
",0,1,0,0,0,0
496,Isogenies for point counting on genus two hyperelliptic curves with maximal real multiplication,"  Schoof's classic algorithm allows point-counting for elliptic curves over
finite fields in polynomial time. This algorithm was subsequently improved by
Atkin, using factorizations of modular polynomials, and by Elkies, using a
theory of explicit isogenies. Moving to Jacobians of genus-2 curves, the
current state of the art for point counting is a generalization of Schoof's
algorithm. While we are currently missing the tools we need to generalize
Elkies' methods to genus 2, recently Martindale and Milio have computed
analogues of modular polynomials for genus-2 curves whose Jacobians have real
multiplication by maximal orders of small discriminant. In this article, we
prove Atkin-style results for genus-2 Jacobians with real multiplication by
maximal orders, with a view to using these new modular polynomials to improve
the practicality of point-counting algorithms for these curves.
",1,0,1,0,0,0
497,On the self-duality of rings of integers in tame and abelian extensions,"  Let $L/K$ be a tame and Galois extension of number fields with group $G$. It
is well-known that any ambiguous ideal in $L$ is locally free over
$\mathcal{O}_KG$ (of rank one), and so it defines a class in the locally free
class group of $\mathcal{O}_KG$, where $\mathcal{O}_K$ denotes the ring of
integers of $K$. In this paper, we shall study the relationship among the
classes arising from the ring of integers $\mathcal{O}_L$ of $L$, the inverse
different $\mathfrak{D}_{L/K}^{-1}$ of $L/K$, and the square root of the
inverse different $A_{L/K}$ of $L/K$ (if it exists), in the case that $G$ is
abelian. They are naturally related because $A_{L/K}^2 =
\mathfrak{D}_{L/K}^{-1} = \mathcal{O}_L^*$, and $A_{L/K}$ is special because
$A_{L/K} = A_{L/K}^*$, where $*$ denotes dual with respect to the trace of
$L/K$.
",0,0,1,0,0,0
498,Forecasting Transformative AI: An Expert Survey,"  Transformative AI technologies have the potential to reshape critical aspects
of society in the near future. However, in order to properly prepare policy
initiatives for the arrival of such technologies accurate forecasts and
timelines are necessary. A survey was administered to attendees of three AI
conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).
The survey included questions for estimating AI capabilities over the next
decade, questions for forecasting five scenarios of transformative AI and
questions concerning the impact of computational resources in AI research.
Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that
humans are currently paid to do) can be feasibly automated now, and that this
figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts
indicated a 50% probability of AI systems being capable of automating 90% of
current human tasks in 25 years and 99% of current human tasks in 50 years. The
conference of attendance was found to have a statistically significant impact
on all forecasts, with attendees of HLAI providing more optimistic timelines
with less uncertainty. These findings suggest that AI experts expect major
advances in AI technology to continue over the next decade to a degree that
will likely have profound transformative impacts on society.
",1,0,0,0,0,0
499,"Change of grading, injective dimension and dualizing complexes","  Let $G,H$ be groups, $\phi: G \rightarrow H$ a group morphism, and $A$ a
$G$-graded algebra. The morphism $\phi$ induces an $H$-grading on $A$, and on
any $G$-graded $A$-module, which thus becomes an $H$-graded $A$-module. Given
an injective $G$-graded $A$-module, we give bounds for its injective dimension
when seen as $H$-graded $A$-module. Following ideas by Van den Bergh, we give
an application of our results to the stability of dualizing complexes through
change of grading.
",0,0,1,0,0,0
500,Approximately certifying the restricted isometry property is hard,"  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts
as an approximate isometry when restricted to sparse vectors. Previous work has
shown it to be NP-hard to determine whether a matrix possess this property, but
only in a narrow range of parameters. In this work, we show that it is NP-hard
to make this determination for any accuracy parameter, even when we restrict
ourselves to instances which are either RIP or far from being RIP. This result
implies that it is NP-hard to approximate the range of parameters for which a
matrix possesses the Restricted Isometry Property with accuracy better than
some constant. Ours is the first work to prove such a claim without any
additional assumptions.
",1,0,0,0,0,0
501,A compact design for velocity-map imaging energetic electrons and ions,"  We present a compact design for a velocity-map imaging spectrometer for
energetic electrons and ions. The standard geometry by Eppink and Parker [A. T.
J. B. Eppink and D. H. Parker, Rev. Sci. Instrum. 68, 3477 (1997)] is augmented
by just two extended electrodes so as to realize an additional einzel lens. In
this way, for a maximum electrode voltage of 7 kV we experimentally demonstrate
imaging of electrons with energies up to 65 eV. Simulations show that energy
acceptances of <270 and <1,200 eV with an energy resolution of dE / E <5% are
achievable for electrode voltages of <20 kV when using diameters of the
position-sensitive detector of 42 and 78 mm, respectively.
",0,1,0,0,0,0
502,Multiset Combinatorial Batch Codes,"  Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai,
mimic a distributed storage of a set of $n$ data items on $m$ servers, in such
a way that any batch of $k$ data items can be retrieved by reading at most some
$t$ symbols from each server. Combinatorial batch codes, are replication-based
batch codes in which each server stores a subset of the data items.
In this paper, we propose a generalization of combinatorial batch codes,
called multiset combinatorial batch codes (MCBC), in which $n$ data items are
stored in $m$ servers, such that any multiset request of $k$ items, where any
item is requested at most $r$ times, can be retrieved by reading at most $t$
items from each server. The setup of this new family of codes is motivated by
recent work on codes which enable high availability and parallel reads in
distributed storage systems. The main problem under this paradigm is to
minimize the number of items stored in the servers, given the values of
$n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and
sufficient condition for the existence of MCBCs. Then, we present several
bounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we
determine the value of $N(n,k,m,1;r)$ for any $n\geq
\left\lfloor\frac{k-1}{r}\right\rfloor{m\choose k-1}-(m-k+1)A(m,4,k-2)$, where
$A(m,4,k-2)$ is the maximum size of a binary constant weight code of length
$m$, distance four and weight $k-2$. We also determine the exact value of
$N(n,k,m,1;r)$ when $r\in\{k,k-1\}$ or $k=m$.
",1,0,1,0,0,0
503,Thermoelectric Cooperative Effect in Three-Terminal Elastic Transport through a Quantum Dot,"  The energy efficiency and power of a three-terminal thermoelectric nanodevice
are studied by considering elastic tunneling through a single quantum dot.
Facilitated by the three-terminal geometry, the nanodevice is able to generate
simultaneously two electrical powers by utilizing only one temperature bias.
These two electrical powers can add up constructively or destructively,
depending on their signs. It is demonstrated that the constructive addition
leads to the enhancement of both energy efficiency and output power for various
system parameters. In fact, such enhancement, dubbed as thermoelectric
cooperative effect, can lead to maximum efficiency and power no less than when
only one of the electrical power is harvested.
",0,1,0,0,0,0
504,DeepSaucer: Unified Environment for Verifying Deep Neural Networks,"  In recent years, a number of methods for verifying DNNs have been developed.
Because the approaches of the methods differ and have their own limitations, we
think that a number of verification methods should be applied to a developed
DNN. To apply a number of methods to the DNN, it is necessary to translate
either the implementation of the DNN or the verification method so that one
runs in the same environment as the other. Since those translations are
time-consuming, a utility tool, named DeepSaucer, which helps to retain and
reuse implementations of DNNs, verification methods, and their environments, is
proposed. In DeepSaucer, code snippets of loading DNNs, running verification
methods, and creating their environments are retained and reused as software
assets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer
is confirmed by implementing it on the basis of Anaconda, which provides
virtual environment for loading a DNN and running a verification method. In
addition, the effectiveness of DeepSaucer is demonstrated by usecase examples.
",1,0,0,0,0,0
505,Checklists to Support Test Charter Design in Exploratory Testing,"  During exploratory testing sessions the tester simultaneously learns, designs
and executes tests. The activity is iterative and utilizes the skills of the
tester and provides flexibility and creativity.Test charters are used as a
vehicle to support the testers during the testing. The aim of this study is to
support practitioners in the design of test charters through checklists. We
aimed to identify factors allowing practitioners to critically reflect on their
designs and contents of test charters to support practitioners in making
informed decisions of what to include in test charters. The factors and
contents have been elicited through interviews. Overall, 30 factors and 35
content elements have been elicited.
",1,0,0,0,0,0
506,Fast non-destructive parallel readout of neutral atom registers in optical potentials,"  We demonstrate the parallel and non-destructive readout of the hyperfine
state for optically trapped $^{87}$Rb atoms. The scheme is based on
state-selective fluorescence imaging and achieves detection fidelities $>$98%
within 10$\,$ms, while keeping 99% of the atoms trapped. For the read-out of
dense arrays of neutral atoms in optical lattices, where the fluorescence
images of neighboring atoms overlap, we apply a novel image analysis technique
using Bayesian inference to determine the internal state of multiple atoms. Our
method is scalable to large neutral atom registers relevant for future quantum
information processing tasks requiring fast and non-destructive readout and can
also be used for the simultaneous read-out of quantum information stored in
internal qubit states and in the atoms' positions.
",0,1,0,0,0,0
507,Binaural Source Localization based on Modulation-Domain Features and Decision Pooling,"  In this work we apply Amplitude Modulation Spectrum (AMS) features to the
source localization problem. Our approach computes 36 bilateral features for 2s
long signal segments and estimates the azimuthal directions of a sound source
through a binaurally trained classifier. This directional information of a
sound source could be e.g. used to steer the beamformer in a hearing aid to the
source of interest in order to increase the SNR. We evaluated our approach on
the development set of the IEEE-AASP Challenge on sound source localization and
tracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline
approach. Additionally, our approach is computationally less complex.
",1,0,0,0,0,0
508,Dynamic Shrinkage Processes,"  We propose a novel class of dynamic shrinkage processes for Bayesian time
series and regression analysis. Building upon a global-local framework of prior
construction, in which continuous scale mixtures of Gaussian distributions are
employed for both desirable shrinkage properties and computational
tractability, we model dependence among the local scale parameters. The
resulting processes inherit the desirable shrinkage behavior of popular
global-local priors, such as the horseshoe prior, but provide additional
localized adaptivity, which is important for modeling time series data or
regression functions with local features. We construct a computationally
efficient Gibbs sampling algorithm based on a Pólya-Gamma scale mixture
representation of the proposed process. Using dynamic shrinkage processes, we
develop a Bayesian trend filtering model that produces more accurate estimates
and tighter posterior credible intervals than competing methods, and apply the
model for irregular curve-fitting of minute-by-minute Twitter CPU usage data.
In addition, we develop an adaptive time-varying parameter regression model to
assess the efficacy of the Fama-French five-factor asset pricing model with
momentum added as a sixth factor. Our dynamic analysis of manufacturing and
healthcare industry data shows that with the exception of the market risk, no
other risk factors are significant except for brief periods.
",0,0,0,1,0,0
509,A Multiple Source Framework for the Identification of Activities of Daily Living Based on Mobile Device Data,"  The monitoring of the lifestyles may be performed based on a system for the
recognition of Activities of Daily Living (ADL) and their environments,
combining the results obtained with the user agenda. The system may be
developed with the use of the off-the-shelf mobile devices commonly used,
because they have several types of sensors available, including motion,
magnetic, acoustic, and location sensors. Data acquisition, data processing,
data fusion, and artificial intelligence methods are applied in different
stages of the system developed, which recognizes the ADL with pattern
recognition methods. The motion and magnetic sensors allow the recognition of
activities with movement, but the acoustic sensors allow the recognition of the
environments. The fusion of the motion, magnetic and acoustic sensors allows
the differentiation of other ADL. On the other hand, the location sensors
allows the recognition of ADL with large movement, and the combination of these
sensors with the other sensors increases the number of ADL recognized by the
system. This study consists on the comparison of different types of ANN for
choosing the best methods for the recognition of several ADL, which they are
implemented in a system for the recognition of ADL that combines the sensors
data with the users agenda for the monitoring of the lifestyles. Conclusions
point to the use of Deep Neural Networks (DNN) with normalized data for the
identification of ADL with 85.89% of accuracy, the use of Feedforward neural
networks with non-normalized data for the identification of the environments
with 86.50% of accuracy, and the use of DNN with normalized data for the
identification of standing activities with 100% of accuracy, proving the
reliability of the framework presented in this study.
",1,0,0,0,0,0
510,Maximum likelihood estimators based on the block maxima method,"  The extreme value index is a fundamental parameter in univariate Extreme
Value Theory (EVT). It captures the tail behavior of a distribution and is
central in the extrapolation beyond observed data. Among other semi-parametric
methods (such as the popular Hill's estimator), the Block Maxima (BM) and
Peaks-Over-Threshold (POT) methods are widely used for assessing the extreme
value index and related normalizing constants. We provide asymptotic theory for
the maximum likelihood estimators (MLE) based on the BM method. Our main result
is the asymptotic normality of the MLE with a non-trivial bias depending on the
extreme value index and on the so-called second order parameter. Our approach
combines asymptotic expansions of the likelihood process and of the empirical
quantile process of block maxima. The results permit to complete the comparison
of most common semi-parametric estimators in EVT (MLE and probability weighted
moment estimators based on the POT or BM methods) through their asymptotic
variances, biases and optimal mean square errors.
",0,0,1,1,0,0
511,Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority,"  The spread of opinions, memes, diseases, and ""alternative facts"" in a
population depends both on the details of the spreading process and on the
structure of the social and communication networks on which they spread. In
this paper, we explore how \textit{anti-establishment} nodes (e.g.,
\textit{hipsters}) influence the spreading dynamics of two competing products.
We consider a model in which spreading follows a deterministic rule for
updating node states (which describe which product has been adopted) in which
an adjustable fraction $p_{\rm Hip}$ of the nodes in a network are hipsters,
who choose to adopt the product that they believe is the less popular of the
two. The remaining nodes are conformists, who choose which product to adopt by
considering which products their immediate neighbors have adopted. We simulate
our model on both synthetic and real networks, and we show that the hipsters
have a major effect on the final fraction of people who adopt each product:
even when only one of the two products exists at the beginning of the
simulations, a very small fraction of hipsters in a network can still cause the
other product to eventually become the more popular one. To account for this
behavior, we construct an approximation for the steady-state adoption fraction
on $k$-regular trees in the limit of few hipsters. Additionally, our
simulations demonstrate that a time delay $\tau$ in the knowledge of the
product distribution in a population, as compared to immediate knowledge of
product adoption among nearest neighbors, can have a large effect on the final
distribution of product adoptions. Our simple model and analysis may help shed
light on the road to success for anti-establishment choices in elections, as
such success can arise rather generically in our model from a small number of
anti-establishment individuals and ordinary processes of social influence on
normal individuals.
",1,1,0,0,0,0
512,Optimal Identity Testing with High Probability,"  We study the problem of testing identity against a given distribution with a
focus on the high confidence regime. More precisely, given samples from an
unknown distribution $p$ over $n$ elements, an explicitly given distribution
$q$, and parameters $0< \epsilon, \delta < 1$, we wish to distinguish, {\em
with probability at least $1-\delta$}, whether the distributions are identical
versus $\varepsilon$-far in total variation distance. Most prior work focused
on the case that $\delta = \Omega(1)$, for which the sample complexity of
identity testing is known to be $\Theta(\sqrt{n}/\epsilon^2)$. Given such an
algorithm, one can achieve arbitrarily small values of $\delta$ via black-box
amplification, which multiplies the required number of samples by
$\Theta(\log(1/\delta))$.
We show that black-box amplification is suboptimal for any $\delta = o(1)$,
and give a new identity tester that achieves the optimal sample complexity. Our
new upper and lower bounds show that the optimal sample complexity of identity
testing is \[
\Theta\left( \frac{1}{\epsilon^2}\left(\sqrt{n \log(1/\delta)} +
\log(1/\delta) \right)\right) \] for any $n, \varepsilon$, and $\delta$. For
the special case of uniformity testing, where the given distribution is the
uniform distribution $U_n$ over the domain, our new tester is surprisingly
simple: to test whether $p = U_n$ versus $d_{\mathrm TV}(p, U_n) \geq
\varepsilon$, we simply threshold $d_{\mathrm TV}(\widehat{p}, U_n)$, where
$\widehat{p}$ is the empirical probability distribution. The fact that this
simple ""plug-in"" estimator is sample-optimal is surprising, even in the
constant $\delta$ case. Indeed, it was believed that such a tester would not
attain sublinear sample complexity even for constant values of $\varepsilon$
and $\delta$.
",1,0,1,1,0,0
513,Handling state space explosion in verification of component-based systems: A review,"  Component-based design is a different way of constructing systems which
offers numerous benefits, in particular, decreasing the complexity of system
design. However, deploying components into a system is a challenging and
error-prone task. Model checking is one of the reliable methods that
automatically and systematically analyse the correctness of a given system. Its
brute-force check of the state space significantly expands the level of
confidence in the system. Nevertheless, model checking is limited by a critical
problem so-called State Space Explosion (SSE). To benefit from model checking,
appropriate methods to reduce SSE, is required. In two last decades, a great
number of methods to mitigate the state space explosion have been proposed
which have many similarities, dissimilarities, and unclear concepts in some
cases. This research, firstly, aims at present a review and brief discussion of
the methods of handling SSE problem and classify them based on their
similarities, principle and characteristics. Second, it investigates the
methods for handling SSE problem in verifying Component-based system (CBS) and
provides insight into CBS verification limitations that have not been addressed
yet. The analysis in this research has revealed the patterns, specific
features, and gaps in the state-of-the-art methods. In addition, we identified
and discussed suitable methods to soften SSE problem in CBS and underlined the
key challenges for future research efforts.
",1,0,1,0,0,0
514,A Framework for Relating the Structures and Recovery Statistics in Pressure Time-Series Surveys for Dust Devils,"  Dust devils are likely the dominant source of dust for the martian
atmosphere, but the amount and frequency of dust-lifting depend on the
statistical distribution of dust devil parameters. Dust devils exhibit pressure
perturbations and, if they pass near a barometric sensor, they may register as
a discernible dip in a pressure time-series. Leveraging this fact, several
surveys using barometric sensors on landed spacecraft have revealed dust devil
structures and occurrence rates. However powerful they are, though, such
surveys suffer from non-trivial biases that skew the inferred dust devil
properties. For example, such surveys are most sensitive to dust devils with
the widest and deepest pressure profiles, but the recovered profiles will be
distorted, broader and shallow than the actual profiles. In addition, such
surveys often do not provide wind speed measurements alongside the pressure
time series, and so the durations of the dust devil signals in the time series
cannot be directly converted to profile widths. Fortunately, simple statistical
and geometric considerations can de-bias these surveys, allowing conversion of
the duration of dust devil signals into physical widths, given only a
distribution of likely translation velocities, and the recovery of the
underlying distributions of physical parameters. In this study, we develop a
scheme for de-biasing such surveys. Applying our model to an in-situ survey
using data from the Phoenix lander suggests a larger dust flux and a dust devil
occurrence rate about ten times larger than previously inferred. Comparing our
results to dust devil track surveys suggests only about one in five
low-pressure cells lifts sufficient dust to leave a visible track.
",0,1,0,0,0,0
515,"Investigation on different physical aspects such as structural, elastic, mechanical, optical properties and Debye temperature of Fe2ScM (M = P and As) semiconductors: a DFT based first principles study","  With the help of first principles calculation method based on the density
functional theory we have investigated the structural, elastic, mechanical
properties and Debye temperature of Fe2ScM (M = P and As) compounds under
pressure up to 60 GPa. The optical properties have been investigated under zero
pressure. Our calculated optimized structural parameters of both the compounds
are in good agreement with the other theoretical results. The calculated
elastic constants show that Fe2ScM (M = P and As) compounds are mechanically
stable up to 60 GPa.
",0,1,0,0,0,0
516,Optimal Envelope Approximation in Fourier Basis with Applications in TV White Space,"  Lowpass envelope approximation of smooth continuous-variable signals are
introduced in this work. Envelope approximations are necessary when a given
signal has to be approximated always to a larger value (such as in TV white
space protection regions). In this work, a near-optimal approximate algorithm
for finding a signal's envelope, while minimizing a mean-squared cost function,
is detailed. The sparse (lowpass) signal approximation is obtained in the
linear Fourier series basis. This approximate algorithm works by discretizing
the envelope property from an infinite number of points to a large (but finite)
number of points. It is shown that this approximate algorithm is near-optimal
and can be solved by using efficient convex optimization programs available in
the literature. Simulation results are provided towards the end to gain more
insights into the analytical results presented.
",1,0,0,0,0,0
517,The curl operator on odd-dimensional manifolds,"  We study the spectral properties of curl, a linear differential operator of
first order acting on differential forms of appropriate degree on an
odd-dimensional closed oriented Riemannian manifold. In three dimensions its
eigenvalues are the electromagnetic oscillation frequencies in vacuum without
external sources. In general, the spectrum consists of the eigenvalue 0 with
infinite multiplicity and further real discrete eigenvalues of finite
multiplicity. We compute the Weyl asymptotics and study the zeta-function. We
give a sharp lower eigenvalue bound for positively curved manifolds and analyze
the equality case. Finally, we compute the spectrum for flat tori, round
spheres and 3-dimensional spherical space forms.
",0,0,1,0,0,0
518,Topology optimization for transient response of structures subjected to dynamic loads,"  This paper presents a topology optimization framework for structural problems
subjected to transient loading. The mechanical model assumes a linear elastic
isotropic material, infinitesimal strains, and a dynamic response. The
optimization problem is solved using the gradient-based optimizer Method of
Moving Asymptotes (MMA) with time-dependent sensitivities provided via the
adjoint method. The stiffness of materials is interpolated using the Solid
Isotropic Material with Penalization (SIMP) method and the Heaviside Projection
Method (HPM) is used to stabilize the problem numerically and improve the
manufacturability of the topology-optimized designs. Both static and dynamic
optimization examples are considered here. The resulting optimized designs
demonstrate the ability of topology optimization to tailor the transient
response of structures.
",0,1,1,0,0,0
519,Generalized Lambert series and arithmetic nature of odd zeta values,"  It is pointed out that the generalized Lambert series
$\displaystyle\sum_{n=1}^{\infty}\frac{n^{N-2h}}{e^{n^{N}x}-1}$ studied by
Kanemitsu, Tanigawa and Yoshimoto can be found on page $332$ of Ramanujan's
Lost Notebook in a slightly more general form. We extend an important
transformation of this series obtained by Kanemitsu, Tanigawa and Yoshimoto by
removing restrictions on the parameters $N$ and $h$ that they impose. From our
extension we deduce a beautiful new generalization of Ramanujan's famous
formula for odd zeta values which, for $N$ odd and $m>0$, gives a relation
between $\zeta(2m+1)$ and $\zeta(2Nm+1)$. A result complementary to the
aforementioned generalization is obtained for any even $N$ and
$m\in\mathbb{Z}$. It generalizes a transformation of Wigert and can be regarded
as a formula for $\zeta\left(2m+1-\frac{1}{N}\right)$. Applications of these
transformations include a generalization of the transformation for the
logarithm of Dedekind eta-function $\eta(z)$, Zudilin- and Rivoal-type results
on transcendence of certain values, and a transcendence criterion for Euler's
constant $\gamma$.
",0,0,1,0,0,0
520,Perfect phylogenies via branchings in acyclic digraphs and a generalization of Dilworth's theorem,"  Motivated by applications in cancer genomics and following the work of
Hajirasouliha and Raphael (WABI 2014), Hujdurović et al. (IEEE TCBB, to
appear) introduced the minimum conflict-free row split (MCRS) problem: split
each row of a given binary matrix into a bitwise OR of a set of rows so that
the resulting matrix corresponds to a perfect phylogeny and has the minimum
possible number of rows among all matrices with this property. Hajirasouliha
and Raphael also proposed the study of a similar problem, in which the task is
to minimize the number of distinct rows of the resulting matrix. Hujdurović
et al. proved that both problems are NP-hard, gave a related characterization
of transitively orientable graphs, and proposed a polynomial-time heuristic
algorithm for the MCRS problem based on coloring cocomparability graphs.
We give new, more transparent formulations of the two problems, showing that
the problems are equivalent to two optimization problems on branchings in a
derived directed acyclic graph. Building on these formulations, we obtain new
results on the two problems, including: (i) a strengthening of the heuristic by
Hujdurović et al. via a new min-max result in digraphs generalizing
Dilworth's theorem, which may be of independent interest, (ii) APX-hardness
results for both problems, (iii) approximation algorithms, and (iv)
exponential-time algorithms solving the two problems to optimality faster than
the naïve brute-force approach. Our work relates to several well studied
notions in combinatorial optimization: chain partitions in partially ordered
sets, laminar hypergraphs, and (classical and weighted) colorings of graphs.
",1,0,1,0,0,0
521,Towards a Service-oriented Platform for Intelligent Apps in Intermediate Cities,"  Smart cities are a growing trend in many cities in Argentina. In particular,
the so-called intermediate cities present a context and requirements different
from those of large cities with respect to smart cities. One aspect of
relevance is to encourage the development of applications (generally for mobile
devices) that enable citizens to take advantage of data and services normally
associated with the city, for example, in the urban mobility domain. In this
work, a platform is proposed for intermediate cities that provide ""high level""
services and that allow the construction of software applications that consume
those services. Our platform-centric strategy focused aims to integrate systems
and heterogeneous data sources, and provide ""intelligent"" services to different
applications. Examples of these services include: construction of user
profiles, recommending local events, and collaborative sensing based on data
mining techniques, among others. In this work, the design of this platform
(currently in progress) is described, and experiences of applications for urban
mobility are discussed, which are being migrated in the form of reusable
services provided by the platform
",1,0,0,0,0,0
522,Fully Bayesian Estimation Under Informative Sampling,"  Bayesian estimation is increasingly popular for performing model based
inference to support policymaking. These data are often collected from surveys
under informative sampling designs where subject inclusion probabilities are
designed to be correlated with the response variable of interest. Sampling
weights constructed from marginal inclusion probabilities are typically used to
form an exponentiated pseudo likelihood that adjusts the population likelihood
for estimation on the sample due to ease-of-estimation. We propose an
alternative adjustment based on a Bayes rule construction that simultaneously
performs weight smoothing and estimates the population model parameters in a
fully Bayesian construction. We formulate conditions on known marginal and
pairwise inclusion probabilities that define a class of sampling designs where
$L_{1}$ consistency of the joint posterior is guaranteed. We compare
performances between the two approaches on synthetic data, which reveals that
our fully Bayesian approach better estimates posterior uncertainty without a
requirement to calibrate the normalization of the sampling weights. We
demonstrate our method on an application concerning the National Health and
Nutrition Examination Survey exploring the relationship between caffeine
consumption and systolic blood pressure.
",0,0,1,1,0,0
523,Bistable reaction equations with doubly nonlinear diffusion,"  Reaction-diffusion equations appear in biology and chemistry, and combine
linear diffusion with different kind of reaction terms. Some of them are
remarkable from the mathematical point of view, since they admit families of
travelling waves that describe the asymptotic behaviour of a larger class of
solutions $0\leq u(x,t)\leq 1$ of the problem posed in the real line. We
investigate here the existence of waves with constant propagation speed, when
the linear diffusion is replaced by the ""slow"" doubly nonlinear diffusion. In
the present setting we consider bistable reaction terms, which present
interesting differences w.r.t. the Fisher-KPP framework recently studied in
\cite{AA-JLV:art}. We find different families of travelling waves that are
employed to describe the wave propagation of more general solutions and to
study the stability/instability of the steady states, even when we extend the
study to several space dimensions. A similar study is performed in the critical
case that we call ""pseudo-linear"", i.e., when the operator is still nonlinear
but has homogeneity one. With respect to the classical model and the
""pseudo-linear"" case, the travelling waves of the ""slow"" diffusion setting
exhibit free boundaries. \\ Finally, as a complement of \cite{AA-JLV:art}, we
study the asymptotic behaviour of more general solutions in the presence of a
""heterozygote superior"" reaction function and doubly nonlinear diffusion
(""slow"" and ""pseudo-linear"").
",0,0,1,0,0,0
524,Static Free Space Detection with Laser Scanner using Occupancy Grid Maps,"  Drivable free space information is vital for autonomous vehicles that have to
plan evasive maneuvers in real-time. In this paper, we present a new efficient
method for environmental free space detection with laser scanner based on 2D
occupancy grid maps (OGM) to be used for Advanced Driving Assistance Systems
(ADAS) and Collision Avoidance Systems (CAS). Firstly, we introduce an enhanced
inverse sensor model tailored for high-resolution laser scanners for building
OGM. It compensates the unreflected beams and deals with the ray casting to
grid cells accuracy and computational effort problems. Secondly, we introduce
the 'vehicle on a circle for grid maps' map alignment algorithm that allows
building more accurate local maps by avoiding the computationally expensive
inaccurate operations of image sub-pixel shifting and rotation. The resulted
grid map is more convenient for ADAS features than existing methods, as it
allows using less memory sizes, and hence, results into a better real-time
performance. Thirdly, we present an algorithm to detect what we call the
'in-sight edges'. These edges guarantee modeling the free space area with a
single polygon of a fixed number of vertices regardless the driving situation
and map complexity. The results from real world experiments show the
effectiveness of our approach.
",1,0,0,0,0,0
525,Proactive Edge Computing in Latency-Constrained Fog Networks,"  In this paper, the fundamental problem of distribution and proactive caching
of computing tasks in fog networks is studied under latency and reliability
constraints. In the proposed scenario, computing can be executed either locally
at the user device or offloaded to an edge cloudlet. Moreover, cloudlets
exploit both their computing and storage capabilities by proactively caching
popular task computation results to minimize computing latency. To this end, a
clustering method to group spatially proximate user devices with mutual task
popularity interests and their serving cloudlets is proposed. Then, cloudlets
can proactively cache the popular tasks' computations of their cluster members
to minimize computing latency. Additionally, the problem of distributing tasks
to cloudlets is formulated as a matching game in which a cost function of
computing delay is minimized under latency and reliability constraints.
Simulation results show that the proposed scheme guarantees reliable
computations with bounded latency and achieves up to 91% decrease in computing
latency as compared to baseline schemes.
",1,0,0,0,0,0
526,Deuterium fractionation and H2D+ evolution in turbulent and magnetized cloud cores,"  High-mass stars are expected to form from dense prestellar cores. Their
precise formation conditions are widely discussed, including their virial
condition, which results in slow collapse for super-virial cores with strong
support by turbulence or magnetic fields, or fast collapse for sub-virial
sources. To disentangle their formation processes, measurements of the
deuterium fractions are frequently employed to approximately estimate the ages
of these cores and to obtain constraints on their dynamical evolution. We here
present 3D magneto-hydrodynamical simulations including for the first time an
accurate non-equilibrium chemical network with 21 gas-phase species plus dust
grains and 213 reactions. With this network we model the deuteration process in
fully depleted prestellar cores in great detail and determine its response to
variations in the initial conditions. We explore the dependence on the initial
gas column density, the turbulent Mach number, the mass-to-magnetic flux ratio
and the distribution of the magnetic field, as well as the initial
ortho-to-para ratio of H2. We find excellent agreement with recent observations
of deuterium fractions in quiescent sources. Our results show that deuteration
is rather efficient, even when assuming a conservative ortho-to-para ratio of 3
and highly sub-virial initial conditions, leading to large deuterium fractions
already within roughly a free-fall time. We discuss the implications of our
results and give an outlook to relevant future investigations.
",0,1,0,0,0,0
527,Experimental data over quantum mechanics simulations for inferring the repulsive exponent of the Lennard-Jones potential in Molecular Dynamics,"  The Lennard-Jones (LJ) potential is a cornerstone of Molecular Dynamics (MD)
simulations and among the most widely used computational kernels in science.
The potential models atomistic attraction and repulsion with century old
prescribed parameters ($q=6, \; p=12$, respectively), originally related by a
factor of two for simplicity of calculations. We re-examine the value of the
repulsion exponent through data driven uncertainty quantification. We perform
Hierarchical Bayesian inference on MD simulations of argon using experimental
data of the radial distribution function (RDF) for a range of thermodynamic
conditions, as well as dimer interaction energies from quantum mechanics
simulations. The experimental data suggest a repulsion exponent ($p \approx
6.5$), in contrast to the quantum simulations data that support values closer
to the original ($p=12$) exponent. Most notably, we find that predictions of
RDF, diffusion coefficient and density of argon are more accurate and robust in
producing the correct argon phase around its triple point, when using the
values inferred from experimental data over those from quantum mechanics
simulations. The present results suggest the need for data driven recalibration
of the LJ potential across MD simulations.
",0,1,0,1,0,0
528,Ensemble Sampling,"  Thompson sampling has emerged as an effective heuristic for a broad range of
online decision problems. In its basic form, the algorithm requires computing
and sampling from a posterior distribution over models, which is tractable only
for simple special cases. This paper develops ensemble sampling, which aims to
approximate Thompson sampling while maintaining tractability even in the face
of complex models such as neural networks. Ensemble sampling dramatically
expands on the range of applications for which Thompson sampling is viable. We
establish a theoretical basis that supports the approach and present
computational results that offer further insight.
",1,0,0,1,0,0
529,Failures of Gradient-Based Deep Learning,"  In recent years, Deep Learning has become the go-to solution for a broad
range of applications, often outperforming state-of-the-art. However, it is
important, for both theoreticians and practitioners, to gain a deeper
understanding of the difficulties and limitations associated with common
approaches and algorithms. We describe four types of simple problems, for which
the gradient-based algorithms commonly used in deep learning either fail or
suffer from significant difficulties. We illustrate the failures through
practical experiments, and provide theoretical insights explaining their
source, and how they might be remedied.
",1,0,0,1,0,0
530,Searching for the Transit of the Earth--mass exoplanet Proxima~Centauri~b in Antarctica: Preliminary Result,"  Proxima Centauri is known as the closest star from the Sun. Recently, radial
velocity observations revealed the existence of an Earth-mass planet around it.
With an orbital period of ~11 days, the surface of Proxima Centauri b is
temperate and might be habitable. We took a photometric monitoring campaign to
search for its transit, using the Bright Star Survey Telescope at the Zhongshan
Station in Antarctica. A transit-like signal appearing on 2016 September 8th,
is identified tentatively. Its midtime, $T_{C}=2,457,640.1990\pm0.0017$ HJD, is
consistent with the predicted ephemeris based on RV orbit in a 1$\sigma$
confidence interval. Time-correlated noise is pronounced in the light curve of
Proxima Centauri, affecting detection of transits. We develop a technique, in a
Gaussian process framework, to gauge the statistical significance of potential
transit detection. The tentative transit signal reported here, has a confidence
level of $2.5\sigma$. Further detection of its periodic signals is necessary to
confirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima
Centauri in next Polar night at Dome A in Antarctica, taking the advantage of
continuous darkness. \citet{Kipping17} reported two tentative transit-like
signals of Proxima Centauri b, observed by the Microvariability and Oscillation
of Stars space Telescope in 2014 and 2015, respectively. The midtransit time of
our detection is 138 minutes later than that predicted by their transit
ephemeris. If all the signals are real transits, the misalignment of the epochs
plausibly suggests transit timing variations of Proxima Centauri b induced by
an outer planet in this system.
",0,1,0,0,0,0
531,A Data-Driven Sparse-Learning Approach to Model Reduction in Chemical Reaction Networks,"  In this paper, we propose an optimization-based sparse learning approach to
identify the set of most influential reactions in a chemical reaction network.
This reduced set of reactions is then employed to construct a reduced chemical
reaction mechanism, which is relevant to chemical interaction network modeling.
The problem of identifying influential reactions is first formulated as a
mixed-integer quadratic program, and then a relaxation method is leveraged to
reduce the computational complexity of our approach. Qualitative and
quantitative validation of the sparse encoding approach demonstrates that the
model captures important network structural properties with moderate
computational load.
",1,0,0,0,0,0
532,Parity-Forbidden Transitions and Their Impacts on the Optical Absorption Properties of Lead-Free Metal Halide Perovskites and Double Perovskites,"  Using density-functional theory calculations, we analyze the optical
absorption properties of lead (Pb)-free metal halide perovskites
(AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or
monovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent
metal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is
not Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions
because of their indirect bandgap nature. Among the nine possible types of
Pb-free metal halide double perovskites, six have direct bandgaps. Of these six
types, four show inversion symmetry-induced parity-forbidden or weak
transitions between band edges, making them not ideal for thin-film solar cell
application. Only one type of Pb-free double perovskite shows optical
absorption and electronic properties suitable for solar cell applications,
namely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide
important insights for designing new metal halide perovskites and double
perovskites for optoelectronic applications.
",0,1,0,0,0,0
533,Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer,"  MicroRNAs play important roles in many biological processes. Their aberrant
expression can have oncogenic or tumor suppressor function directly
participating to carcinogenesis, malignant transformation, invasiveness and
metastasis. Indeed, miRNA profiles can distinguish not only between normal and
cancerous tissue but they can also successfully classify different subtypes of
a particular cancer. Here, we focus on a particular class of transcripts
encoding polycistronic miRNA genes that yields multiple miRNA components. We
describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully
redesigned release of the MMRA computational pipeline (MiRNA Master Regulator
Analysis), developed to search for clustered miRNAs potentially driving cancer
molecular subtyping. Genomically clustered miRNAs are frequently co-expressed
to target different components of pro-tumorigenic signalling pathways. By
applying ClustMMRA to breast cancer patient data, we identified key miRNA
clusters driving the phenotype of different tumor subgroups. The pipeline was
applied to two independent breast cancer datasets, providing statistically
concordant results between the two analysis. We validated in cell lines the
miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative
subtype phenotype through its control of proliferation and EMT.
",0,0,0,0,1,0
534,A Belief Propagation Algorithm for Multipath-Based SLAM,"  We present a simultaneous localization and mapping (SLAM) algorithm that is
based on radio signals and the association of specular multipath components
(MPCs) with geometric features. Especially in indoor scenarios, robust
localization from radio signals is challenging due to diffuse multipath
propagation, unknown MPC-feature association, and limited visibility of
features. In our approach, specular reflections at flat surfaces are described
in terms of virtual anchors (VAs) that are mirror images of the physical
anchors (PAs). The positions of these VAs and possibly also of the PAs are
unknown. We develop a Bayesian model of the SLAM problem including the unknown
MPC-VA/PA association. We represent this model by a factor graph, which enables
the use of the belief propagation (BP) scheme for efficient marginalization of
the joint posterior distribution. The resulting BP-based SLAM algorithm detects
the VAs associated with the PAs and estimates jointly the time-varying position
of the mobile agent and the positions of the VAs and possibly also of the PAs,
thereby leveraging the MPCs in the radio signal for improved accuracy and
robustness of agent localization. A core aspect of the algorithm is BP-based
probabilistic MPC-VA/PA association. Moreover, for improved initialization of
new VA positions, the states of unobserved potential VAs are modeled as a
random finite set and propagated in time by means of a ""zero-measurement""
probability hypothesis density filter. The proposed BP-based SLAM algorithm has
a low computational complexity and scales well in all relevant system
parameters. Experimental results using both synthetically generated
measurements and real ultra-wideband radio signals demonstrate the excellent
performance of the algorithm in challenging indoor environments.
",1,0,0,0,0,0
535,Local methods for blocks of finite simple groups,"  This survey is about old and new results about the modular representation
theory of finite reductive groups with a strong emphasis on local methods. This
includes subpairs, Brauer's Main Theorems, fusion, Rickard equivalences. In the
defining characteristic we describe the relation between $p$-local subgroups
and parabolic subgroups, then give classical consequences on simple modules and
blocks, including the Alperin weight conjecture in that case. In the
non-defining characteristics, we sketch a picture of the local methods
pioneered by Fong-Srinivasan in the determination of blocks and their ordinary
characters. This includes the relationship with Lusztig's twisted induction and
the determination of defect groups. We conclude with a survey of the results
and methods by Bonnafé-Dat-Rouquier giving Morita equivalences between blocks
that preserve defect groups and the local structures.
The text grew out of the course and talks given by the author in July and
September 2016 during the program ""Local representation theory and simple
groups"" at CIB Lausanne. Written Oct 2017, to appear in a proceedings volume
published by EMS.
",0,0,1,0,0,0
536,Motion Planning for a Humanoid Mobile Manipulator System,"  A high redundant non-holonomic humanoid mobile dual-arm manipulator system is
presented in this paper where the motion planning to realize ""human-like""
autonomous navigation and manipulation tasks is studied. Firstly, an improved
MaxiMin NSGA-II algorithm, which optimizes five objective functions to solve
the problems of singularity, redundancy, and coupling between mobile base and
manipulator simultaneously, is proposed to design the optimal pose to
manipulate the target object. Then, in order to link the initial pose and that
optimal pose, an off-line motion planning algorithm is designed. In detail, an
efficient direct-connect bidirectional RRT and gradient descent algorithm is
proposed to reduce the sampled nodes largely, and a geometric optimization
method is proposed for path pruning. Besides, head forward behaviors are
realized by calculating the reasonable orientations and assigning them to the
mobile base to improve the quality of human-robot interaction. Thirdly, the
extension to on-line planning is done by introducing real-time sensing,
collision-test and control cycles to update robotic motion in dynamic
environments. Fourthly, an EEs' via-point-based multi-objective genetic
algorithm is proposed to design the ""human-like"" via-poses by optimizing four
objective functions. Finally, numerous simulations are presented to validate
the effectiveness of proposed algorithms.
",1,0,0,0,0,0
537,End-to-end Planning of Fixed Millimeter-Wave Networks,"  This article discusses a framework to support the design and end-to-end
planning of fixed millimeter-wave networks. Compared to traditional techniques,
the framework allows an organization to quickly plan a deployment in a
cost-effective way. We start by using LiDAR data---basically, a 3D point cloud
captured from a city---to estimate potential sites to deploy antennas and
whether there is line-of-sight between them. With that data on hand, we use
combinatorial optimization techniques to determine the optimal set of locations
and how they should communicate with each other, to satisfy engineering (e.g.,
latency, polarity), design (e.g., reliability) and financial (e.g., total cost
of operation) constraints. The primary goal is to connect as many people as
possible to the network. Our methodology can be used for strategic planning
when an organization is in the process of deciding whether to adopt a
millimeter-wave technology or choosing between locations, or for operational
planning when conducting a detailed design of the actual network to be deployed
in a selected location.
",1,0,1,0,0,0
538,A giant planet undergoing extreme ultraviolet irradiation by its hot massive-star host,"  The amount of ultraviolet irradiation and ablation experienced by a planet
depends strongly on the temperature of its host star. Of the thousands of
extra-solar planets now known, only four giant planets have been found that
transit hot, A-type stars (temperatures of 7300-10,000K), and none are known to
transit even hotter B-type stars. WASP-33 is an A-type star with a temperature
of ~7430K, which hosts the hottest known transiting planet; the planet is
itself as hot as a red dwarf star of type M. The planet displays a large heat
differential between its day-side and night-side, and is highly inflated,
traits that have been linked to high insolation. However, even at the
temperature of WASP-33b's day-side, its atmosphere likely resembles the
molecule-dominated atmospheres of other planets, and at the level of
ultraviolet irradiation it experiences, its atmosphere is unlikely to be
significantly ablated over the lifetime of its star. Here we report
observations of the bright star HD 195689, which reveal a close-in (orbital
period ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star
is at the dividing line between stars of type A and B, and we measure the
KELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar
type K4. The molecules in K stars are entirely dissociated, and thus the
primary sources of opacity in the day-side atmosphere of KELT-9b are likely
atomic metals. Furthermore, KELT-9b receives ~700 times more extreme
ultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b,
leading to a predicted range of mass-loss rates that could leave the planet
largely stripped of its envelope during the main-sequence lifetime of the host
star.
",0,1,0,0,0,0
539,"Characterizing videos, audience and advertising in Youtube channels for kids","  Online video services, messaging systems, games and social media services are
tremendously popular among young people and children in many countries. Most of
the digital services offered on the internet are advertising funded, which
makes advertising ubiquitous in children's everyday life. To understand the
impact of advertising-based digital services on children, we study the
collective behavior of users of YouTube for kids channels and present the
demographics of a large number of users. We collected data from 12,848 videos
from 17 channels in US and UK and 24 channels in Brazil. The channels in
English have been viewed more than 37 billion times. We also collected more
than 14 million comments made by users. Based on a combination of text-analysis
and face recognition tools, we show the presence of racial and gender biases in
our large sample of users. We also identify children actively using YouTube,
although the minimum age for using the service is 13 years in most countries.
We provide comparisons of user behavior among the three countries, which
represent large user populations in the global North and the global South.
",1,0,0,0,0,0
540,Vanishing theorems for the negative K-theory of stacks,"  We prove that the homotopy algebraic K-theory of tame quasi-DM stacks
satisfies cdh-descent. We apply this descent result to prove that if X is a
Noetherian tame quasi-DM stack and i < -dim(X), then K_i(X)[1/n] = 0 (resp.
K_i(X, Z/n) = 0) provided that n is nilpotent on X (resp. is invertible on X).
Our descent and vanishing results apply more generally to certain Artin stacks
whose stabilizers are extensions of finite group schemes by group schemes of
multiplicative type.
",0,0,1,0,0,0
541,Gravitational radiation from compact binary systems in screened modified gravity,"  Screened modified gravity (SMG) is a kind of scalar-tensor theory with
screening mechanisms, which can suppress the fifth force in dense regions and
allow theories to evade the solar system and laboratory tests. In this paper,
we investigate how the screening mechanisms in SMG affect the gravitational
radiation damping effects, calculate in detail the rate of the energy loss due
to the emission of tensor and scalar gravitational radiations, and derive their
contributions to the change in the orbital period of the binary system. We find
that the scalar radiation depends on the screened parameters and the
propagation speed of scalar waves, and the scalar dipole radiation dominates
the orbital decay of the binary system. For strongly self-gravitating bodies,
all effects of scalar sector are strongly suppressed by the screening
mechanisms in SMG. By comparing our results to observations of binary system
PSR J1738+0333, we place the stringent constraints on the screening mechanisms
in SMG. As an application of these results, we focus on three specific models
of SMG (chameleon, symmetron, and dilaton), and derive the constraints on the
model parameters, respectively.
",0,1,0,0,0,0
542,weedNet: Dense Semantic Weed Classification Using Multispectral Images and MAV for Smart Farming,"  Selective weed treatment is a critical step in autonomous crop management as
related to crop health and yield. However, a key challenge is reliable, and
accurate weed detection to minimize damage to surrounding plants. In this
paper, we present an approach for dense semantic weed classification with
multispectral images collected by a micro aerial vehicle (MAV). We use the
recently developed encoder-decoder cascaded Convolutional Neural Network (CNN),
Segnet, that infers dense semantic classes while allowing any number of input
image channels and class balancing with our sugar beet and weed datasets. To
obtain training datasets, we established an experimental field with varying
herbicide levels resulting in field plots containing only either crop or weed,
enabling us to use the Normalized Difference Vegetation Index (NDVI) as a
distinguishable feature for automatic ground truth generation. We train 6
models with different numbers of input channels and condition (fine-tune) it to
achieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification
metrics. For model deployment, an embedded GPU system (Jetson TX2) is tested
for MAV integration. Dataset used in this paper is released to support the
community and future work.
",1,0,0,0,0,0
543,An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$,"  Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to
$SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine
an inductive formula for the Schubert basis structure constants in the
torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of
ordinary (non-equivariant) $K$-theory we find an explicit closed form for the
structure constants. We also determine an inductive formula for the structure
constants in the torus-equivariant cohomology ring, and use this formula to
find closed forms for some of the structure constants.
",0,0,1,0,0,0
544,The Correct Application of Variance Concept in Measurement Theory,"  The existing measurement theory interprets the variance as the dispersion of
measured value, which is actually contrary to a general mathematical knowledge
that the variance of a constant is 0. This paper will fully demonstrate that
the variance in measurement theory is actually the evaluation of probability
interval of an error instead of the dispersion of a measured value, point out
the key point of mistake in the existing interpretation, and fully interpret a
series of changes in conceptual logic and processing method brought about by
this new concept.
",0,0,1,1,0,0
545,A Generalized Framework for the Estimation of Causal Moderation Effects with Randomized Treatments and Non-Randomized Moderators,"  Researchers are often interested in analyzing conditional treatment effects.
One variant of this is ""causal moderation,"" which implies that intervention
upon a third (moderator) variable would alter the treatment effect. This study
presents a generalized, non-parametric framework for estimating causal
moderation effects given randomized treatments and non-randomized moderators
that achieves a number of goals. First, it highlights how conventional
approaches do not constitute unbiased or consistent estimators of causal
moderation effects. Second, it offers researchers a simple, transparent
approach for estimating causal moderation effects and lays out the assumptions
under which this can be performed consistently and/or without bias. Third, as
part of the estimation process, it allows researchers to implement their
preferred method of covariate adjustment, including parametric and
non-parametric methods, or alternative identification strategies of their
choosing. Fourth, it provides a set-up whereby sensitivity analysis designed
for the average-treatment-effect context can be extended to the moderation
context. An original application is also presented.
",0,0,0,1,0,0
546,Spherical Functions on Riemannian Symmetric Spaces,"  This paper deals with some simple results about spherical functions of type
$\delta$, namely new integral formulas, new results about behavior at infinity
and some facts about the related $C_\sigma$ functions.
",0,0,1,0,0,0
547,Modular curves with infinitely many cubic points,"  In this study, we determine all modular curves $X_0(N)$ that admit infinitely
many cubic points.
",0,0,1,0,0,0
548,Semi-Analytical Perturbative Approaches to Third Body Resonant Trajectories,"  In the framework of multi-body dynamics, successive encounters with a third
body, even if well outside of its sphere of influence, can noticeably alter the
trajectory of a spacecraft. Examples of these effects have already been
exploited by past missions such as SMART-1, as well as are proposed to benefit
future missions to Jupiter, Saturn or Neptune, and disposal strategies from
Earth's High Eccentric or Libration Point Orbits. This paper revises three
totally different descriptions of the effects of the third body gravitational
perturbation. These are the averaged dynamics of the classical third body
perturbing function, the Opik's close encounter theory and the Keplerian map
approach. The first two techniques have respectively been applied to the cases
of a spacecraft either always remaining very far or occasionally experiencing
extremely close approaches to the third body. However, the paper also seeks
solutions for trajectories that undergo one or more close approaches at
distances in the order of the sphere of influence of the third body. The paper
attempts to gain insight into the accuracy of these different perturbative
techniques into each of these scenarios, as compared with the motion in the
Circular Restricted Three Body Problem.
",0,1,0,0,0,0
549, Robust and Imperceptible Adversarial Attacks on Capsule Networks,"  Capsule Networks envision an innovative point of view about the
representation of the objects in the brain and preserve the hierarchical
spatial relationships between them. This type of networks exhibits a huge
potential for several Machine Learning tasks like image classification, while
outperforming Convolutional Neural Networks (CNNs). A large body of work has
explored adversarial examples for CNNs, but their efficacy to Capsule Networks
is not well explored. In our work, we study the vulnerabilities in Capsule
Networks to adversarial attacks. These perturbations, added to the test inputs,
are small and imperceptible to humans, but fool the network to mis-predict. We
propose a greedy algorithm to automatically generate targeted imperceptible
adversarial examples in a black-box attack scenario. We show that this kind of
attacks, when applied to the German Traffic Sign Recognition Benchmark (GTSRB),
mislead Capsule Networks. Moreover, we apply the same kind of adversarial
attacks to a 9-layer CNN and analyze the outcome, compared to the Capsule
Networks to study their differences / commonalities.
",1,0,0,1,0,0
550,Thermophoretic MHD Flow and Non-linear Radiative Heat Transfer with Convective Boundary Conditions over a Non-linearly Stretching Sheet,"  The effects of MHD boundary layer flow of non-linear thermal radiation with
convective heat transfer and non-uniform heat source/sink in presence of
thermophortic velocity and chemical reaction investigated in this study.
Suitable similarity transformation are used to solve the partial ordinary
differential equation of considered governing flow. Runge-Kutta fourth fifth
order Fehlberg method with shooting techniques are used to solved
non-dimensional governing equations. The variation of different parameters such
as thermophoretic parameter, chemical reaction parameter, non- uniform heat
source/sink parameters are studied on velocity, temperature and concentration
profiles, and are described by suitable graphs and tables. The obtained results
are in very well agreement with previous results.
",0,1,0,0,0,0
551,Resting-state ASL : Toward an optimal sequence duration,"  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily
practice and academic research stay discreet compared to resting-state BOLD.
However, by giving direct access to cerebral blood flow maps, rs-fASL leads to
significant clinical subject scaled application as CBF can be considered as a
biomarker in common neuropathology. Our work here focuses on the link between
overall quality of rs-fASL and duration of acquisition. To this end, we
consider subject self-Default Mode Network (DMN), and assess DMN quality
depletion compared to a gold standard DMN depending on the duration of
acquisition.
",0,0,0,0,1,0
552,Learning Neural Models for End-to-End Clustering,"  We propose a novel end-to-end neural network architecture that, once trained,
directly outputs a probabilistic clustering of a batch of input examples in one
pass. It estimates a distribution over the number of clusters $k$, and for each
$1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster
assignment for each data point. The network is trained in advance in a
supervised fashion on separate data to learn grouping by any perceptual
similarity criterion based on pairwise labels (same/different group). It can
then be applied to different data containing different groups. We demonstrate
promising performance on high-dimensional data like images (COIL-100) and
speech (TIMIT). We call this ``learning to cluster'' and show its conceptual
difference to deep metric learning, semi-supervise clustering and other related
approaches while having the advantage of performing learnable clustering fully
end-to-end.
",0,0,0,1,0,0
553,Anisotropic exchange and spin-wave damping in pure and electron-doped Sr$_2$IrO$_4$,"  The collective magnetic excitations in the spin-orbit Mott insulator
(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\,0.01,\,0.04,\, 0.1$) were investigated by
means of resonant inelastic x-ray scattering. We report significant magnon
energy gaps at both the crystallographic and antiferromagnetic zone centers at
all doping levels, along with a remarkably pronounced momentum-dependent
lifetime broadening. The spin-wave gap is accounted for by a significant
anisotropy in the interactions between $J_\text{eff}=1/2$ isospins, thus
marking the departure of Sr$_2$IrO$_4$ from the essentially isotropic
Heisenberg model appropriate for the superconducting cuprates.
",0,1,0,0,0,0
554,Anomalous transport properties in Nb/Bi1.95Sb0.05Se3 hybrid structure,"  We report the proximity induced anomalous transport behavior in a Nb
Bi1.95Sb0.05Se3 heterostructure. Mechanically Exfoliated single crystal of
Bi1.95Sb0.05Se3 topological insulator (TI) is partially covered with a 100 nm
thick Niobium superconductor using DC magnetron sputtering by shadow masking
technique. The magnetotransport (MR) measurements have been performed
simultaneously on the TI sample with and without Nb top layer in the
temperature,T, range of 3 to 8 K, and a magnetic field B up to 15 T. MR on TI
region shows Subnikov de Haas oscillation at fields greater than 5 T. Anomalous
linear change in resistance is observed in the field range of negative 4T to
positive 4T at which Nb is superconducting. At 0 T field, the temperature
dependence of resistance on the Nb covered region revealed a superconducting
transition (TC) at 8.2 K, whereas TI area showed similar TC with the absence of
zero resistance states due to the additional resistance from superconductor
(SC) TI interface. Interestingly below the TC the R vs T measured on TI showed
an enhancement in resistance for positive field and prominent fall in
resistance for negative field direction. This indicates the directional
dependent scattering of the Cooper pairs on the surface of the TI due to the
superposition of spin singlet and triplet states in the superconductor and TI
respectively.
",0,1,0,0,0,0
555,State-dependent Priority Scheduling for Networked Control Systems,"  Networked control systems (NCS) have attracted considerable attention in
recent years. While the stabilizability and optimal control of NCS for a given
communication system has already been studied extensively, the design of the
communication system for NCS has recently seen an increase in more thorough
investigation. In this paper, we address an optimal scheduling problem for a
set of NCS sharing a dedicated communication channel, providing performance
bounds and asymptotic stability. We derive a suboptimal scheduling policy with
dynamic state-based priorities calculated at the sensors, which are then used
for stateless priority queuing in the network, making it both scalable and
efficient to implement on routers or multi-layer switches. These properties are
beneficial towards leveraging existing IP networks for control, which will be a
crucial factor for the proliferation of wide-area NCS applications. By allowing
for an arbitrary number of concurrent transmissions, we are able to investigate
the relationship between available bandwidth, transmission rate, and delay. To
demonstrate the feasibility of our approach, we provide a proof-of-concept
implementation of the priority scheduler using real networking hardware.
",1,0,0,0,0,0
556,Deformation conditions for pseudorepresentations,"  Given a property of representations satisfying a basic stability condition,
Ramakrishna developed a variant of Mazur's Galois deformation theory for
representations with that property. We introduce an axiomatic definition of
pseudorepresentations with such a property. Among other things, we show that
pseudorepresentations with a property enjoy a good deformation theory,
generalizing Ramakrishna's theory to pseudorepresentations.
",0,0,1,0,0,0
557,Adaptive local surface refinement based on LR NURBS and its application to contact,"  A novel adaptive local surface refinement technique based on Locally Refined
Non-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model
complex geometries exactly and are the rational extension of LR B-splines. The
local representation of the parameter space overcomes the drawback of
non-existent local refinement in standard NURBS-based isogeometric analysis.
For a convenient embedding into general finite element code, the Bézier
extraction operator for LR NURBS is formulated. An automatic remeshing
technique is presented that allows adaptive local refinement and coarsening of
LR NURBS. In this work, LR NURBS are applied to contact computations of 3D
solids and membranes. For solids, LR NURBS-enriched finite elements are used to
discretize the contact surfaces with LR NURBS finite elements, while the rest
of the body is discretized by linear Lagrange finite elements. For membranes,
the entire surface is discretized by LR NURBS. Various numerical examples are
shown, and they demonstrate the benefit of using LR NURBS: Compared to uniform
refinement, LR NURBS can achieve high accuracy at lower computational cost.
",1,0,0,0,0,0
558,"Approximate Program Smoothing Using Mean-Variance Statistics, with Application to Procedural Shader Bandlimiting","  This paper introduces a general method to approximate the convolution of an
arbitrary program with a Gaussian kernel. This process has the effect of
smoothing out a program. Our compiler framework models intermediate values in
the program as random variables, by using mean and variance statistics. Our
approach breaks the input program into parts and relates the statistics of the
different parts, under the smoothing process. We give several approximations
that can be used for the different parts of the program. These include the
approximation of Dorn et al., a novel adaptive Gaussian approximation, Monte
Carlo sampling, and compactly supported kernels. Our adaptive Gaussian
approximation is accurate up to the second order in the standard deviation of
the smoothing kernel, and mathematically smooth. We show how to construct a
compiler that applies chosen approximations to given parts of the input
program. Because each expression can have multiple approximation choices, we
use a genetic search to automatically select the best approximations. We apply
this framework to the problem of automatically bandlimiting procedural shader
programs. We evaluate our method on a variety of complex shaders, including
shaders with parallax mapping, animation, and spatially varying statistics. The
resulting smoothed shader programs outperform previous approaches both
numerically, and aesthetically, due to the smoothing properties of our
approximations.
",1,0,0,0,0,0
559,Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach,"  Large-scale computational experiments, often running over weeks and over
large datasets, are used extensively in fields such as epidemiology,
meteorology, computational biology, and healthcare to understand phenomena, and
design high-stakes policies affecting everyday health and economy. For
instance, the OpenMalaria framework is a computationally-intensive simulation
used by various non-governmental and governmental agencies to understand
malarial disease spread and effectiveness of intervention strategies, and
subsequently design healthcare policies. Given that such shared results form
the basis of inferences drawn, technological solutions designed, and day-to-day
policies drafted, it is essential that the computations are validated and
trusted. In particular, in a multi-agent environment involving several
independent computing agents, a notion of trust in results generated by peers
is critical in facilitating transparency, accountability, and collaboration.
Using a novel combination of distributed validation of atomic computation
blocks and a blockchain-based immutable audits mechanism, this work proposes a
universal framework for distributed trust in computations. In particular we
address the scalaibility problem by reducing the storage and communication
costs using a lossy compression scheme. This framework guarantees not only
verifiability of final results, but also the validity of local computations,
and its cost-benefit tradeoffs are studied using a synthetic example of
training a neural network.
",1,0,0,0,0,0
560,Empirical analysis of non-linear activation functions for Deep Neural Networks in classification tasks,"  We provide an overview of several non-linear activation functions in a neural
network architecture that have proven successful in many machine learning
applications. We conduct an empirical analysis on the effectiveness of using
these function on the MNIST classification task, with the aim of clarifying
which functions produce the best results overall. Based on this first set of
results, we examine the effects of building deeper architectures with an
increasing number of hidden layers. We also survey the impact of using, on the
same task, different initialisation schemes for the weights of our neural
network. Using these sets of experiments as a base, we conclude by providing a
optimal neural network architecture that yields impressive results in accuracy
on the MNIST classification task.
",1,0,0,1,0,0
561,A Survey of Deep Learning Techniques for Mobile Robot Applications,"  Advancements in deep learning over the years have attracted research into how
deep artificial neural networks can be used in robotic systems. This research
survey will present a summarization of the current research with a specific
focus on the gains and obstacles for deep learning to be applied to mobile
robotics.
",1,0,0,0,0,0
562,Design of the Artificial: lessons from the biological roots of general intelligence,"  Our desire and fascination with intelligent machines dates back to the
antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought
(syllogism) and Heron of Alexandria's mechanical machines and automata.
However, the quest for Artificial General Intelligence (AGI) is troubled with
repeated failures of strategies and approaches throughout the history. This
decade has seen a shift in interest towards bio-inspired software and hardware,
with the assumption that such mimicry entails intelligence. Though these steps
are fruitful in certain directions and have advanced automation, their singular
design focus renders them highly inefficient in achieving AGI. Which set of
requirements have to be met in the design of AGI? What are the limits in the
design of the artificial? Here, a careful examination of computation in
biological systems hints that evolutionary tinkering of contextual processing
of information enabled by a hierarchical architecture is the key to build AGI.
",1,1,0,0,0,0
563,Continuum of quantum fluctuations in a three-dimensional $S\!=\!1$ Heisenberg magnet,"  Conventional crystalline magnets are characterized by symmetry breaking and
normal modes of excitation called magnons with quantized angular momentum
$\hbar$. Neutron scattering correspondingly features extra magnetic Bragg
diffraction at low temperatures and dispersive inelastic scattering associated
with single magnon creation and annihilation. Exceptions are anticipated in
so-called quantum spin liquids as exemplified by the one-dimensional spin-1/2
chain which has no magnetic order and where magnons accordingly fractionalize
into spinons with angular momentum $\hbar/2$. This is spectacularly revealed by
a continuum of inelastic neutron scattering associated with two-spinon
processes and the absence of magnetic Bragg diffraction. Here, we report
evidence for these same key features of a quantum spin liquid in the
three-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific
heat and neutron scattering measurements, Monte Carlo simulations, and analytic
approximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is
an almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on
a pyrochlore lattice with weak connectivity and frustrated interactions.
Magnetic Bragg diffraction is absent and 90\% of the spectral weight forms a
continuum of magnetic scattering not dissimilar to that of the spin-1/2 chain
but with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb
phase. The residual entropy and diffuse elastic scattering points to an exotic
state of matter driven by frustration, quantum fluctuations and weak exchange
disorder.
",0,1,0,0,0,0
564,SimProp v2r4: Monte Carlo simulation code for UHECR propagation,"  We introduce the new version of SimProp, a Monte Carlo code for simulating
the propagation of ultra-high energy cosmic rays in intergalactic space. This
version, SimProp v2r4, together with an overall improvement of the code
capabilities with a substantial reduction in the computation time, also
computes secondary cosmogenic particles such as electron-positron pairs and
gamma rays produced during the propagation of ultra-high energy cosmic rays. As
recently pointed out by several authors, the flux of this secondary radiation
and its products, within reach of the current observatories, provides useful
information about models of ultra-high energy cosmic ray sources which would be
hard to discriminate otherwise.
",0,1,0,0,0,0
565,Non-negative Matrix Factorization via Archetypal Analysis,"  Given a collection of data points, non-negative matrix factorization (NMF)
suggests to express them as convex combinations of a small set of `archetypes'
with non-negative entries. This decomposition is unique only if the true
archetypes are non-negative and sufficiently sparse (or the weights are
sufficiently sparse), a regime that is captured by the separability condition
and its generalizations.
In this paper, we study an approach to NMF that can be traced back to the
work of Cutler and Breiman (1994) and does not require the data to be
separable, while providing a generally unique decomposition. We optimize the
trade-off between two objectives: we minimize the distance of the data points
from the convex envelope of the archetypes (which can be interpreted as an
empirical risk), while minimizing the distance of the archetypes from the
convex envelope of the data (which can be interpreted as a data-dependent
regularization). The archetypal analysis method of (Cutler, Breiman, 1994) is
recovered as the limiting case in which the last term is given infinite weight.
We introduce a `uniqueness condition' on the data which is necessary for
exactly recovering the archetypes from noiseless data. We prove that, under
uniqueness (plus additional regularity conditions on the geometry of the
archetypes), our estimator is robust. While our approach requires solving a
non-convex optimization problem, we find that standard optimization methods
succeed in finding good solutions both for real and synthetic data.
",1,0,0,0,0,0
566,Muon Reconstruction in the Daya Bay Water Pools,"  Muon reconstruction in the Daya Bay water pools would serve to verify the
simulated muon fluxes and offer the possibility of studying cosmic muons in
general. This reconstruction is, however, complicated by many optical obstacles
and the small coverage of photomultiplier tubes (PMTs) as compared to other
large water Cherenkov detectors. The PMTs' timing information is useful only in
the case of direct, unreflected Cherenkov light. This requires PMTs to be added
and removed as an hypothesized muon trajectory is iteratively improved, to
account for the changing effects of obstacles and direction of light.
Therefore, muon reconstruction in the Daya Bay water pools does not lend itself
to a general fitting procedure employing smoothly varying functions with
continuous derivatives. Here, an algorithm is described which overcomes these
complications. It employs the method of Least Mean Squares to determine an
hypothesized trajectory from the PMTs' charge-weighted positions. This
initially hypothesized trajectory is then iteratively refined using the PMTs'
timing information. Reconstructions with simulated data reproduce the simulated
trajectory to within about 5 degrees in direction and about 45 cm in position
at the pool surface, with a bias that tends to pull tracks away from the
vertical by about 3 degrees.
",0,1,0,0,0,0
567,Improving Foot-Mounted Inertial Navigation Through Real-Time Motion Classification,"  We present a method to improve the accuracy of a foot-mounted,
zero-velocity-aided inertial navigation system (INS) by varying estimator
parameters based on a real-time classification of motion type. We train a
support vector machine (SVM) classifier using inertial data recorded by a
single foot-mounted sensor to differentiate between six motion types (walking,
jogging, running, sprinting, crouch-walking, and ladder-climbing) and report
mean test classification accuracy of over 90% on a dataset with five different
subjects. From these motion types, we select two of the most common (walking
and running), and describe a method to compute optimal zero-velocity detection
parameters tailored to both a specific user and motion type by maximizing the
detector F-score. By combining the motion classifier with a set of optimal
detection parameters, we show how we can reduce INS position error during mixed
walking and running motion. We evaluate our adaptive system on a total of 5.9
km of indoor pedestrian navigation performed by five different subjects moving
along a 130 m path with surveyed ground truth markers.
",1,0,0,0,0,0
568,An Extended Low Fat Allocator API and Applications,"  The primary function of memory allocators is to allocate and deallocate
chunks of memory primarily through the malloc API. Many memory allocators also
implement other API extensions, such as deriving the size of an allocated
object from the object's pointer, or calculating the base address of an
allocation from an interior pointer. In this paper, we propose a general
purpose extended allocator API built around these common extensions. We argue
that such extended APIs have many applications and demonstrate several use
cases, such as (manual) memory error detection, meta data storage, typed
pointers and compact data-structures. Because most existing allocators were not
designed for the extended API, traditional implementations are expensive or not
possible.
Recently, the LowFat allocator for heap and stack objects has been developed.
The LowFat allocator is an implementation of the idea of low-fat pointers,
where object bounds information (size and base) are encoded into the native
machine pointer representation itself. The ""killer app"" for low-fat pointers is
automated bounds check instrumentation for program hardening and bug detection.
However, the LowFat allocator can also be used to implement highly optimized
version of the extended allocator API, which makes the new applications (listed
above) possible. In this paper, we implement and evaluate several applications
based efficient memory allocator API extensions using low-fat pointers. We also
extend the LowFat allocator to cover global objects for the first time.
",1,0,0,0,0,0
569,Exoplanet Atmosphere Retrieval using Multifractal Analysis of Secondary Eclipse Spectra,"  We extend a data-based model-free multifractal method of exoplanet detection
to probe exoplanetary atmospheres. Whereas the transmission spectrum is studied
during the primary eclipse, we analyze the emission spectrum during the
secondary eclipse, thereby probing the atmospheric limb. In addition to the
spectral structure of exoplanet atmospheres, the approach provides information
to study phenomena such as atmospheric flows, tidal-locking behavior, and the
dayside-nightside redistribution of energy. The approach is demonstrated using
Spitzer data for exoplanet HD189733b. The central advantage of the method is
the lack of model assumptions in the detection and observational schemes.
",0,1,0,1,0,0
570,Permutation Tests for Infection Graphs,"  We formulate and analyze a novel hypothesis testing problem for inferring the
edge structure of an infection graph. In our model, a disease spreads over a
network via contagion or random infection, where the random variables governing
the rates of contracting the disease from neighbors or random infection are
independent exponential random variables with unknown rate parameters. A subset
of nodes is also censored uniformly at random. Given the statuses of nodes in
the network, the goal is to determine the underlying graph. We present a
procedure based on permutation testing, and we derive sufficient conditions for
the validity of our test in terms of automorphism groups of the graphs
corresponding to the null and alternative hypotheses. Further, the test is
valid more generally for infection processes satisfying a basic symmetry
condition. Our test is easy to compute and does not involve estimating unknown
parameters governing the process. We also derive risk bounds for our
permutation test in a variety of settings, and motivate our test statistic in
terms of approximate equivalence to likelihood ratio testing and maximin tests.
We conclude with an application to real data from an HIV infection network.
",1,0,1,1,0,0
571,A novel distribution-free hybrid regression model for manufacturing process efficiency improvement,"  This work is motivated by a particular problem of a modern paper
manufacturing industry, in which maximum efficiency of the fiber-filler
recovery process is desired. A lot of unwanted materials along with valuable
fibers and fillers come out as a by-product of the paper manufacturing process
and mostly goes as waste. The job of an efficient Krofta supracell is to
separate the unwanted materials from the valuable ones so that fibers and
fillers can be collected from the waste materials and reused in the
manufacturing process. The efficiency of Krofta depends on several crucial
process parameters and monitoring them is a difficult proposition. To solve
this problem, we propose a novel hybridization of regression trees (RT) and
artificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of
low recovery percentage of the supracell. This model is used to achieve the
goal of improving supracell efficiency, viz., gain in percentage recovery. In
addition, theoretical results for the universal consistency of the proposed
model are given with the optimal value of a vital model parameter. Experimental
findings show that the proposed hybrid RT-ANN model achieves higher accuracy in
predicting Krofta recovery percentage than other conventional regression models
for solving the Krofta efficiency problem. This work will help the paper
manufacturing company to become environmentally friendly with minimal
ecological damage and improved waste recovery.
",0,0,0,1,0,0
572,Exploring deep learning as an event classification method for the Cherenkov Telescope Array,"  Telescopes based on the imaging atmospheric Cherenkov technique (IACTs)
detect images of the atmospheric showers generated by gamma rays and cosmic
rays as they are absorbed by the atmosphere. The much more frequent cosmic-ray
events form the main background when looking for gamma-ray sources, and
therefore IACT sensitivity is significantly driven by the capability to
distinguish between these two types of events. Supervised learning algorithms,
like random forests and boosted decision trees, have been shown to effectively
classify IACT events. In this contribution we present results from exploratory
work using deep learning as an event classification method for the Cherenkov
Telescope Array (CTA). CTA, conceived as an array of tens of IACTs, is an
international project for a next-generation ground-based gamma-ray observatory,
aiming to improve on the sensitivity of current-generation experiments by an
order of magnitude and provide energy coverage from 20 GeV to more than 300
TeV.
",0,1,0,0,0,0
573,Photo-Induced Bandgap Renormalization Governs the Ultrafast Response of Single-Layer MoS2,"  Transition metal dichalcogenides (TMDs) are emerging as promising
two-dimensional (2d) semiconductors for optoelectronic and flexible devices.
However, a microscopic explanation of their photophysics -- of pivotal
importance for the understanding and optimization of device operation -- is
still lacking. Here we use femtosecond transient absorption spectroscopy, with
pump pulse tunability and broadband probing, to monitor the relaxation dynamics
of single-layer MoS2 over the entire visible range, upon photoexcitation of
different excitonic transitions. We find that, irrespective of excitation
photon energy, the transient absorption spectrum shows the simultaneous
bleaching of all excitonic transitions and corresponding red-shifted
photoinduced absorption bands. First-principle modeling of the ultrafast
optical response reveals that a transient bandgap renormalization, caused by
the presence of photo-excited carriers, is primarily responsible for the
observed features. Our results demonstrate the strong impact of many-body
effects in the transient optical response of TMDs even in the
low-excitation-density regime.
",0,1,0,0,0,0
574,Improved Quantile Regression Estimators when the Errors are Independently and Non-identically Distributed,"  In a classical regression model, it is usually assumed that the explanatory
variables are independent of each other and error terms are normally
distributed. But when these assumptions are not met, situations like the error
terms are not independent or they are not identically distributed or both of
these, LSE will not be robust. Hence, quantile regression has been used to
complement this deficiency of classical regression analysis and to improve the
least square estimation (LSE). In this study, we consider preliminary test and
shrinkage estimation strategies for quantile regression models with
independently and non-identically distributed (i.ni.d.) errors. A Monte Carlo
simulation study is conducted to assess the relative performance of the
estimators. Also, we numerically compare their performance with Ridge, Lasso,
Elastic Net penalty estimation strategies. A real data example is presented to
illustrate the usefulness of the suggested methods. Finally, we obtain the
asymptotic results of suggested estimators
",0,0,1,1,0,0
575,Language Modeling by Clustering with Word Embeddings for Text Readability Assessment,"  We present a clustering-based language model using word embeddings for text
readability prediction. Presumably, an Euclidean semantic space hypothesis
holds true for word embeddings whose training is done by observing word
co-occurrences. We argue that clustering with word embeddings in the metric
space should yield feature representations in a higher semantic space
appropriate for text regression. Also, by representing features in terms of
histograms, our approach can naturally address documents of varying lengths. An
empirical evaluation using the Common Core Standards corpus reveals that the
features formed on our clustering-based language model significantly improve
the previously known results for the same corpus in readability prediction. We
also evaluate the task of sentence matching based on semantic relatedness using
the Wiki-SimpleWiki corpus and find that our features lead to superior matching
performance.
",1,0,0,0,0,0
576,Discriminant circle bundles over local models of Strebel graphs and Boutroux curves,"  We study special circle bundles over two elementary moduli spaces of
meromorphic quadratic differentials with real periods denoted by $\mathcal
Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$. The space
$\mathcal Q_0^{\mathbb R}(-7)$ is the moduli space of meromorphic quadratic
differentials on the Riemann sphere with one pole of order 7 with real periods;
it appears naturally in the study of a neighbourhood of the Witten's cycle
$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic
differentials of $\mathcal M_{g,n}$. The space $\mathcal Q^{\mathbb
R}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the
Riemann sphere with two poles of order at most 3 with real periods; it appears
in description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the
combinatorial model. The application of the formalism of the Bergman
tau-function to the combinatorial model (with the goal of computing
analytically Poincare dual cycles to certain combinations of tautological
classes) requires the study of special sections of circle bundles over
$\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$; in the
case of the space $\mathcal Q_0^{\mathbb R}(-7)$ a section of this circle
bundle is given by the argument of the modular discriminant. We study the
spaces $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$,
also called the spaces of Boutroux curves, in detail, together with
corresponding circle bundles.
",0,1,1,0,0,0
577,Far-field theory for trajectories of magnetic ellipsoids in rectangular and circular channels,"  We report a method to control the positions of ellipsoidal magnets in flowing
channels of rectangular or circular cross section at low Reynolds number.A
static uniform magnetic field is used to pin the particle orientation, and the
particles move with translational drift velocities resulting from hydrodynamic
interactions with the channel walls which can be described using Blake's image
tensor.Building on his insights, we are able to present a far-field theory
predicting the particle motion in rectangular channels, and validate the
accuracy of the theory by comparing to numerical solutions using the boundary
element method.We find that, by changing the direction of the applied magnetic
field, the motion can be controlled so that particles move either to a curved
focusing region or to the channel walls.We also use simulations to show that
the particles are focused to a single line in a circular channel.Our results
suggest ways to focus and segregate magnetic particles in lab-on-a-chip
devices.
",0,1,0,0,0,0
578,On M-functions associated with modular forms,"  Let $f$ be a primitive cusp form of weight $k$ and level $N,$ let $\chi$ be a
Dirichlet character of conductor coprime with $N,$ and let
$\mathfrak{L}(f\otimes \chi, s)$ denote either $\log L(f\otimes \chi, s)$ or
$(L'/L)(f\otimes \chi, s).$ In this article we study the distribution of the
values of $\mathfrak{L}$ when either $\chi$ or $f$ vary. First, for a
quasi-character $\psi\colon \mathbb{C} \to \mathbb{C}^\times$ we find the limit
for the average $\mathrm{Avg}\_\chi \psi(L(f\otimes\chi, s)),$ when $f$ is
fixed and $\chi$ varies through the set of characters with prime conductor that
tends to infinity. Second, we prove an equidistribution result for the values
of $\mathfrak{L}(f\otimes \chi,s)$ by establishing analytic properties of the
above limit function. Third, we study the limit of the harmonic average
$\mathrm{Avg}^h\_f \psi(L(f, s)),$ when $f$ runs through the set of primitive
cusp forms of given weight $k$ and level $N\to \infty.$ Most of the results are
obtained conditionally on the Generalized Riemann Hypothesis for
$L(f\otimes\chi, s).$
",0,0,1,0,0,0
579,Learning Graph Representations by Dendrograms,"  Hierarchical graph clustering is a common technique to reveal the multi-scale
structure of complex networks. We propose a novel metric for assessing the
quality of a hierarchical clustering. This metric reflects the ability to
reconstruct the graph from the dendrogram, which encodes the hierarchy. The
optimal representation of the graph defines a class of reducible linkages
leading to regular dendrograms by greedy agglomerative clustering.
",1,0,0,1,0,0
580,Slow and Long-ranged Dynamical Heterogeneities in Dissipative Fluids,"  A two-dimensional bidisperse granular fluid is shown to exhibit pronounced
long-ranged dynamical heterogeneities as dynamical arrest is approached. Here
we focus on the most direct approach to study these heterogeneities: we
identify clusters of slow particles and determine their size, $N_c$, and their
radius of gyration, $R_G$. We show that $N_c\propto R_G^{d_f}$, providing
direct evidence that the most immobile particles arrange in fractal objects
with a fractal dimension, $d_f$, that is observed to increase with packing
fraction $\phi$. The cluster size distribution obeys scaling, approaching an
algebraic decay in the limit of structural arrest, i.e., $\phi\to\phi_c$.
Alternatively, dynamical heterogeneities are analyzed via the four-point
structure factor $S_4(q,t)$ and the dynamical susceptibility $\chi_4(t)$.
$S_4(q,t)$ is shown to obey scaling in the full range of packing fractions,
$0.6\leq\phi\leq 0.805$, and to become increasingly long-ranged as
$\phi\to\phi_c$. Finite size scaling of $\chi_4(t)$ provides a consistency
check for the previously analyzed divergences of $\chi_4(t)\propto
(\phi-\phi_c)^{-\gamma_{\chi}}$ and the correlation length $\xi\propto
(\phi-\phi_c)^{-\gamma_{\xi}}$. We check the robustness of our results with
respect to our definition of mobility. The divergences and the scaling for
$\phi\to\phi_c$ suggest a non-equilibrium glass transition which seems
qualitatively independent of the coefficient of restitution.
",0,1,0,0,0,0
581,A Globally Linearly Convergent Method for Pointwise Quadratically Supportable Convex-Concave Saddle Point Problems,"  We study the \emph{Proximal Alternating Predictor-Corrector} (PAPC) algorithm
introduced recently by Drori, Sabach and Teboulle to solve nonsmooth structured
convex-concave saddle point problems consisting of the sum of a smooth convex
function, a finite collection of nonsmooth convex functions and bilinear terms.
We introduce the notion of pointwise quadratic supportability, which is a
relaxation of a standard strong convexity assumption and allows us to show that
the primal sequence is R-linearly convergent to an optimal solution and the
primal-dual sequence is globally Q-linearly convergent. We illustrate the
proposed method on total variation denoising problems and on locally adaptive
estimation in signal/image deconvolution and denoising with multiresolution
statistical constraints.
",0,0,1,0,0,0
582,Stability of casein micelles cross-linked with genipin: a physicochemical study as a function of pH,"  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their
stability against dissociating agents. In this paper, a comparative study of
stability between native CMs and CMs cross-linked with genipin (CMs-GP) as a
function of pH is described. Stability to temperature and ethanol were
investigated in the pH range 2.0-7.0. The size and the charge
($\zeta$-potential) of the particles were determined by dynamic light
scattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH
3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.
The isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability
against heat and ethanol was observed for CMs-GP at pH 2, where visible
coagulation was determined only after 800 s at 140 $^\circ$C or 87.5% (v/v) of
ethanol. These results confirmed the hypothesis that cross-linking by GP
increased the stability of CMs.
",0,1,0,0,0,0
583,Kites and Residuated Lattices,"  We investigate a construction of an integral residuated lattice starting from
an integral residuated lattice and two sets with an injective mapping from one
set into the second one. The resulting algebra has a shape of a Chinese cascade
kite, therefore, we call this algebra simply a kite. We describe subdirectly
irreducible kites and we classify them. We show that the variety of integral
residuated lattices generated by kites is generated by all finite-dimensional
kites. In particular, we describe some homomorphisms among kites.
",0,0,1,0,0,0
584,TED Talk Recommender Using Speech Transcripts,"  Nowadays, online video platforms mostly recommend related videos by analyzing
user-driven data such as viewing patterns, rather than the content of the
videos. However, content is more important than any other element when videos
aim to deliver knowledge. Therefore, we have developed a web application which
recommends related TED lecture videos to the users, considering the content of
the videos from the transcripts. TED Talk Recommender constructs a network for
recommending videos that are similar content-wise and providing a user
interface.
",1,0,0,0,0,0
585,Attitude Control of the Asteroid Origins Satellite 1 (AOSAT 1),"  Exploration of asteroids and small-bodies can provide valuable insight into
the origins of the solar system, into the origins of Earth and the origins of
the building blocks of life. However, the low-gravity and unknown surface
conditions of asteroids presents a daunting challenge for surface exploration,
manipulation and for resource processing. This has resulted in the loss of
several landers or shortened missions. Fundamental studies are required to
obtain better readings of the material surface properties and physical models
of these small bodies. The Asteroid Origins Satellite 1 (AOSAT 1) is a CubeSat
centrifuge laboratory that spins at up to 4 rpm to simulate the milligravity
conditions of sub 1 km asteroids. Such a laboratory will help to de-risk
development and testing of landing and resource processing technology for
asteroids. Inside the laboratory are crushed meteorites, the remains of
asteroids. The laboratory is equipped with cameras and actuators to perform a
series of science experiments to better understand material properties and
asteroid surface physics. These results will help to improve our physics models
of asteroids. The CubeSat has been designed to be low-cost and contains 3-axis
magnetorquers and a single reaction-wheel to induce spin. In our work, we first
analyze how the attitude control system will de-tumble the spacecraft after
deployment. Further analysis has been conducted to analyze the impact and
stability of the attitude control system to shifting mass (crushed meteorites)
inside the spacecraft as its spinning in its centrifuge mode. AOSAT 1 will be
the first in a series of low-cost CubeSat centrifuges that will be launched
setting the stage for a larger, permanent, on-orbit centrifuge laboratory for
experiments in planetary science, life sciences and manufacturing.
",1,1,0,0,0,0
586,Triplet Network with Attention for Speaker Diarization,"  In automatic speech processing systems, speaker diarization is a crucial
front-end component to separate segments from different speakers. Inspired by
the recent success of deep neural networks (DNNs) in semantic inferencing,
triplet loss-based architectures have been successfully used for this problem.
However, existing work utilizes conventional i-vectors as the input
representation and builds simple fully connected networks for metric learning,
thus not fully leveraging the modeling power of DNN architectures. This paper
investigates the importance of learning effective representations from the
sequences directly in metric learning pipelines for speaker diarization. More
specifically, we propose to employ attention models to learn embeddings and the
metric jointly in an end-to-end fashion. Experiments are conducted on the
CALLHOME conversational speech corpus. The diarization results demonstrate
that, besides providing a unified model, the proposed approach achieves
improved performance when compared against existing approaches.
",0,0,0,1,0,0
587,Dynamics of cracks in disordered materials,"  Predicting when rupture occurs or cracks progress is a major challenge in
numerous elds of industrial, societal and geophysical importance. It remains
largely unsolved: Stress enhancement at cracks and defects, indeed, makes the
macroscale dynamics extremely sensitive to the microscale material disorder.
This results in giant statistical uctuations and non-trivial behaviors upon
upscaling dicult to assess via the continuum approaches of engineering. These
issues are examined here. We will see: How linear elastic fracture mechanics
sidetracks the diculty by reducing the problem to that of the propagation of a
single crack in an eective material free of defects, How slow cracks sometimes
display jerky dynamics, with sudden violent events incompatible with the
previous approach, and how some paradigms of statistical physics can explain
it, How abnormally fast cracks sometimes emerge due to the formation of
microcracks at very small scales.
",0,1,0,0,0,0
588,Optimum Decoder for Multiplicative Spread Spectrum Image Watermarking with Laplacian Modeling,"  This paper investigates the multiplicative spread spectrum watermarking
method for the image. The information bit is spreaded into middle-frequency
Discrete Cosine Transform (DCT) coefficients of each block of an image using a
generated pseudo-random sequence. Unlike the conventional signal modeling, we
suppose that both signal and noise are distributed with Laplacian distribution
because the sample loss of digital media can be better modeled with this
distribution than the Gaussian one. We derive the optimum decoder for the
proposed embedding method thanks to the maximum likelihood decoding scheme. We
also analyze our watermarking system in the presence of noise and provide
analytical evaluations and several simulations. The results show that it has
the suitable performance and transparency required for watermarking
applications.
",1,0,0,0,0,0
589,Adaptive Path-Integral Autoencoder: Representation Learning and Planning for Dynamical Systems,"  We present a representation learning algorithm that learns a low-dimensional
latent dynamical system from high-dimensional \textit{sequential} raw data,
e.g., video. The framework builds upon recent advances in amortized inference
methods that use both an inference network and a refinement procedure to output
samples from a variational distribution given an observation sequence, and
takes advantage of the duality between control and inference to approximately
solve the intractable inference problem using the path integral control
approach. The learned dynamical model can be used to predict and plan the
future states; we also present the efficient planning method that exploits the
learned low-dimensional latent dynamics. Numerical experiments show that the
proposed path-integral control based variational inference method leads to
tighter lower bounds in statistical model learning of sequential data. The
supplementary video: this https URL
",1,0,0,1,0,0
590,Semiblind subgraph reconstruction in Gaussian graphical models,"  Consider a social network where only a few nodes (agents) have meaningful
interactions in the sense that the conditional dependency graph over node
attribute variables (behaviors) is sparse. A company that can only observe the
interactions between its own customers will generally not be able to accurately
estimate its customers' dependency subgraph: it is blinded to any external
interactions of its customers and this blindness creates false edges in its
subgraph. In this paper we address the semiblind scenario where the company has
access to a noisy summary of the complementary subgraph connecting external
agents, e.g., provided by a consolidator. The proposed framework applies to
other applications as well, including field estimation from a network of awake
and sleeping sensors and privacy-constrained information sharing over social
subnetworks. We propose a penalized likelihood approach in the context of a
graph signal obeying a Gaussian graphical models (GGM). We use a convex-concave
iterative optimization algorithm to maximize the penalized likelihood.
",1,0,0,1,0,0
591,Hybrid Collaborative Recommendation via Semi-AutoEncoder,"  In this paper, we present a novel structure, Semi-AutoEncoder, based on
AutoEncoder. We generalize it into a hybrid collaborative filtering model for
rating prediction as well as personalized top-n recommendations. Experimental
results on two real-world datasets demonstrate its state-of-the-art
performances.
",1,0,0,0,0,0
592,The Linear Point: A cleaner cosmological standard ruler,"  We show how a characteristic length scale imprinted in the galaxy two-point
correlation function, dubbed the ""linear point"", can serve as a comoving
cosmological standard ruler. In contrast to the Baryon Acoustic Oscillation
peak location, this scale is constant in redshift and is unaffected by
non-linear effects to within $0.5$ percent precision. We measure the location
of the linear point in the galaxy correlation function of the LOWZ and CMASS
samples from the Twelfth Data Release (DR12) of the Baryon Oscillation
Spectroscopic Survey (BOSS) collaboration. We combine our linear-point
measurement with cosmic-microwave-background constraints from the Planck
satellite to estimate the isotropic-volume distance $D_{V}(z)$, without relying
on a model-template or reconstruction method. We find $D_V(0.32)=1264\pm 28$
Mpc and $D_V(0.57)=2056\pm 22$ Mpc respectively, consistent with the quoted
values from the BOSS collaboration. This remarkable result suggests that all
the distance information contained in the baryon acoustic oscillations can be
conveniently compressed into the single length associated with the linear
point.
",0,1,0,0,0,0
593,Incompressible limit of the Navier-Stokes model with a growth term,"  Starting from isentropic compressible Navier-Stokes equations with growth
term in the continuity equation, we rigorously justify that performing an
incompressible limit one arrives to the two-phase free boundary fluid system.
",0,0,1,0,0,0
594,Stochastic Primal-Dual Method on Riemannian Manifolds with Bounded Sectional Curvature,"  We study a stochastic primal-dual method for constrained optimization over
Riemannian manifolds with bounded sectional curvature. We prove non-asymptotic
convergence to the optimal objective value. More precisely, for the class of
hyperbolic manifolds, we establish a convergence rate that is related to the
sectional curvature lower bound. To prove a convergence rate in terms of
sectional curvature for the elliptic manifolds, we leverage Toponogov's
comparison theorem. In addition, we provide convergence analysis for the
asymptotically elliptic manifolds, where the sectional curvature at each given
point on manifold is locally bounded from below by the distance function. We
demonstrate the performance of the primal-dual algorithm on the sphere for the
non-negative principle component analysis (PCA). In particular, under the
non-negativity constraint on the principle component and for the symmetric
spiked covariance model, we empirically show that the primal-dual approach
outperforms the spectral method. We also examine the performance of the
primal-dual method for the anchored synchronization from partial noisy
measurements of relative rotations on the Lie group SO(3). Lastly, we show that
the primal-dual algorithm can be applied to the weighted MAX-CUT problem under
constraints on the admissible cut. Specifically, we propose different
approximation algorithms for the weighted MAX-CUT problem based on optimizing a
function on the manifold of direct products of the unit spheres as well as the
manifold of direct products of the rotation groups.
",0,0,1,0,0,0
595,Nudging the particle filter,"  We investigate a new sampling scheme aimed at improving the performance of
particle filters whenever (a) there is a significant mismatch between the
assumed model dynamics and the actual system, or (b) the posterior probability
tends to concentrate in relatively small regions of the state space. The
proposed scheme pushes some particles towards specific regions where the
likelihood is expected to be high, an operation known as nudging in the
geophysics literature. We re-interpret nudging in a form applicable to any
particle filtering scheme, as it does not involve any changes in the rest of
the algorithm. Since the particles are modified, but the importance weights do
not account for this modification, the use of nudging leads to additional bias
in the resulting estimators. However, we prove analytically that nudged
particle filters can still attain asymptotic convergence with the same error
rates as conventional particle methods. Simple analysis also yields an
alternative interpretation of the nudging operation that explains its
robustness to model errors. Finally, we show numerical results that illustrate
the improvements that can be attained using the proposed scheme. In particular,
we present nonlinear tracking examples with synthetic data and a model
inference example using real-world financial data.
",0,0,0,1,0,0
596,Quantum oscillations and a non-trivial Berry phase in the noncentrosymmetric superconductor BiPd,"  We report the measurements of de Haas-van Alphen (dHvA) oscillations in the
noncentrosymmetric superconductor BiPd. Several pieces of a complex multi-sheet
Fermi surface are identified, including a small pocket (frequency 40 T) which
is three dimensional and anisotropic. From the temperature dependence of the
amplitude of the oscillations, the cyclotron effective mass is ($0.18$ $\pm$
0.1) $m_e$. Further analysis showed a non-trivial $\pi$-Berry phase is
associated with the 40 T pocket, which strongly supports the presence of
topological states in bulk BiPd and may result in topological superconductivity
due to the proximity coupling to other bands.
",0,1,0,0,0,0
597,ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional Linear Models,"  Statistical inference can be computationally prohibitive in
ultrahigh-dimensional linear models. Correlation-based variable screening, in
which one leverages marginal correlations for removal of irrelevant variables
from the model prior to statistical inference, can be used to overcome this
challenge. Prior works on correlation-based variable screening either impose
strong statistical priors on the linear model or assume specific post-screening
inference methods. This paper first extends the analysis of correlation-based
variable screening to arbitrary linear models and post-screening inference
techniques. In particular, ($i$) it shows that a condition---termed the
screening condition---is sufficient for successful correlation-based screening
of linear models, and ($ii$) it provides insights into the dependence of
marginal correlation-based screening on different problem parameters. Numerical
experiments confirm that these insights are not mere artifacts of analysis;
rather, they are reflective of the challenges associated with marginal
correlation-based variable screening. Second, the paper explicitly derives the
screening condition for two families of linear models, namely, sub-Gaussian
linear models and arbitrary (random or deterministic) linear models. In the
process, it establishes that---under appropriate conditions---it is possible to
reduce the dimension of an ultrahigh-dimensional, arbitrary linear model to
almost the sample size even when the number of active variables scales almost
linearly with the sample size.
",0,0,1,1,0,0
598,Unveiling ADP-binding sites and channels in respiratory complexes: Validation of Murburn concept as a holistic explanation for oxidative phosphorylation,"  Mitochondrial oxidative phosphorylation (mOxPhos) makes ATP, the energy
currency of life. Chemiosmosis, a proton centric mechanism, advocates that
Complex V harnesses a transmembrane potential (TMP) for ATP synthesis. This
perception of cellular respiration requires oxygen to stay tethered at Complex
IV (an association inhibited by cyanide) and diffusible reactive oxygen species
(DROS) are considered wasteful and toxic products. With new mechanistic
insights on heme and flavin enzymes, an oxygen or DROS centric explanation
(called murburn concept) was recently proposed for mOxPhos. In the new
mechanism, TMP is not directly harnessed, protons are a rate limiting reactant
and DROS within matrix serve as the chemical coupling agents that directly link
NADH oxidation with ATP synthesis. Herein, we report multiple ADP binding sites
and solvent accessible DROS channels in respiratory proteins, which validate
the oxygen or DROS centric power generation (ATP synthesis) system in mOxPhos.
Since cyanide's heme binding Kd is high (mM), low doses (uM) of cyanide is
lethal because cyanide disrupts DROS dynamics in mOxPhos. The critical study
also provides comprehensive arguments against Mitchell's and Boyer's
explanations and extensive support for murburn concept based holistic
perspectives for mOxPhos.
",0,0,0,0,1,0
599,Banach synaptic algebras,"  Using a representation theorem of Erik Alfsen, Frederic Schultz, and Erling
Stormer for special JB-algebras, we prove that a synaptic algebra is norm
complete (i.e., Banach) if and only if it is isomorphic to the self-adjoint
part of a Rickart C*-algebra. Also, we give conditions on a Banach synaptic
algebra that are equivalent to the condition that it is isomorphic to the
self-adjoint part of an AW*-algebra. Moreover, we study some relationships
between synaptic algebras and so-called generalized Hermitian algebras.
",0,0,1,0,0,0
600,"Pressure tuning of structure, superconductivity and novel magnetic order in the Ce-underdoped electron-doped cuprate T'-Pr_1.3